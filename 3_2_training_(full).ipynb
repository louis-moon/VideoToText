{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SxJ9yxBW9d_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e971444e-f9a4-47b4-9d15-b06a426abeb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!pip install transformers\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import os\n",
        "import logging\n",
        "from transformers import BertTokenizerFast\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# Logger setup\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Tensorboard Writer\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# 3D CNN Encoder\n",
        "class EncoderCNN3D(nn.Module):\n",
        "  def __init__(self, channel_size=64, output_feature_size=512):\n",
        "    super(EncoderCNN3D, self).__init__()\n",
        "    self.conv1 = nn.Conv3d(channel_size, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
        "    self.pool = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
        "    self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.fc = nn.Linear(2097152, output_feature_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "        # Use full precision for convolution operations\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# DecoderRNN\n",
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, feature_size):\n",
        "    super(DecoderRNN, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "    self.lstm = nn.LSTM(input_size=embed_size + feature_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
        "    self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, features, captions):\n",
        "    embeddings = self.embedding(captions).view(captions.size(0), captions.size(1), -1)\n",
        "    features = features.unsqueeze(1).repeat(1, captions.size(1), 1)\n",
        "    combined = torch.cat((features, embeddings), dim=2)\n",
        "    hiddens, _ = self.lstm(combined)\n",
        "    outputs = self.linear(hiddens)\n",
        "    return outputs\n",
        "\n",
        "# EncoderDecoderModel\n",
        "class EncoderDecoderModel(nn.Module):\n",
        "    def __init__(self, encoder, decoder, embed_size):\n",
        "        super(EncoderDecoderModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, frames, captions):\n",
        "        features = self.encoder(frames)\n",
        "        outputs = self.decoder(features, captions[:, :-1])\n",
        "        return outputs\n",
        "\n",
        "# Dataset\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path\n",
        "        try:\n",
        "            self.data = torch.load(self.file_path)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading {self.file_path}: {e}\")\n",
        "            self.data = {'frames': [], 'input_ids': []}\n",
        "    def __len__(self):\n",
        "        return len(self.data['input_ids'])\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            data = torch.load(self.file_path)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading {self.file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "        frames = data['frames']\n",
        "        captions = data['input_ids'][idx]  # Get the tokenized description for the specific index\n",
        "\n",
        "        return frames, captions\n",
        "\n",
        "# Metrics Tracking\n",
        "def compute_accuracy(outputs, targets):\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == targets).float()\n",
        "    accuracy = correct.sum() / len(correct)\n",
        "    return accuracy.item()\n",
        "\n",
        "# Model\n",
        "encoder = EncoderCNN3D()\n",
        "decoder = DecoderRNN(embed_size=256, hidden_size=512, vocab_size=tokenizer.vocab_size, feature_size=512)\n",
        "model = EncoderDecoderModel(encoder, decoder, embed_size=256).cuda()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "# Learning Rate Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.NLLLoss(ignore_index=tokenizer.pad_token_id, reduction='none')\n",
        "\n",
        "# Custom sequence loss\n",
        "def sequence_loss(outputs, targets, mask):\n",
        "    log_probs = F.log_softmax(outputs, dim=2)\n",
        "\n",
        "    # Ensure targets are correctly shaped\n",
        "    # targets should be: [batch_size, seq_len] with each element being a class label index\n",
        "    if targets.dim() != 2:\n",
        "        raise ValueError(f\"targets tensor has incorrect number of dimensions: {targets.dim()}\")\n",
        "\n",
        "    # Expanding targets to match log_probs dimensions\n",
        "    targets_expanded = targets.unsqueeze(-1)  # Shape: [batch_size, seq_len, 1]\n",
        "\n",
        "    log_probs_for_targets = log_probs.gather(2, targets_expanded).squeeze(-1)\n",
        "\n",
        "    log_probs_for_targets *= mask\n",
        "\n",
        "    loss = -log_probs_for_targets.sum() / mask.sum()\n",
        "    return loss\n",
        "\n",
        "# Create train/validation split\n",
        "num_batches = 1573\n",
        "batch_indices = list(range(num_batches))\n",
        "random.shuffle(batch_indices)\n",
        "val_batches = batch_indices[:int(0.2 * num_batches)]\n",
        "train_batches = batch_indices[int(0.2 * num_batches):]\n",
        "\n",
        "# Define a consistent sequence length for captions\n",
        "max_len = 50\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Training Loop\n",
        "clip_value = 1\n",
        "problematic_batches = [423, 1207, 1208]\n",
        "batch_size = 32  # Adjust as needed\n",
        "accumulation_steps = 2\n",
        "for epoch in range(1):\n",
        "    for phase in [\"train\", \"val\"]:\n",
        "        if phase == \"train\":\n",
        "            model.train()\n",
        "            batch_list = train_batches\n",
        "        else:\n",
        "            model.eval()\n",
        "            batch_list = val_batches\n",
        "            val_loss = 0\n",
        "            val_accuracy = 0\n",
        "            num_batches = 1573\n",
        "\n",
        "        total_batches = len(batch_list)\n",
        "        for batch_index, batch_num in enumerate(batch_list):\n",
        "            if batch_num in problematic_batches:\n",
        "                continue # Skip problematic batches\n",
        "            batch_file = f\"/content/drive/MyDrive/Video-to-Text/processed_data/batch_{batch_num}.pt\"\n",
        "            dataset = TestDataset(batch_file)\n",
        "            dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=6)\n",
        "\n",
        "            for step, (frames, captions) in enumerate(dataloader):\n",
        "                if frames is None or captions is None:\n",
        "                    continue\n",
        "                if frames.size(1) != 64:\n",
        "                    continue  # Skip this batch\n",
        "                frames, captions = frames.cuda(), captions.cuda()\n",
        "                # Pad captions to a consistent length\n",
        "                padded_captions = F.pad(captions, (0, max_len - captions.shape[1]), value=tokenizer.pad_token_id)\n",
        "                inputs = padded_captions[:, :-1].cuda()  # All tokens except the last\n",
        "                targets = padded_captions[:, 1:].cuda()  # All tokens except the first\n",
        "\n",
        "                with autocast():\n",
        "                    # Forward pass\n",
        "                    outputs = model(frames, inputs)\n",
        "\n",
        "                    # Ensure targets are aligned with the outputs\n",
        "                    if targets.shape[1] > outputs.shape[1]:\n",
        "                        targets = targets[:, :outputs.shape[1]]\n",
        "\n",
        "                    # Prepare the mask\n",
        "                    mask = (inputs != tokenizer.pad_token_id).float()[:, :outputs.shape[1]].cuda()\n",
        "\n",
        "                    if torch.isnan(outputs).any():\n",
        "                        print(\"Nan deteced in model outputs\")\n",
        "\n",
        "                    # Calculate loss\n",
        "                    loss = sequence_loss(outputs, targets, mask) / accumulation_steps\n",
        "\n",
        "                    if torch.isnan(loss).any():\n",
        "                        print(\"Nan deteced in loss\")\n",
        "\n",
        "                if phase == \"train\":\n",
        "                    # Backpropagation\n",
        "                    optimizer.zero_grad()\n",
        "                    scaler.scale(loss).backward()\n",
        "\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "                    if (step + 1) % accumulation_steps == 0:\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "                        optimizer.zero_grad()\n",
        "\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                # Log metrics\n",
        "                print(f\"Epoch {epoch}, Phase {phase}, Batch {batch_num}, Batch {batch_index + 1}/{total_batches}, Loss: {loss.item()}\")\n",
        "\n",
        "            # Save checkpoints periodically\n",
        "            if phase == \"train\" and batch_num % 100 == 0:\n",
        "                torch.save(model.state_dict(), f\"model_epoch_{epoch}_batch_{batch_num}.pt\")\n",
        "\n",
        "        # Logging at the end of each phase\n",
        "        if phase == \"val\":\n",
        "            val_loss /= 1573\n",
        "            val_accuracy /= 1573\n",
        "            print(f\"Epoch {epoch}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "            writer.add_scalar('Loss/val', val_loss, epoch)\n",
        "            writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
        "        elif phase == \"train\":\n",
        "            writer.add_scalar('Loss/train', loss.item(), epoch)\n",
        "\n",
        "        print(f\"Epoch {epoch}, Phase {phase} completed.\")\n",
        "    scheduler.step()\n",
        "\n",
        "# Saving final model\n",
        "torch.save(model.state_dict(), \"final_model.pt\")\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "ISvTTku43jRM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd58182a-d10d-4f29-88c4-e29885f38fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Phase train, Batch 858, Batch 1/1259, Loss: 5.160451412200928\n",
            "Epoch 0, Phase train, Batch 858, Batch 1/1259, Loss: 5.162769794464111\n",
            "Epoch 0, Phase train, Batch 1289, Batch 2/1259, Loss: 4.974930763244629\n",
            "Epoch 0, Phase train, Batch 1289, Batch 2/1259, Loss: 4.968714714050293\n",
            "Epoch 0, Phase train, Batch 1324, Batch 3/1259, Loss: 4.888542652130127\n",
            "Epoch 0, Phase train, Batch 1324, Batch 3/1259, Loss: 4.837698459625244\n",
            "Epoch 0, Phase train, Batch 1242, Batch 4/1259, Loss: 4.749791622161865\n",
            "Epoch 0, Phase train, Batch 1242, Batch 4/1259, Loss: 4.786004066467285\n",
            "Epoch 0, Phase train, Batch 1566, Batch 5/1259, Loss: 4.717471122741699\n",
            "Epoch 0, Phase train, Batch 1566, Batch 5/1259, Loss: 4.725526809692383\n",
            "Epoch 0, Phase train, Batch 741, Batch 6/1259, Loss: 4.695276737213135\n",
            "Epoch 0, Phase train, Batch 741, Batch 6/1259, Loss: 4.675205230712891\n",
            "Epoch 0, Phase train, Batch 985, Batch 7/1259, Loss: 4.625764846801758\n",
            "Epoch 0, Phase train, Batch 985, Batch 7/1259, Loss: 4.621814727783203\n",
            "Epoch 0, Phase train, Batch 358, Batch 8/1259, Loss: 4.58424186706543\n",
            "Epoch 0, Phase train, Batch 358, Batch 8/1259, Loss: 4.624622344970703\n",
            "Epoch 0, Phase train, Batch 1116, Batch 9/1259, Loss: 4.538355350494385\n",
            "Epoch 0, Phase train, Batch 1116, Batch 9/1259, Loss: 4.542969703674316\n",
            "Epoch 0, Phase train, Batch 920, Batch 10/1259, Loss: 4.464626312255859\n",
            "Epoch 0, Phase train, Batch 920, Batch 10/1259, Loss: 4.521992206573486\n",
            "Epoch 0, Phase train, Batch 282, Batch 11/1259, Loss: 4.4414381980896\n",
            "Epoch 0, Phase train, Batch 282, Batch 11/1259, Loss: 4.463678359985352\n",
            "Epoch 0, Phase train, Batch 569, Batch 12/1259, Loss: 4.468766689300537\n",
            "Epoch 0, Phase train, Batch 569, Batch 12/1259, Loss: 4.421420097351074\n",
            "Epoch 0, Phase train, Batch 175, Batch 13/1259, Loss: 4.423035144805908\n",
            "Epoch 0, Phase train, Batch 175, Batch 13/1259, Loss: 4.373476505279541\n",
            "Epoch 0, Phase train, Batch 84, Batch 14/1259, Loss: 4.458652973175049\n",
            "Epoch 0, Phase train, Batch 84, Batch 14/1259, Loss: 4.3570966720581055\n",
            "Epoch 0, Phase train, Batch 1093, Batch 15/1259, Loss: 4.335729598999023\n",
            "Epoch 0, Phase train, Batch 1093, Batch 15/1259, Loss: 4.389504909515381\n",
            "Epoch 0, Phase train, Batch 404, Batch 16/1259, Loss: 4.29239559173584\n",
            "Epoch 0, Phase train, Batch 404, Batch 16/1259, Loss: 4.295472621917725\n",
            "Epoch 0, Phase train, Batch 613, Batch 17/1259, Loss: 4.2060418128967285\n",
            "Epoch 0, Phase train, Batch 613, Batch 17/1259, Loss: 4.26616907119751\n",
            "Epoch 0, Phase train, Batch 1088, Batch 18/1259, Loss: 4.178776264190674\n",
            "Epoch 0, Phase train, Batch 1088, Batch 18/1259, Loss: 4.255499839782715\n",
            "Epoch 0, Phase train, Batch 1033, Batch 19/1259, Loss: 4.175478458404541\n",
            "Epoch 0, Phase train, Batch 1033, Batch 19/1259, Loss: 4.1372833251953125\n",
            "Epoch 0, Phase train, Batch 1484, Batch 20/1259, Loss: 4.131178379058838\n",
            "Epoch 0, Phase train, Batch 1484, Batch 20/1259, Loss: 4.134335041046143\n",
            "Epoch 0, Phase train, Batch 622, Batch 21/1259, Loss: 4.113475322723389\n",
            "Epoch 0, Phase train, Batch 622, Batch 21/1259, Loss: 4.1490702629089355\n",
            "Epoch 0, Phase train, Batch 645, Batch 22/1259, Loss: 4.073427677154541\n",
            "Epoch 0, Phase train, Batch 645, Batch 22/1259, Loss: 4.077903747558594\n",
            "Epoch 0, Phase train, Batch 815, Batch 23/1259, Loss: 4.0017991065979\n",
            "Epoch 0, Phase train, Batch 815, Batch 23/1259, Loss: 3.9629709720611572\n",
            "Epoch 0, Phase train, Batch 1217, Batch 24/1259, Loss: 4.017516613006592\n",
            "Epoch 0, Phase train, Batch 1217, Batch 24/1259, Loss: 3.9702203273773193\n",
            "Epoch 0, Phase train, Batch 1071, Batch 25/1259, Loss: 3.936293125152588\n",
            "Epoch 0, Phase train, Batch 1071, Batch 25/1259, Loss: 3.98982310295105\n",
            "Epoch 0, Phase train, Batch 773, Batch 26/1259, Loss: 3.894094944000244\n",
            "Epoch 0, Phase train, Batch 773, Batch 26/1259, Loss: 3.9291093349456787\n",
            "Epoch 0, Phase train, Batch 807, Batch 27/1259, Loss: 3.9176831245422363\n",
            "Epoch 0, Phase train, Batch 807, Batch 27/1259, Loss: 3.796877861022949\n",
            "Epoch 0, Phase train, Batch 1150, Batch 28/1259, Loss: 3.8116252422332764\n",
            "Epoch 0, Phase train, Batch 1150, Batch 28/1259, Loss: 3.79763126373291\n",
            "Epoch 0, Phase train, Batch 557, Batch 29/1259, Loss: 3.827077627182007\n",
            "Epoch 0, Phase train, Batch 557, Batch 29/1259, Loss: 3.821261405944824\n",
            "Epoch 0, Phase train, Batch 782, Batch 30/1259, Loss: 3.694603204727173\n",
            "Epoch 0, Phase train, Batch 782, Batch 30/1259, Loss: 3.7340474128723145\n",
            "Epoch 0, Phase train, Batch 638, Batch 31/1259, Loss: 3.724616050720215\n",
            "Epoch 0, Phase train, Batch 638, Batch 31/1259, Loss: 3.6714658737182617\n",
            "Epoch 0, Phase train, Batch 1555, Batch 32/1259, Loss: 3.597412586212158\n",
            "Epoch 0, Phase train, Batch 1555, Batch 32/1259, Loss: 3.6608757972717285\n",
            "Epoch 0, Phase train, Batch 1061, Batch 33/1259, Loss: 3.6392757892608643\n",
            "Epoch 0, Phase train, Batch 1061, Batch 33/1259, Loss: 3.612596035003662\n",
            "Epoch 0, Phase train, Batch 118, Batch 34/1259, Loss: 3.5799081325531006\n",
            "Epoch 0, Phase train, Batch 118, Batch 34/1259, Loss: 3.6027238368988037\n",
            "Epoch 0, Phase train, Batch 575, Batch 35/1259, Loss: 3.584599494934082\n",
            "Epoch 0, Phase train, Batch 575, Batch 35/1259, Loss: 3.5940606594085693\n",
            "Epoch 0, Phase train, Batch 102, Batch 36/1259, Loss: 3.5477712154388428\n",
            "Epoch 0, Phase train, Batch 102, Batch 36/1259, Loss: 3.55708646774292\n",
            "Epoch 0, Phase train, Batch 1041, Batch 37/1259, Loss: 3.534886360168457\n",
            "Epoch 0, Phase train, Batch 1041, Batch 37/1259, Loss: 3.4860923290252686\n",
            "Epoch 0, Phase train, Batch 420, Batch 38/1259, Loss: 3.3717713356018066\n",
            "Epoch 0, Phase train, Batch 420, Batch 38/1259, Loss: 3.4674808979034424\n",
            "Epoch 0, Phase train, Batch 530, Batch 39/1259, Loss: 3.3491244316101074\n",
            "Epoch 0, Phase train, Batch 530, Batch 39/1259, Loss: 3.4413580894470215\n",
            "Epoch 0, Phase train, Batch 199, Batch 40/1259, Loss: 3.4144835472106934\n",
            "Epoch 0, Phase train, Batch 199, Batch 40/1259, Loss: 3.400705337524414\n",
            "Epoch 0, Phase train, Batch 517, Batch 41/1259, Loss: 3.358086347579956\n",
            "Epoch 0, Phase train, Batch 517, Batch 41/1259, Loss: 3.3611502647399902\n",
            "Epoch 0, Phase train, Batch 60, Batch 42/1259, Loss: 3.2767255306243896\n",
            "Epoch 0, Phase train, Batch 60, Batch 42/1259, Loss: 3.5184717178344727\n",
            "Epoch 0, Phase train, Batch 893, Batch 43/1259, Loss: 3.283773899078369\n",
            "Epoch 0, Phase train, Batch 893, Batch 43/1259, Loss: 3.267765522003174\n",
            "Epoch 0, Phase train, Batch 905, Batch 44/1259, Loss: 3.20627498626709\n",
            "Epoch 0, Phase train, Batch 905, Batch 44/1259, Loss: 3.210080146789551\n",
            "Epoch 0, Phase train, Batch 171, Batch 45/1259, Loss: 3.2492756843566895\n",
            "Epoch 0, Phase train, Batch 171, Batch 45/1259, Loss: 3.269536018371582\n",
            "Epoch 0, Phase train, Batch 1098, Batch 46/1259, Loss: 3.195509195327759\n",
            "Epoch 0, Phase train, Batch 1098, Batch 46/1259, Loss: 3.1976613998413086\n",
            "Epoch 0, Phase train, Batch 332, Batch 47/1259, Loss: 3.26503849029541\n",
            "Epoch 0, Phase train, Batch 332, Batch 47/1259, Loss: 3.1667416095733643\n",
            "Epoch 0, Phase train, Batch 669, Batch 48/1259, Loss: 3.1431217193603516\n",
            "Epoch 0, Phase train, Batch 669, Batch 48/1259, Loss: 3.16969633102417\n",
            "Epoch 0, Phase train, Batch 482, Batch 49/1259, Loss: 3.136650562286377\n",
            "Epoch 0, Phase train, Batch 482, Batch 49/1259, Loss: 3.0270798206329346\n",
            "Epoch 0, Phase train, Batch 301, Batch 50/1259, Loss: 3.101475238800049\n",
            "Epoch 0, Phase train, Batch 301, Batch 50/1259, Loss: 3.019319534301758\n",
            "Epoch 0, Phase train, Batch 426, Batch 51/1259, Loss: 2.988311767578125\n",
            "Epoch 0, Phase train, Batch 426, Batch 51/1259, Loss: 3.0768449306488037\n",
            "Epoch 0, Phase train, Batch 949, Batch 52/1259, Loss: 3.0266799926757812\n",
            "Epoch 0, Phase train, Batch 949, Batch 52/1259, Loss: 3.0277442932128906\n",
            "Epoch 0, Phase train, Batch 1206, Batch 53/1259, Loss: 2.9812023639678955\n",
            "Epoch 0, Phase train, Batch 1206, Batch 53/1259, Loss: 3.081378698348999\n",
            "Epoch 0, Phase train, Batch 328, Batch 54/1259, Loss: 2.9563701152801514\n",
            "Epoch 0, Phase train, Batch 328, Batch 54/1259, Loss: 2.9827754497528076\n",
            "Epoch 0, Phase train, Batch 924, Batch 55/1259, Loss: 3.0319948196411133\n",
            "Epoch 0, Phase train, Batch 924, Batch 55/1259, Loss: 2.923398733139038\n",
            "Epoch 0, Phase train, Batch 455, Batch 56/1259, Loss: 2.953272581100464\n",
            "Epoch 0, Phase train, Batch 455, Batch 56/1259, Loss: 3.008643388748169\n",
            "Epoch 0, Phase train, Batch 836, Batch 57/1259, Loss: 3.052164316177368\n",
            "Epoch 0, Phase train, Batch 836, Batch 57/1259, Loss: 3.11086106300354\n",
            "Epoch 0, Phase train, Batch 536, Batch 58/1259, Loss: 2.945235252380371\n",
            "Epoch 0, Phase train, Batch 536, Batch 58/1259, Loss: 2.89823317527771\n",
            "Epoch 0, Phase train, Batch 595, Batch 59/1259, Loss: 3.004458427429199\n",
            "Epoch 0, Phase train, Batch 595, Batch 59/1259, Loss: 2.9610660076141357\n",
            "Epoch 0, Phase train, Batch 1010, Batch 60/1259, Loss: 3.007390260696411\n",
            "Epoch 0, Phase train, Batch 1010, Batch 60/1259, Loss: 3.0490195751190186\n",
            "Epoch 0, Phase train, Batch 362, Batch 61/1259, Loss: 3.0026254653930664\n",
            "Epoch 0, Phase train, Batch 362, Batch 61/1259, Loss: 2.8995747566223145\n",
            "Epoch 0, Phase train, Batch 1480, Batch 62/1259, Loss: 2.892824411392212\n",
            "Epoch 0, Phase train, Batch 1480, Batch 62/1259, Loss: 2.834174633026123\n",
            "Epoch 0, Phase train, Batch 27, Batch 63/1259, Loss: 3.0760552883148193\n",
            "Epoch 0, Phase train, Batch 27, Batch 63/1259, Loss: 3.0359089374542236\n",
            "Epoch 0, Phase train, Batch 959, Batch 64/1259, Loss: 2.9700300693511963\n",
            "Epoch 0, Phase train, Batch 959, Batch 64/1259, Loss: 2.9151771068573\n",
            "Epoch 0, Phase train, Batch 1267, Batch 65/1259, Loss: 2.8045709133148193\n",
            "Epoch 0, Phase train, Batch 1267, Batch 65/1259, Loss: 2.8393967151641846\n",
            "Epoch 0, Phase train, Batch 1003, Batch 66/1259, Loss: 2.8680484294891357\n",
            "Epoch 0, Phase train, Batch 1003, Batch 66/1259, Loss: 2.892871618270874\n",
            "Epoch 0, Phase train, Batch 935, Batch 67/1259, Loss: 2.9494688510894775\n",
            "Epoch 0, Phase train, Batch 935, Batch 67/1259, Loss: 2.893033266067505\n",
            "Epoch 0, Phase train, Batch 484, Batch 68/1259, Loss: 2.725839138031006\n",
            "Epoch 0, Phase train, Batch 484, Batch 68/1259, Loss: 2.7796311378479004\n",
            "Epoch 0, Phase train, Batch 110, Batch 69/1259, Loss: 2.8411900997161865\n",
            "Epoch 0, Phase train, Batch 110, Batch 69/1259, Loss: 3.008819341659546\n",
            "Epoch 0, Phase train, Batch 68, Batch 70/1259, Loss: 2.9121484756469727\n",
            "Epoch 0, Phase train, Batch 68, Batch 70/1259, Loss: 2.900604248046875\n",
            "Epoch 0, Phase train, Batch 188, Batch 71/1259, Loss: 2.8210737705230713\n",
            "Epoch 0, Phase train, Batch 188, Batch 71/1259, Loss: 2.8534960746765137\n",
            "Epoch 0, Phase train, Batch 968, Batch 72/1259, Loss: 2.782042980194092\n",
            "Epoch 0, Phase train, Batch 968, Batch 72/1259, Loss: 2.7783219814300537\n",
            "Epoch 0, Phase train, Batch 586, Batch 73/1259, Loss: 2.9077582359313965\n",
            "Epoch 0, Phase train, Batch 586, Batch 73/1259, Loss: 2.9042880535125732\n",
            "Epoch 0, Phase train, Batch 1248, Batch 74/1259, Loss: 2.7079148292541504\n",
            "Epoch 0, Phase train, Batch 1248, Batch 74/1259, Loss: 2.599250316619873\n",
            "Epoch 0, Phase train, Batch 834, Batch 75/1259, Loss: 2.8466386795043945\n",
            "Epoch 0, Phase train, Batch 834, Batch 75/1259, Loss: 2.903337240219116\n",
            "Epoch 0, Phase train, Batch 1141, Batch 76/1259, Loss: 2.851203441619873\n",
            "Epoch 0, Phase train, Batch 1141, Batch 76/1259, Loss: 2.8936877250671387\n",
            "Epoch 0, Phase train, Batch 923, Batch 77/1259, Loss: 2.8787004947662354\n",
            "Epoch 0, Phase train, Batch 923, Batch 77/1259, Loss: 3.113858699798584\n",
            "Epoch 0, Phase train, Batch 643, Batch 78/1259, Loss: 2.8698315620422363\n",
            "Epoch 0, Phase train, Batch 643, Batch 78/1259, Loss: 2.8581783771514893\n",
            "Epoch 0, Phase train, Batch 597, Batch 79/1259, Loss: 2.964749574661255\n",
            "Epoch 0, Phase train, Batch 597, Batch 79/1259, Loss: 2.8799853324890137\n",
            "Epoch 0, Phase train, Batch 388, Batch 80/1259, Loss: 2.83821439743042\n",
            "Epoch 0, Phase train, Batch 388, Batch 80/1259, Loss: 2.7381646633148193\n",
            "Epoch 0, Phase train, Batch 1163, Batch 81/1259, Loss: 2.7398502826690674\n",
            "Epoch 0, Phase train, Batch 1163, Batch 81/1259, Loss: 2.937940835952759\n",
            "Epoch 0, Phase train, Batch 1170, Batch 82/1259, Loss: 2.81839656829834\n",
            "Epoch 0, Phase train, Batch 1170, Batch 82/1259, Loss: 2.7907919883728027\n",
            "Epoch 0, Phase train, Batch 1416, Batch 83/1259, Loss: 2.7622170448303223\n",
            "Epoch 0, Phase train, Batch 1416, Batch 83/1259, Loss: 2.719653367996216\n",
            "Epoch 0, Phase train, Batch 642, Batch 84/1259, Loss: 2.703010320663452\n",
            "Epoch 0, Phase train, Batch 642, Batch 84/1259, Loss: 2.711968421936035\n",
            "Epoch 0, Phase train, Batch 1107, Batch 85/1259, Loss: 2.7278614044189453\n",
            "Epoch 0, Phase train, Batch 1107, Batch 85/1259, Loss: 2.736611843109131\n",
            "Epoch 0, Phase train, Batch 269, Batch 86/1259, Loss: 2.8612682819366455\n",
            "Epoch 0, Phase train, Batch 269, Batch 86/1259, Loss: 2.8415074348449707\n",
            "Epoch 0, Phase train, Batch 748, Batch 87/1259, Loss: 2.772952079772949\n",
            "Epoch 0, Phase train, Batch 748, Batch 87/1259, Loss: 2.7027108669281006\n",
            "Epoch 0, Phase train, Batch 370, Batch 88/1259, Loss: 2.8173916339874268\n",
            "Epoch 0, Phase train, Batch 370, Batch 88/1259, Loss: 2.8191893100738525\n",
            "Epoch 0, Phase train, Batch 1485, Batch 89/1259, Loss: 2.694505214691162\n",
            "Epoch 0, Phase train, Batch 1485, Batch 89/1259, Loss: 2.785994291305542\n",
            "Epoch 0, Phase train, Batch 196, Batch 90/1259, Loss: 2.795281171798706\n",
            "Epoch 0, Phase train, Batch 196, Batch 90/1259, Loss: 2.9060280323028564\n",
            "Epoch 0, Phase train, Batch 1184, Batch 91/1259, Loss: 2.8192038536071777\n",
            "Epoch 0, Phase train, Batch 1184, Batch 91/1259, Loss: 2.6827712059020996\n",
            "Epoch 0, Phase train, Batch 1389, Batch 92/1259, Loss: 2.738960027694702\n",
            "Epoch 0, Phase train, Batch 1389, Batch 92/1259, Loss: 2.7988507747650146\n",
            "Epoch 0, Phase train, Batch 1158, Batch 93/1259, Loss: 2.8008882999420166\n",
            "Epoch 0, Phase train, Batch 1158, Batch 93/1259, Loss: 2.732174873352051\n",
            "Epoch 0, Phase train, Batch 1549, Batch 94/1259, Loss: 2.6929640769958496\n",
            "Epoch 0, Phase train, Batch 1549, Batch 94/1259, Loss: 2.787353277206421\n",
            "Epoch 0, Phase train, Batch 394, Batch 95/1259, Loss: 2.7723026275634766\n",
            "Epoch 0, Phase train, Batch 394, Batch 95/1259, Loss: 2.7845709323883057\n",
            "Epoch 0, Phase train, Batch 803, Batch 96/1259, Loss: 2.8323192596435547\n",
            "Epoch 0, Phase train, Batch 803, Batch 96/1259, Loss: 2.6998512744903564\n",
            "Epoch 0, Phase train, Batch 194, Batch 97/1259, Loss: 2.797450542449951\n",
            "Epoch 0, Phase train, Batch 194, Batch 97/1259, Loss: 2.8071837425231934\n",
            "Epoch 0, Phase train, Batch 813, Batch 98/1259, Loss: 2.592543601989746\n",
            "Epoch 0, Phase train, Batch 813, Batch 98/1259, Loss: 2.70560884475708\n",
            "Epoch 0, Phase train, Batch 623, Batch 99/1259, Loss: 2.7461836338043213\n",
            "Epoch 0, Phase train, Batch 623, Batch 99/1259, Loss: 2.7291736602783203\n",
            "Epoch 0, Phase train, Batch 567, Batch 100/1259, Loss: 2.893044948577881\n",
            "Epoch 0, Phase train, Batch 567, Batch 100/1259, Loss: 2.782595634460449\n",
            "Epoch 0, Phase train, Batch 43, Batch 101/1259, Loss: 2.714571952819824\n",
            "Epoch 0, Phase train, Batch 43, Batch 101/1259, Loss: 2.821350336074829\n",
            "Epoch 0, Phase train, Batch 1081, Batch 102/1259, Loss: 2.732619047164917\n",
            "Epoch 0, Phase train, Batch 1081, Batch 102/1259, Loss: 2.7228739261627197\n",
            "Epoch 0, Phase train, Batch 1220, Batch 103/1259, Loss: 2.7634341716766357\n",
            "Epoch 0, Phase train, Batch 1220, Batch 103/1259, Loss: 2.6033856868743896\n",
            "Epoch 0, Phase train, Batch 686, Batch 104/1259, Loss: 2.9130237102508545\n",
            "Epoch 0, Phase train, Batch 686, Batch 104/1259, Loss: 2.6962642669677734\n",
            "Epoch 0, Phase train, Batch 13, Batch 105/1259, Loss: 2.8432514667510986\n",
            "Epoch 0, Phase train, Batch 13, Batch 105/1259, Loss: 2.8174564838409424\n",
            "Epoch 0, Phase train, Batch 719, Batch 106/1259, Loss: 2.6949820518493652\n",
            "Epoch 0, Phase train, Batch 719, Batch 106/1259, Loss: 2.7586748600006104\n",
            "Epoch 0, Phase train, Batch 637, Batch 107/1259, Loss: 2.7655131816864014\n",
            "Epoch 0, Phase train, Batch 637, Batch 107/1259, Loss: 2.761474847793579\n",
            "Epoch 0, Phase train, Batch 746, Batch 108/1259, Loss: 2.6667263507843018\n",
            "Epoch 0, Phase train, Batch 746, Batch 108/1259, Loss: 2.8642218112945557\n",
            "Epoch 0, Phase train, Batch 720, Batch 109/1259, Loss: 2.7215449810028076\n",
            "Epoch 0, Phase train, Batch 720, Batch 109/1259, Loss: 2.70585560798645\n",
            "Epoch 0, Phase train, Batch 1351, Batch 110/1259, Loss: 2.5402863025665283\n",
            "Epoch 0, Phase train, Batch 1351, Batch 110/1259, Loss: 2.575070381164551\n",
            "Epoch 0, Phase train, Batch 23, Batch 111/1259, Loss: 2.7989821434020996\n",
            "Epoch 0, Phase train, Batch 23, Batch 111/1259, Loss: 2.801438331604004\n",
            "Epoch 0, Phase train, Batch 943, Batch 112/1259, Loss: 2.679697275161743\n",
            "Epoch 0, Phase train, Batch 943, Batch 112/1259, Loss: 2.673811435699463\n",
            "Epoch 0, Phase train, Batch 1273, Batch 113/1259, Loss: 2.5517921447753906\n",
            "Epoch 0, Phase train, Batch 1273, Batch 113/1259, Loss: 2.544501304626465\n",
            "Epoch 0, Phase train, Batch 891, Batch 114/1259, Loss: 2.675104856491089\n",
            "Epoch 0, Phase train, Batch 891, Batch 114/1259, Loss: 2.7376158237457275\n",
            "Epoch 0, Phase train, Batch 447, Batch 115/1259, Loss: 2.646699905395508\n",
            "Epoch 0, Phase train, Batch 447, Batch 115/1259, Loss: 2.5688023567199707\n",
            "Epoch 0, Phase train, Batch 457, Batch 116/1259, Loss: 2.668527364730835\n",
            "Epoch 0, Phase train, Batch 457, Batch 116/1259, Loss: 2.6417899131774902\n",
            "Epoch 0, Phase train, Batch 617, Batch 117/1259, Loss: 2.708467960357666\n",
            "Epoch 0, Phase train, Batch 617, Batch 117/1259, Loss: 2.6526906490325928\n",
            "Epoch 0, Phase train, Batch 1131, Batch 118/1259, Loss: 2.7398359775543213\n",
            "Epoch 0, Phase train, Batch 1131, Batch 118/1259, Loss: 2.776693105697632\n",
            "Epoch 0, Phase train, Batch 1068, Batch 119/1259, Loss: 2.7129290103912354\n",
            "Epoch 0, Phase train, Batch 1068, Batch 119/1259, Loss: 2.574683427810669\n",
            "Epoch 0, Phase train, Batch 1379, Batch 120/1259, Loss: 2.619206428527832\n",
            "Epoch 0, Phase train, Batch 1379, Batch 120/1259, Loss: 2.613645553588867\n",
            "Epoch 0, Phase train, Batch 2, Batch 121/1259, Loss: 2.8651628494262695\n",
            "Epoch 0, Phase train, Batch 2, Batch 121/1259, Loss: 2.7877321243286133\n",
            "Epoch 0, Phase train, Batch 146, Batch 122/1259, Loss: 2.5827691555023193\n",
            "Epoch 0, Phase train, Batch 146, Batch 122/1259, Loss: 2.6284148693084717\n",
            "Epoch 0, Phase train, Batch 55, Batch 123/1259, Loss: 2.8023464679718018\n",
            "Epoch 0, Phase train, Batch 55, Batch 123/1259, Loss: 2.698762893676758\n",
            "Epoch 0, Phase train, Batch 7, Batch 124/1259, Loss: 2.6766815185546875\n",
            "Epoch 0, Phase train, Batch 7, Batch 124/1259, Loss: 2.699686050415039\n",
            "Epoch 0, Phase train, Batch 109, Batch 125/1259, Loss: 2.7105541229248047\n",
            "Epoch 0, Phase train, Batch 109, Batch 125/1259, Loss: 2.6938910484313965\n",
            "Epoch 0, Phase train, Batch 1510, Batch 126/1259, Loss: 2.653897285461426\n",
            "Epoch 0, Phase train, Batch 1510, Batch 126/1259, Loss: 2.6245696544647217\n",
            "Epoch 0, Phase train, Batch 1240, Batch 127/1259, Loss: 2.4879562854766846\n",
            "Epoch 0, Phase train, Batch 1240, Batch 127/1259, Loss: 2.5190796852111816\n",
            "Epoch 0, Phase train, Batch 1040, Batch 128/1259, Loss: 2.660628318786621\n",
            "Epoch 0, Phase train, Batch 1040, Batch 128/1259, Loss: 2.659668445587158\n",
            "Epoch 0, Phase train, Batch 1101, Batch 129/1259, Loss: 2.624540328979492\n",
            "Epoch 0, Phase train, Batch 1101, Batch 129/1259, Loss: 2.619965076446533\n",
            "Epoch 0, Phase train, Batch 1444, Batch 130/1259, Loss: 2.6108174324035645\n",
            "Epoch 0, Phase train, Batch 1444, Batch 130/1259, Loss: 2.6017372608184814\n",
            "Epoch 0, Phase train, Batch 1348, Batch 131/1259, Loss: 2.504176139831543\n",
            "Epoch 0, Phase train, Batch 1348, Batch 131/1259, Loss: 2.514132499694824\n",
            "Epoch 0, Phase train, Batch 1404, Batch 132/1259, Loss: 2.6550076007843018\n",
            "Epoch 0, Phase train, Batch 1404, Batch 132/1259, Loss: 2.6741631031036377\n",
            "Epoch 0, Phase train, Batch 1271, Batch 133/1259, Loss: 2.5801899433135986\n",
            "Epoch 0, Phase train, Batch 1271, Batch 133/1259, Loss: 2.5138134956359863\n",
            "Epoch 0, Phase train, Batch 1398, Batch 134/1259, Loss: 2.6173503398895264\n",
            "Epoch 0, Phase train, Batch 1398, Batch 134/1259, Loss: 2.4880318641662598\n",
            "Epoch 0, Phase train, Batch 65, Batch 135/1259, Loss: 2.752307891845703\n",
            "Epoch 0, Phase train, Batch 65, Batch 135/1259, Loss: 2.687734365463257\n",
            "Epoch 0, Phase train, Batch 1312, Batch 136/1259, Loss: 2.5410449504852295\n",
            "Epoch 0, Phase train, Batch 1312, Batch 136/1259, Loss: 2.536348819732666\n",
            "Epoch 0, Phase train, Batch 1122, Batch 137/1259, Loss: 2.625189781188965\n",
            "Epoch 0, Phase train, Batch 1122, Batch 137/1259, Loss: 2.4761598110198975\n",
            "Epoch 0, Phase train, Batch 763, Batch 138/1259, Loss: 2.6338019371032715\n",
            "Epoch 0, Phase train, Batch 763, Batch 138/1259, Loss: 2.6327879428863525\n",
            "Epoch 0, Phase train, Batch 1004, Batch 139/1259, Loss: 2.6207261085510254\n",
            "Epoch 0, Phase train, Batch 1004, Batch 139/1259, Loss: 2.5858500003814697\n",
            "Epoch 0, Phase train, Batch 1539, Batch 140/1259, Loss: 2.619598865509033\n",
            "Epoch 0, Phase train, Batch 1539, Batch 140/1259, Loss: 2.6683270931243896\n",
            "Epoch 0, Phase train, Batch 722, Batch 141/1259, Loss: 2.7280936241149902\n",
            "Epoch 0, Phase train, Batch 722, Batch 141/1259, Loss: 2.641622304916382\n",
            "Epoch 0, Phase train, Batch 698, Batch 142/1259, Loss: 2.661774158477783\n",
            "Epoch 0, Phase train, Batch 698, Batch 142/1259, Loss: 2.619107961654663\n",
            "Epoch 0, Phase train, Batch 1314, Batch 143/1259, Loss: 2.549224376678467\n",
            "Epoch 0, Phase train, Batch 1314, Batch 143/1259, Loss: 2.595730781555176\n",
            "Epoch 0, Phase train, Batch 849, Batch 144/1259, Loss: 2.5628373622894287\n",
            "Epoch 0, Phase train, Batch 849, Batch 144/1259, Loss: 2.643134355545044\n",
            "Epoch 0, Phase train, Batch 614, Batch 145/1259, Loss: 2.971811532974243\n",
            "Epoch 0, Phase train, Batch 614, Batch 145/1259, Loss: 3.055453062057495\n",
            "Epoch 0, Phase train, Batch 1120, Batch 146/1259, Loss: 2.7060184478759766\n",
            "Epoch 0, Phase train, Batch 1120, Batch 146/1259, Loss: 2.616529941558838\n",
            "Epoch 0, Phase train, Batch 1181, Batch 147/1259, Loss: 2.4563255310058594\n",
            "Epoch 0, Phase train, Batch 1181, Batch 147/1259, Loss: 2.5573573112487793\n",
            "Epoch 0, Phase train, Batch 1431, Batch 148/1259, Loss: 2.5796658992767334\n",
            "Epoch 0, Phase train, Batch 1431, Batch 148/1259, Loss: 2.56258487701416\n",
            "Epoch 0, Phase train, Batch 207, Batch 149/1259, Loss: 2.598076581954956\n",
            "Epoch 0, Phase train, Batch 207, Batch 149/1259, Loss: 2.665437698364258\n",
            "Epoch 0, Phase train, Batch 239, Batch 150/1259, Loss: 2.7047297954559326\n",
            "Epoch 0, Phase train, Batch 239, Batch 150/1259, Loss: 2.691452980041504\n",
            "Epoch 0, Phase train, Batch 1543, Batch 151/1259, Loss: 2.6189072132110596\n",
            "Epoch 0, Phase train, Batch 1543, Batch 151/1259, Loss: 2.5463809967041016\n",
            "Epoch 0, Phase train, Batch 939, Batch 152/1259, Loss: 2.802178382873535\n",
            "Epoch 0, Phase train, Batch 939, Batch 152/1259, Loss: 2.52827525138855\n",
            "Epoch 0, Phase train, Batch 1381, Batch 153/1259, Loss: 2.64579701423645\n",
            "Epoch 0, Phase train, Batch 1381, Batch 153/1259, Loss: 2.468764066696167\n",
            "Epoch 0, Phase train, Batch 494, Batch 154/1259, Loss: 2.5364127159118652\n",
            "Epoch 0, Phase train, Batch 494, Batch 154/1259, Loss: 2.561450481414795\n",
            "Epoch 0, Phase train, Batch 369, Batch 155/1259, Loss: 2.5819222927093506\n",
            "Epoch 0, Phase train, Batch 369, Batch 155/1259, Loss: 2.48303484916687\n",
            "Epoch 0, Phase train, Batch 951, Batch 156/1259, Loss: 2.5499114990234375\n",
            "Epoch 0, Phase train, Batch 951, Batch 156/1259, Loss: 2.5595850944519043\n",
            "Epoch 0, Phase train, Batch 148, Batch 157/1259, Loss: 2.56699538230896\n",
            "Epoch 0, Phase train, Batch 148, Batch 157/1259, Loss: 2.6236343383789062\n",
            "Epoch 0, Phase train, Batch 1186, Batch 158/1259, Loss: 2.5537467002868652\n",
            "Epoch 0, Phase train, Batch 1186, Batch 158/1259, Loss: 2.577521324157715\n",
            "Epoch 0, Phase train, Batch 644, Batch 159/1259, Loss: 2.598472833633423\n",
            "Epoch 0, Phase train, Batch 644, Batch 159/1259, Loss: 2.5423498153686523\n",
            "Epoch 0, Phase train, Batch 950, Batch 160/1259, Loss: 2.5995700359344482\n",
            "Epoch 0, Phase train, Batch 950, Batch 160/1259, Loss: 2.5776491165161133\n",
            "Epoch 0, Phase train, Batch 157, Batch 161/1259, Loss: 2.641298294067383\n",
            "Epoch 0, Phase train, Batch 157, Batch 161/1259, Loss: 2.4417316913604736\n",
            "Epoch 0, Phase train, Batch 124, Batch 162/1259, Loss: 2.642768383026123\n",
            "Epoch 0, Phase train, Batch 124, Batch 162/1259, Loss: 2.618764877319336\n",
            "Epoch 0, Phase train, Batch 1199, Batch 163/1259, Loss: 2.4675495624542236\n",
            "Epoch 0, Phase train, Batch 1199, Batch 163/1259, Loss: 2.478597402572632\n",
            "Epoch 0, Phase train, Batch 450, Batch 164/1259, Loss: 2.399556875228882\n",
            "Epoch 0, Phase train, Batch 450, Batch 164/1259, Loss: 2.512056589126587\n",
            "Epoch 0, Phase train, Batch 728, Batch 165/1259, Loss: 2.5862972736358643\n",
            "Epoch 0, Phase train, Batch 728, Batch 165/1259, Loss: 2.6779625415802\n",
            "Epoch 0, Phase train, Batch 1562, Batch 166/1259, Loss: 2.442960023880005\n",
            "Epoch 0, Phase train, Batch 1562, Batch 166/1259, Loss: 2.4464516639709473\n",
            "Epoch 0, Phase train, Batch 1202, Batch 167/1259, Loss: 2.4711031913757324\n",
            "Epoch 0, Phase train, Batch 1202, Batch 167/1259, Loss: 2.456268787384033\n",
            "Epoch 0, Phase train, Batch 1174, Batch 168/1259, Loss: 2.441652774810791\n",
            "Epoch 0, Phase train, Batch 1174, Batch 168/1259, Loss: 2.52645206451416\n",
            "Epoch 0, Phase train, Batch 1037, Batch 169/1259, Loss: 2.5205180644989014\n",
            "Epoch 0, Phase train, Batch 1037, Batch 169/1259, Loss: 2.6239960193634033\n",
            "Epoch 0, Phase train, Batch 1434, Batch 170/1259, Loss: 2.4959261417388916\n",
            "Epoch 0, Phase train, Batch 1434, Batch 170/1259, Loss: 2.5202934741973877\n",
            "Epoch 0, Phase train, Batch 1435, Batch 171/1259, Loss: 2.467057704925537\n",
            "Epoch 0, Phase train, Batch 1435, Batch 171/1259, Loss: 2.496598243713379\n",
            "Epoch 0, Phase train, Batch 1423, Batch 172/1259, Loss: 2.6113386154174805\n",
            "Epoch 0, Phase train, Batch 1423, Batch 172/1259, Loss: 2.5012481212615967\n",
            "Epoch 0, Phase train, Batch 1430, Batch 173/1259, Loss: 2.568774938583374\n",
            "Epoch 0, Phase train, Batch 1430, Batch 173/1259, Loss: 2.5423898696899414\n",
            "Epoch 0, Phase train, Batch 685, Batch 174/1259, Loss: 2.480351448059082\n",
            "Epoch 0, Phase train, Batch 685, Batch 174/1259, Loss: 2.4665608406066895\n",
            "Epoch 0, Phase train, Batch 218, Batch 175/1259, Loss: 2.5768840312957764\n",
            "Epoch 0, Phase train, Batch 218, Batch 175/1259, Loss: 2.618917465209961\n",
            "Epoch 0, Phase train, Batch 189, Batch 176/1259, Loss: 2.576107978820801\n",
            "Epoch 0, Phase train, Batch 189, Batch 176/1259, Loss: 2.517540216445923\n",
            "Epoch 0, Phase train, Batch 47, Batch 177/1259, Loss: 2.797987461090088\n",
            "Epoch 0, Phase train, Batch 47, Batch 177/1259, Loss: 2.5665016174316406\n",
            "Epoch 0, Phase train, Batch 1218, Batch 178/1259, Loss: 2.5208632946014404\n",
            "Epoch 0, Phase train, Batch 1218, Batch 178/1259, Loss: 2.603937864303589\n",
            "Epoch 0, Phase train, Batch 116, Batch 179/1259, Loss: 2.6750850677490234\n",
            "Epoch 0, Phase train, Batch 116, Batch 179/1259, Loss: 2.659468173980713\n",
            "Epoch 0, Phase train, Batch 238, Batch 180/1259, Loss: 2.4932727813720703\n",
            "Epoch 0, Phase train, Batch 238, Batch 180/1259, Loss: 2.4009604454040527\n",
            "Epoch 0, Phase train, Batch 526, Batch 181/1259, Loss: 2.4972755908966064\n",
            "Epoch 0, Phase train, Batch 526, Batch 181/1259, Loss: 2.458880662918091\n",
            "Epoch 0, Phase train, Batch 26, Batch 182/1259, Loss: 2.709782123565674\n",
            "Epoch 0, Phase train, Batch 26, Batch 182/1259, Loss: 2.4472320079803467\n",
            "Epoch 0, Phase train, Batch 1326, Batch 183/1259, Loss: 2.5567564964294434\n",
            "Epoch 0, Phase train, Batch 1326, Batch 183/1259, Loss: 2.4985756874084473\n",
            "Epoch 0, Phase train, Batch 900, Batch 184/1259, Loss: 2.6711573600769043\n",
            "Epoch 0, Phase train, Batch 900, Batch 184/1259, Loss: 2.5088915824890137\n",
            "Epoch 0, Phase train, Batch 130, Batch 185/1259, Loss: 2.7262723445892334\n",
            "Epoch 0, Phase train, Batch 130, Batch 185/1259, Loss: 2.5854568481445312\n",
            "Epoch 0, Phase train, Batch 1232, Batch 186/1259, Loss: 2.4839463233947754\n",
            "Epoch 0, Phase train, Batch 1232, Batch 186/1259, Loss: 2.333667039871216\n",
            "Epoch 0, Phase train, Batch 870, Batch 187/1259, Loss: 2.3288443088531494\n",
            "Epoch 0, Phase train, Batch 870, Batch 187/1259, Loss: 2.560990810394287\n",
            "Epoch 0, Phase train, Batch 1482, Batch 188/1259, Loss: 2.4984028339385986\n",
            "Epoch 0, Phase train, Batch 1482, Batch 188/1259, Loss: 2.3510022163391113\n",
            "Epoch 0, Phase train, Batch 1083, Batch 189/1259, Loss: 2.4902632236480713\n",
            "Epoch 0, Phase train, Batch 1083, Batch 189/1259, Loss: 2.5524253845214844\n",
            "Epoch 0, Phase train, Batch 334, Batch 190/1259, Loss: 2.6209909915924072\n",
            "Epoch 0, Phase train, Batch 334, Batch 190/1259, Loss: 2.5419793128967285\n",
            "Epoch 0, Phase train, Batch 435, Batch 191/1259, Loss: 2.4814605712890625\n",
            "Epoch 0, Phase train, Batch 435, Batch 191/1259, Loss: 2.4558849334716797\n",
            "Epoch 0, Phase train, Batch 889, Batch 192/1259, Loss: 2.3896141052246094\n",
            "Epoch 0, Phase train, Batch 889, Batch 192/1259, Loss: 2.4908530712127686\n",
            "Epoch 0, Phase train, Batch 119, Batch 193/1259, Loss: 2.421203136444092\n",
            "Epoch 0, Phase train, Batch 119, Batch 193/1259, Loss: 2.539560317993164\n",
            "Epoch 0, Phase train, Batch 1293, Batch 194/1259, Loss: 2.3676083087921143\n",
            "Epoch 0, Phase train, Batch 1293, Batch 194/1259, Loss: 2.462247371673584\n",
            "Epoch 0, Phase train, Batch 1125, Batch 195/1259, Loss: 2.446566581726074\n",
            "Epoch 0, Phase train, Batch 1125, Batch 195/1259, Loss: 2.5115954875946045\n",
            "Epoch 0, Phase train, Batch 507, Batch 196/1259, Loss: 2.69856595993042\n",
            "Epoch 0, Phase train, Batch 507, Batch 196/1259, Loss: 2.5610013008117676\n",
            "Epoch 0, Phase train, Batch 581, Batch 197/1259, Loss: 2.5372321605682373\n",
            "Epoch 0, Phase train, Batch 581, Batch 197/1259, Loss: 2.639145851135254\n",
            "Epoch 0, Phase train, Batch 1474, Batch 198/1259, Loss: 2.5284621715545654\n",
            "Epoch 0, Phase train, Batch 1474, Batch 198/1259, Loss: 2.5927178859710693\n",
            "Epoch 0, Phase train, Batch 651, Batch 199/1259, Loss: 2.4742000102996826\n",
            "Epoch 0, Phase train, Batch 651, Batch 199/1259, Loss: 2.574904680252075\n",
            "Epoch 0, Phase train, Batch 1355, Batch 200/1259, Loss: 2.4162254333496094\n",
            "Epoch 0, Phase train, Batch 1355, Batch 200/1259, Loss: 2.3702852725982666\n",
            "Epoch 0, Phase train, Batch 1352, Batch 201/1259, Loss: 2.4334964752197266\n",
            "Epoch 0, Phase train, Batch 1352, Batch 201/1259, Loss: 2.300670862197876\n",
            "Epoch 0, Phase train, Batch 141, Batch 202/1259, Loss: 2.457942485809326\n",
            "Epoch 0, Phase train, Batch 141, Batch 202/1259, Loss: 2.5094847679138184\n",
            "Epoch 0, Phase train, Batch 940, Batch 203/1259, Loss: 2.495046615600586\n",
            "Epoch 0, Phase train, Batch 940, Batch 203/1259, Loss: 2.5542502403259277\n",
            "Epoch 0, Phase train, Batch 619, Batch 204/1259, Loss: 2.4829702377319336\n",
            "Epoch 0, Phase train, Batch 619, Batch 204/1259, Loss: 2.5232901573181152\n",
            "Epoch 0, Phase train, Batch 1042, Batch 205/1259, Loss: 2.5387625694274902\n",
            "Epoch 0, Phase train, Batch 1042, Batch 205/1259, Loss: 2.5968973636627197\n",
            "Epoch 0, Phase train, Batch 1056, Batch 206/1259, Loss: 2.4768869876861572\n",
            "Epoch 0, Phase train, Batch 1056, Batch 206/1259, Loss: 2.4852542877197266\n",
            "Epoch 0, Phase train, Batch 1139, Batch 207/1259, Loss: 2.4881439208984375\n",
            "Epoch 0, Phase train, Batch 1139, Batch 207/1259, Loss: 2.48766827583313\n",
            "Epoch 0, Phase train, Batch 778, Batch 208/1259, Loss: 2.6012723445892334\n",
            "Epoch 0, Phase train, Batch 778, Batch 208/1259, Loss: 2.542407274246216\n",
            "Epoch 0, Phase train, Batch 942, Batch 209/1259, Loss: 2.4592177867889404\n",
            "Epoch 0, Phase train, Batch 942, Batch 209/1259, Loss: 2.419943332672119\n",
            "Epoch 0, Phase train, Batch 320, Batch 210/1259, Loss: 2.3662021160125732\n",
            "Epoch 0, Phase train, Batch 320, Batch 210/1259, Loss: 2.393019199371338\n",
            "Epoch 0, Phase train, Batch 670, Batch 211/1259, Loss: 2.4714910984039307\n",
            "Epoch 0, Phase train, Batch 670, Batch 211/1259, Loss: 2.4299557209014893\n",
            "Epoch 0, Phase train, Batch 252, Batch 212/1259, Loss: 2.599860191345215\n",
            "Epoch 0, Phase train, Batch 252, Batch 212/1259, Loss: 2.4532406330108643\n",
            "Epoch 0, Phase train, Batch 1177, Batch 213/1259, Loss: 2.3265743255615234\n",
            "Epoch 0, Phase train, Batch 1177, Batch 213/1259, Loss: 2.4511210918426514\n",
            "Epoch 0, Phase train, Batch 1105, Batch 214/1259, Loss: 2.552293062210083\n",
            "Epoch 0, Phase train, Batch 1105, Batch 214/1259, Loss: 2.3963122367858887\n",
            "Epoch 0, Phase train, Batch 1085, Batch 215/1259, Loss: 2.364313840866089\n",
            "Epoch 0, Phase train, Batch 1085, Batch 215/1259, Loss: 2.4116055965423584\n",
            "Epoch 0, Phase train, Batch 327, Batch 216/1259, Loss: 2.2972161769866943\n",
            "Epoch 0, Phase train, Batch 327, Batch 216/1259, Loss: 2.6198890209198\n",
            "Epoch 0, Phase train, Batch 350, Batch 217/1259, Loss: 2.3197946548461914\n",
            "Epoch 0, Phase train, Batch 350, Batch 217/1259, Loss: 2.3797523975372314\n",
            "Epoch 0, Phase train, Batch 814, Batch 218/1259, Loss: 2.3705954551696777\n",
            "Epoch 0, Phase train, Batch 814, Batch 218/1259, Loss: 2.454221487045288\n",
            "Epoch 0, Phase train, Batch 1067, Batch 219/1259, Loss: 2.4045114517211914\n",
            "Epoch 0, Phase train, Batch 1067, Batch 219/1259, Loss: 2.373955249786377\n",
            "Epoch 0, Phase train, Batch 753, Batch 220/1259, Loss: 2.4144742488861084\n",
            "Epoch 0, Phase train, Batch 753, Batch 220/1259, Loss: 2.407028913497925\n",
            "Epoch 0, Phase train, Batch 990, Batch 221/1259, Loss: 2.3868801593780518\n",
            "Epoch 0, Phase train, Batch 990, Batch 221/1259, Loss: 2.472097158432007\n",
            "Epoch 0, Phase train, Batch 1238, Batch 222/1259, Loss: 2.268778085708618\n",
            "Epoch 0, Phase train, Batch 1238, Batch 222/1259, Loss: 2.3192710876464844\n",
            "Epoch 0, Phase train, Batch 1494, Batch 223/1259, Loss: 2.2912232875823975\n",
            "Epoch 0, Phase train, Batch 1494, Batch 223/1259, Loss: 2.2579400539398193\n",
            "Epoch 0, Phase train, Batch 869, Batch 224/1259, Loss: 2.564556360244751\n",
            "Epoch 0, Phase train, Batch 869, Batch 224/1259, Loss: 2.6166915893554688\n",
            "Epoch 0, Phase train, Batch 296, Batch 225/1259, Loss: 2.507652759552002\n",
            "Epoch 0, Phase train, Batch 296, Batch 225/1259, Loss: 2.4933855533599854\n",
            "Epoch 0, Phase train, Batch 258, Batch 226/1259, Loss: 2.3651278018951416\n",
            "Epoch 0, Phase train, Batch 258, Batch 226/1259, Loss: 2.408367395401001\n",
            "Epoch 0, Phase train, Batch 1335, Batch 227/1259, Loss: 2.2930989265441895\n",
            "Epoch 0, Phase train, Batch 1335, Batch 227/1259, Loss: 2.2308719158172607\n",
            "Epoch 0, Phase train, Batch 1468, Batch 228/1259, Loss: 2.519085168838501\n",
            "Epoch 0, Phase train, Batch 1468, Batch 228/1259, Loss: 2.51578688621521\n",
            "Epoch 0, Phase train, Batch 143, Batch 229/1259, Loss: 2.4756319522857666\n",
            "Epoch 0, Phase train, Batch 143, Batch 229/1259, Loss: 2.6732730865478516\n",
            "Epoch 0, Phase train, Batch 410, Batch 230/1259, Loss: 2.4918460845947266\n",
            "Epoch 0, Phase train, Batch 410, Batch 230/1259, Loss: 2.529663324356079\n",
            "Epoch 0, Phase train, Batch 811, Batch 231/1259, Loss: 2.4243104457855225\n",
            "Epoch 0, Phase train, Batch 811, Batch 231/1259, Loss: 2.3207602500915527\n",
            "Epoch 0, Phase train, Batch 993, Batch 232/1259, Loss: 2.3077142238616943\n",
            "Epoch 0, Phase train, Batch 993, Batch 232/1259, Loss: 2.419583559036255\n",
            "Epoch 0, Phase train, Batch 678, Batch 233/1259, Loss: 2.4271090030670166\n",
            "Epoch 0, Phase train, Batch 678, Batch 233/1259, Loss: 2.524493932723999\n",
            "Epoch 0, Phase train, Batch 302, Batch 234/1259, Loss: 2.4486095905303955\n",
            "Epoch 0, Phase train, Batch 302, Batch 234/1259, Loss: 2.4193336963653564\n",
            "Epoch 0, Phase train, Batch 1070, Batch 235/1259, Loss: 2.3699567317962646\n",
            "Epoch 0, Phase train, Batch 1070, Batch 235/1259, Loss: 2.368640184402466\n",
            "Epoch 0, Phase train, Batch 438, Batch 236/1259, Loss: 2.2461097240448\n",
            "Epoch 0, Phase train, Batch 438, Batch 236/1259, Loss: 2.4568557739257812\n",
            "Epoch 0, Phase train, Batch 1229, Batch 237/1259, Loss: 2.333190679550171\n",
            "Epoch 0, Phase train, Batch 1229, Batch 237/1259, Loss: 2.3964927196502686\n",
            "Epoch 0, Phase train, Batch 1364, Batch 238/1259, Loss: 2.332500457763672\n",
            "Epoch 0, Phase train, Batch 1364, Batch 238/1259, Loss: 2.2581708431243896\n",
            "Epoch 0, Phase train, Batch 150, Batch 239/1259, Loss: 2.476738452911377\n",
            "Epoch 0, Phase train, Batch 150, Batch 239/1259, Loss: 2.513075113296509\n",
            "Epoch 0, Phase train, Batch 1118, Batch 240/1259, Loss: 2.32541561126709\n",
            "Epoch 0, Phase train, Batch 1118, Batch 240/1259, Loss: 2.391558885574341\n",
            "Epoch 0, Phase train, Batch 469, Batch 241/1259, Loss: 2.286478281021118\n",
            "Epoch 0, Phase train, Batch 469, Batch 241/1259, Loss: 2.2030694484710693\n",
            "Epoch 0, Phase train, Batch 1009, Batch 242/1259, Loss: 2.3994548320770264\n",
            "Epoch 0, Phase train, Batch 1009, Batch 242/1259, Loss: 2.4457316398620605\n",
            "Epoch 0, Phase train, Batch 1203, Batch 243/1259, Loss: 2.3333640098571777\n",
            "Epoch 0, Phase train, Batch 1203, Batch 243/1259, Loss: 2.380401372909546\n",
            "Epoch 0, Phase train, Batch 529, Batch 244/1259, Loss: 2.3368425369262695\n",
            "Epoch 0, Phase train, Batch 529, Batch 244/1259, Loss: 2.200423240661621\n",
            "Epoch 0, Phase train, Batch 1115, Batch 245/1259, Loss: 2.398925304412842\n",
            "Epoch 0, Phase train, Batch 1115, Batch 245/1259, Loss: 2.632152795791626\n",
            "Epoch 0, Phase train, Batch 766, Batch 246/1259, Loss: 2.4592080116271973\n",
            "Epoch 0, Phase train, Batch 766, Batch 246/1259, Loss: 2.328643798828125\n",
            "Epoch 0, Phase train, Batch 1134, Batch 247/1259, Loss: 2.44173526763916\n",
            "Epoch 0, Phase train, Batch 1134, Batch 247/1259, Loss: 2.313274383544922\n",
            "Epoch 0, Phase train, Batch 352, Batch 248/1259, Loss: 2.376624822616577\n",
            "Epoch 0, Phase train, Batch 352, Batch 248/1259, Loss: 2.318002223968506\n",
            "Epoch 0, Phase train, Batch 547, Batch 249/1259, Loss: 2.4740781784057617\n",
            "Epoch 0, Phase train, Batch 547, Batch 249/1259, Loss: 2.3969812393188477\n",
            "Epoch 0, Phase train, Batch 1557, Batch 250/1259, Loss: 2.336196184158325\n",
            "Epoch 0, Phase train, Batch 1557, Batch 250/1259, Loss: 2.404876708984375\n",
            "Epoch 0, Phase train, Batch 823, Batch 251/1259, Loss: 2.362626314163208\n",
            "Epoch 0, Phase train, Batch 823, Batch 251/1259, Loss: 2.393766164779663\n",
            "Epoch 0, Phase train, Batch 1065, Batch 252/1259, Loss: 2.3512182235717773\n",
            "Epoch 0, Phase train, Batch 1065, Batch 252/1259, Loss: 2.3462255001068115\n",
            "Epoch 0, Phase train, Batch 1298, Batch 253/1259, Loss: 2.374569892883301\n",
            "Epoch 0, Phase train, Batch 1298, Batch 253/1259, Loss: 2.365602493286133\n",
            "Epoch 0, Phase train, Batch 375, Batch 254/1259, Loss: 2.4779021739959717\n",
            "Epoch 0, Phase train, Batch 375, Batch 254/1259, Loss: 2.357682704925537\n",
            "Epoch 0, Phase train, Batch 1440, Batch 255/1259, Loss: 2.3914949893951416\n",
            "Epoch 0, Phase train, Batch 1440, Batch 255/1259, Loss: 2.3413479328155518\n",
            "Epoch 0, Phase train, Batch 289, Batch 256/1259, Loss: 2.550149440765381\n",
            "Epoch 0, Phase train, Batch 289, Batch 256/1259, Loss: 2.3688693046569824\n",
            "Epoch 0, Phase train, Batch 681, Batch 257/1259, Loss: 2.2946887016296387\n",
            "Epoch 0, Phase train, Batch 681, Batch 257/1259, Loss: 2.52435564994812\n",
            "Epoch 0, Phase train, Batch 18, Batch 258/1259, Loss: 2.491328239440918\n",
            "Epoch 0, Phase train, Batch 18, Batch 258/1259, Loss: 2.326174736022949\n",
            "Epoch 0, Phase train, Batch 861, Batch 259/1259, Loss: 2.300577163696289\n",
            "Epoch 0, Phase train, Batch 861, Batch 259/1259, Loss: 2.3058855533599854\n",
            "Epoch 0, Phase train, Batch 1436, Batch 260/1259, Loss: 2.4256949424743652\n",
            "Epoch 0, Phase train, Batch 1436, Batch 260/1259, Loss: 2.356492280960083\n",
            "Epoch 0, Phase train, Batch 1260, Batch 261/1259, Loss: 2.213782787322998\n",
            "Epoch 0, Phase train, Batch 1260, Batch 261/1259, Loss: 2.3485615253448486\n",
            "Epoch 0, Phase train, Batch 33, Batch 262/1259, Loss: 2.5924456119537354\n",
            "Epoch 0, Phase train, Batch 33, Batch 262/1259, Loss: 2.748250722885132\n",
            "Epoch 0, Phase train, Batch 1258, Batch 263/1259, Loss: 2.3794920444488525\n",
            "Epoch 0, Phase train, Batch 1258, Batch 263/1259, Loss: 2.284698009490967\n",
            "Epoch 0, Phase train, Batch 1302, Batch 264/1259, Loss: 2.2741730213165283\n",
            "Epoch 0, Phase train, Batch 1302, Batch 264/1259, Loss: 2.4247958660125732\n",
            "Epoch 0, Phase train, Batch 304, Batch 265/1259, Loss: 2.3058557510375977\n",
            "Epoch 0, Phase train, Batch 304, Batch 265/1259, Loss: 2.4607760906219482\n",
            "Epoch 0, Phase train, Batch 191, Batch 266/1259, Loss: 2.3984928131103516\n",
            "Epoch 0, Phase train, Batch 191, Batch 266/1259, Loss: 2.4563655853271484\n",
            "Epoch 0, Phase train, Batch 1045, Batch 267/1259, Loss: 2.417074203491211\n",
            "Epoch 0, Phase train, Batch 1045, Batch 267/1259, Loss: 2.335188150405884\n",
            "Epoch 0, Phase train, Batch 862, Batch 268/1259, Loss: 2.2383456230163574\n",
            "Epoch 0, Phase train, Batch 862, Batch 268/1259, Loss: 2.331664562225342\n",
            "Epoch 0, Phase train, Batch 145, Batch 269/1259, Loss: 2.354421615600586\n",
            "Epoch 0, Phase train, Batch 145, Batch 269/1259, Loss: 2.420828104019165\n",
            "Epoch 0, Phase train, Batch 240, Batch 270/1259, Loss: 2.442408323287964\n",
            "Epoch 0, Phase train, Batch 240, Batch 270/1259, Loss: 2.4102699756622314\n",
            "Epoch 0, Phase train, Batch 500, Batch 271/1259, Loss: 2.332807779312134\n",
            "Epoch 0, Phase train, Batch 500, Batch 271/1259, Loss: 2.273198366165161\n",
            "Epoch 0, Phase train, Batch 389, Batch 272/1259, Loss: 2.425016164779663\n",
            "Epoch 0, Phase train, Batch 389, Batch 272/1259, Loss: 2.3101766109466553\n",
            "Epoch 0, Phase train, Batch 1266, Batch 273/1259, Loss: 2.432018518447876\n",
            "Epoch 0, Phase train, Batch 1266, Batch 273/1259, Loss: 2.2974588871002197\n",
            "Epoch 0, Phase train, Batch 397, Batch 274/1259, Loss: 2.273751735687256\n",
            "Epoch 0, Phase train, Batch 397, Batch 274/1259, Loss: 2.2274909019470215\n",
            "Epoch 0, Phase train, Batch 325, Batch 275/1259, Loss: 2.363710880279541\n",
            "Epoch 0, Phase train, Batch 325, Batch 275/1259, Loss: 2.3944625854492188\n",
            "Epoch 0, Phase train, Batch 409, Batch 276/1259, Loss: 2.394749402999878\n",
            "Epoch 0, Phase train, Batch 409, Batch 276/1259, Loss: 2.23703932762146\n",
            "Epoch 0, Phase train, Batch 402, Batch 277/1259, Loss: 2.185823917388916\n",
            "Epoch 0, Phase train, Batch 402, Batch 277/1259, Loss: 2.326500654220581\n",
            "Epoch 0, Phase train, Batch 202, Batch 278/1259, Loss: 2.3753435611724854\n",
            "Epoch 0, Phase train, Batch 202, Batch 278/1259, Loss: 2.2514827251434326\n",
            "Epoch 0, Phase train, Batch 1209, Batch 279/1259, Loss: 2.2867982387542725\n",
            "Epoch 0, Phase train, Batch 1209, Batch 279/1259, Loss: 2.2352373600006104\n",
            "Epoch 0, Phase train, Batch 1369, Batch 280/1259, Loss: 2.347830057144165\n",
            "Epoch 0, Phase train, Batch 1369, Batch 280/1259, Loss: 2.30444073677063\n",
            "Epoch 0, Phase train, Batch 338, Batch 281/1259, Loss: 2.247141122817993\n",
            "Epoch 0, Phase train, Batch 338, Batch 281/1259, Loss: 2.278231382369995\n",
            "Epoch 0, Phase train, Batch 925, Batch 282/1259, Loss: 2.321671485900879\n",
            "Epoch 0, Phase train, Batch 925, Batch 282/1259, Loss: 2.3230533599853516\n",
            "Epoch 0, Phase train, Batch 125, Batch 283/1259, Loss: 2.4410011768341064\n",
            "Epoch 0, Phase train, Batch 125, Batch 283/1259, Loss: 2.4793667793273926\n",
            "Epoch 0, Phase train, Batch 793, Batch 284/1259, Loss: 2.3741965293884277\n",
            "Epoch 0, Phase train, Batch 793, Batch 284/1259, Loss: 2.292552947998047\n",
            "Epoch 0, Phase train, Batch 217, Batch 285/1259, Loss: 2.402312994003296\n",
            "Epoch 0, Phase train, Batch 217, Batch 285/1259, Loss: 2.286322593688965\n",
            "Epoch 0, Phase train, Batch 101, Batch 286/1259, Loss: 2.2537283897399902\n",
            "Epoch 0, Phase train, Batch 101, Batch 286/1259, Loss: 2.502551317214966\n",
            "Epoch 0, Phase train, Batch 491, Batch 287/1259, Loss: 2.3011670112609863\n",
            "Epoch 0, Phase train, Batch 491, Batch 287/1259, Loss: 2.251828908920288\n",
            "Epoch 0, Phase train, Batch 1428, Batch 288/1259, Loss: 2.262993574142456\n",
            "Epoch 0, Phase train, Batch 1428, Batch 288/1259, Loss: 2.2986621856689453\n",
            "Epoch 0, Phase train, Batch 525, Batch 289/1259, Loss: 2.227757215499878\n",
            "Epoch 0, Phase train, Batch 525, Batch 289/1259, Loss: 2.2596802711486816\n",
            "Epoch 0, Phase train, Batch 596, Batch 290/1259, Loss: 2.4631195068359375\n",
            "Epoch 0, Phase train, Batch 596, Batch 290/1259, Loss: 2.357415199279785\n",
            "Epoch 0, Phase train, Batch 542, Batch 291/1259, Loss: 2.5061428546905518\n",
            "Epoch 0, Phase train, Batch 542, Batch 291/1259, Loss: 2.4392945766448975\n",
            "Epoch 0, Phase train, Batch 62, Batch 292/1259, Loss: 2.437074899673462\n",
            "Epoch 0, Phase train, Batch 62, Batch 292/1259, Loss: 2.483511209487915\n",
            "Epoch 0, Phase train, Batch 977, Batch 293/1259, Loss: 2.33646297454834\n",
            "Epoch 0, Phase train, Batch 977, Batch 293/1259, Loss: 2.221320867538452\n",
            "Epoch 0, Phase train, Batch 1031, Batch 294/1259, Loss: 2.235725164413452\n",
            "Epoch 0, Phase train, Batch 1031, Batch 294/1259, Loss: 2.3215248584747314\n",
            "Epoch 0, Phase train, Batch 1411, Batch 295/1259, Loss: 2.3167147636413574\n",
            "Epoch 0, Phase train, Batch 1411, Batch 295/1259, Loss: 2.3656749725341797\n",
            "Epoch 0, Phase train, Batch 1388, Batch 296/1259, Loss: 2.355055093765259\n",
            "Epoch 0, Phase train, Batch 1388, Batch 296/1259, Loss: 2.2190322875976562\n",
            "Epoch 0, Phase train, Batch 885, Batch 297/1259, Loss: 2.302281618118286\n",
            "Epoch 0, Phase train, Batch 885, Batch 297/1259, Loss: 2.296529769897461\n",
            "Epoch 0, Phase train, Batch 795, Batch 298/1259, Loss: 2.281282901763916\n",
            "Epoch 0, Phase train, Batch 795, Batch 298/1259, Loss: 2.348123788833618\n",
            "Epoch 0, Phase train, Batch 1368, Batch 299/1259, Loss: 2.192070245742798\n",
            "Epoch 0, Phase train, Batch 1368, Batch 299/1259, Loss: 2.3154001235961914\n",
            "Epoch 0, Phase train, Batch 1126, Batch 300/1259, Loss: 2.4286997318267822\n",
            "Epoch 0, Phase train, Batch 1126, Batch 300/1259, Loss: 2.2205653190612793\n",
            "Epoch 0, Phase train, Batch 1528, Batch 301/1259, Loss: 2.4835712909698486\n",
            "Epoch 0, Phase train, Batch 1528, Batch 301/1259, Loss: 2.434549331665039\n",
            "Epoch 0, Phase train, Batch 121, Batch 302/1259, Loss: 2.2992186546325684\n",
            "Epoch 0, Phase train, Batch 121, Batch 302/1259, Loss: 2.371826648712158\n",
            "Epoch 0, Phase train, Batch 19, Batch 303/1259, Loss: 2.2604055404663086\n",
            "Epoch 0, Phase train, Batch 19, Batch 303/1259, Loss: 2.4222850799560547\n",
            "Epoch 0, Phase train, Batch 230, Batch 304/1259, Loss: 2.2289085388183594\n",
            "Epoch 0, Phase train, Batch 230, Batch 304/1259, Loss: 2.3260138034820557\n",
            "Epoch 0, Phase train, Batch 400, Batch 305/1259, Loss: 2.394624948501587\n",
            "Epoch 0, Phase train, Batch 400, Batch 305/1259, Loss: 2.282900094985962\n",
            "Epoch 0, Phase train, Batch 716, Batch 306/1259, Loss: 2.2688117027282715\n",
            "Epoch 0, Phase train, Batch 716, Batch 306/1259, Loss: 2.1989521980285645\n",
            "Epoch 0, Phase train, Batch 1334, Batch 307/1259, Loss: 2.3233585357666016\n",
            "Epoch 0, Phase train, Batch 1334, Batch 307/1259, Loss: 2.151505470275879\n",
            "Epoch 0, Phase train, Batch 1412, Batch 308/1259, Loss: 2.291940689086914\n",
            "Epoch 0, Phase train, Batch 1412, Batch 308/1259, Loss: 2.2284059524536133\n",
            "Epoch 0, Phase train, Batch 902, Batch 309/1259, Loss: 2.25240159034729\n",
            "Epoch 0, Phase train, Batch 902, Batch 309/1259, Loss: 2.473949909210205\n",
            "Epoch 0, Phase train, Batch 359, Batch 310/1259, Loss: 2.301225185394287\n",
            "Epoch 0, Phase train, Batch 359, Batch 310/1259, Loss: 2.2138991355895996\n",
            "Epoch 0, Phase train, Batch 37, Batch 311/1259, Loss: 2.355759620666504\n",
            "Epoch 0, Phase train, Batch 37, Batch 311/1259, Loss: 2.2846593856811523\n",
            "Epoch 0, Phase train, Batch 1027, Batch 312/1259, Loss: 2.3694777488708496\n",
            "Epoch 0, Phase train, Batch 1027, Batch 312/1259, Loss: 2.292289972305298\n",
            "Epoch 0, Phase train, Batch 703, Batch 313/1259, Loss: 2.2889060974121094\n",
            "Epoch 0, Phase train, Batch 703, Batch 313/1259, Loss: 2.362501382827759\n",
            "Epoch 0, Phase train, Batch 1064, Batch 314/1259, Loss: 2.19119930267334\n",
            "Epoch 0, Phase train, Batch 1064, Batch 314/1259, Loss: 2.233551263809204\n",
            "Epoch 0, Phase train, Batch 297, Batch 315/1259, Loss: 2.235106945037842\n",
            "Epoch 0, Phase train, Batch 297, Batch 315/1259, Loss: 2.269930362701416\n",
            "Epoch 0, Phase train, Batch 544, Batch 316/1259, Loss: 2.4409735202789307\n",
            "Epoch 0, Phase train, Batch 544, Batch 316/1259, Loss: 2.2373218536376953\n",
            "Epoch 0, Phase train, Batch 558, Batch 317/1259, Loss: 2.4521405696868896\n",
            "Epoch 0, Phase train, Batch 558, Batch 317/1259, Loss: 2.410548448562622\n",
            "Epoch 0, Phase train, Batch 383, Batch 318/1259, Loss: 2.21169114112854\n",
            "Epoch 0, Phase train, Batch 383, Batch 318/1259, Loss: 2.368943452835083\n",
            "Epoch 0, Phase train, Batch 80, Batch 319/1259, Loss: 2.2668373584747314\n",
            "Epoch 0, Phase train, Batch 80, Batch 319/1259, Loss: 2.3333959579467773\n",
            "Epoch 0, Phase train, Batch 539, Batch 320/1259, Loss: 2.3654985427856445\n",
            "Epoch 0, Phase train, Batch 539, Batch 320/1259, Loss: 2.363251209259033\n",
            "Epoch 0, Phase train, Batch 580, Batch 321/1259, Loss: 2.3454084396362305\n",
            "Epoch 0, Phase train, Batch 580, Batch 321/1259, Loss: 2.3720788955688477\n",
            "Epoch 0, Phase train, Batch 267, Batch 322/1259, Loss: 2.2179977893829346\n",
            "Epoch 0, Phase train, Batch 267, Batch 322/1259, Loss: 2.378279447555542\n",
            "Epoch 0, Phase train, Batch 399, Batch 323/1259, Loss: 2.181349754333496\n",
            "Epoch 0, Phase train, Batch 399, Batch 323/1259, Loss: 2.254503011703491\n",
            "Epoch 0, Phase train, Batch 1072, Batch 324/1259, Loss: 2.3840582370758057\n",
            "Epoch 0, Phase train, Batch 1072, Batch 324/1259, Loss: 2.3122336864471436\n",
            "Epoch 0, Phase train, Batch 553, Batch 325/1259, Loss: 2.2777352333068848\n",
            "Epoch 0, Phase train, Batch 553, Batch 325/1259, Loss: 2.432060718536377\n",
            "Epoch 0, Phase train, Batch 1560, Batch 326/1259, Loss: 2.143385171890259\n",
            "Epoch 0, Phase train, Batch 1560, Batch 326/1259, Loss: 2.2040820121765137\n",
            "Epoch 0, Phase train, Batch 975, Batch 327/1259, Loss: 2.269602060317993\n",
            "Epoch 0, Phase train, Batch 975, Batch 327/1259, Loss: 2.33901309967041\n",
            "Epoch 0, Phase train, Batch 152, Batch 328/1259, Loss: 2.3682539463043213\n",
            "Epoch 0, Phase train, Batch 152, Batch 328/1259, Loss: 2.359339714050293\n",
            "Epoch 0, Phase train, Batch 1502, Batch 329/1259, Loss: 2.2451305389404297\n",
            "Epoch 0, Phase train, Batch 1502, Batch 329/1259, Loss: 2.1839022636413574\n",
            "Epoch 0, Phase train, Batch 219, Batch 330/1259, Loss: 2.281614065170288\n",
            "Epoch 0, Phase train, Batch 219, Batch 330/1259, Loss: 2.3117177486419678\n",
            "Epoch 0, Phase train, Batch 1568, Batch 331/1259, Loss: 2.2648279666900635\n",
            "Epoch 0, Phase train, Batch 1568, Batch 331/1259, Loss: 2.296109676361084\n",
            "Epoch 0, Phase train, Batch 228, Batch 332/1259, Loss: 2.1843416690826416\n",
            "Epoch 0, Phase train, Batch 228, Batch 332/1259, Loss: 2.1112380027770996\n",
            "Epoch 0, Phase train, Batch 181, Batch 333/1259, Loss: 2.3114497661590576\n",
            "Epoch 0, Phase train, Batch 181, Batch 333/1259, Loss: 2.2329633235931396\n",
            "Epoch 0, Phase train, Batch 1029, Batch 334/1259, Loss: 2.1569342613220215\n",
            "Epoch 0, Phase train, Batch 1029, Batch 334/1259, Loss: 2.401275873184204\n",
            "Epoch 0, Phase train, Batch 390, Batch 335/1259, Loss: 2.299536943435669\n",
            "Epoch 0, Phase train, Batch 390, Batch 335/1259, Loss: 2.172131061553955\n",
            "Epoch 0, Phase train, Batch 806, Batch 336/1259, Loss: 2.315156936645508\n",
            "Epoch 0, Phase train, Batch 806, Batch 336/1259, Loss: 2.1350960731506348\n",
            "Epoch 0, Phase train, Batch 274, Batch 337/1259, Loss: 2.3539655208587646\n",
            "Epoch 0, Phase train, Batch 274, Batch 337/1259, Loss: 2.3801541328430176\n",
            "Epoch 0, Phase train, Batch 835, Batch 338/1259, Loss: 2.2316925525665283\n",
            "Epoch 0, Phase train, Batch 835, Batch 338/1259, Loss: 2.303046226501465\n",
            "Epoch 0, Phase train, Batch 1282, Batch 339/1259, Loss: 2.109126329421997\n",
            "Epoch 0, Phase train, Batch 1282, Batch 339/1259, Loss: 2.2921314239501953\n",
            "Epoch 0, Phase train, Batch 478, Batch 340/1259, Loss: 2.118082046508789\n",
            "Epoch 0, Phase train, Batch 478, Batch 340/1259, Loss: 2.16416072845459\n",
            "Epoch 0, Phase train, Batch 921, Batch 341/1259, Loss: 2.3580198287963867\n",
            "Epoch 0, Phase train, Batch 921, Batch 341/1259, Loss: 2.276838541030884\n",
            "Epoch 0, Phase train, Batch 1151, Batch 342/1259, Loss: 2.2391185760498047\n",
            "Epoch 0, Phase train, Batch 1151, Batch 342/1259, Loss: 2.446467876434326\n",
            "Epoch 0, Phase train, Batch 555, Batch 343/1259, Loss: 2.403806686401367\n",
            "Epoch 0, Phase train, Batch 555, Batch 343/1259, Loss: 2.4822256565093994\n",
            "Epoch 0, Phase train, Batch 249, Batch 344/1259, Loss: 2.2203752994537354\n",
            "Epoch 0, Phase train, Batch 249, Batch 344/1259, Loss: 2.2168197631835938\n",
            "Epoch 0, Phase train, Batch 857, Batch 345/1259, Loss: 2.3782236576080322\n",
            "Epoch 0, Phase train, Batch 857, Batch 345/1259, Loss: 2.2944869995117188\n",
            "Epoch 0, Phase train, Batch 69, Batch 346/1259, Loss: 2.2344348430633545\n",
            "Epoch 0, Phase train, Batch 69, Batch 346/1259, Loss: 2.3621883392333984\n",
            "Epoch 0, Phase train, Batch 886, Batch 347/1259, Loss: 2.1960291862487793\n",
            "Epoch 0, Phase train, Batch 886, Batch 347/1259, Loss: 2.23884916305542\n",
            "Epoch 0, Phase train, Batch 1367, Batch 348/1259, Loss: 2.2160255908966064\n",
            "Epoch 0, Phase train, Batch 1367, Batch 348/1259, Loss: 2.2421319484710693\n",
            "Epoch 0, Phase train, Batch 314, Batch 349/1259, Loss: 2.1685681343078613\n",
            "Epoch 0, Phase train, Batch 314, Batch 349/1259, Loss: 2.29640531539917\n",
            "Epoch 0, Phase train, Batch 909, Batch 350/1259, Loss: 2.4731411933898926\n",
            "Epoch 0, Phase train, Batch 909, Batch 350/1259, Loss: 2.161700487136841\n",
            "Epoch 0, Phase train, Batch 1108, Batch 351/1259, Loss: 2.1022849082946777\n",
            "Epoch 0, Phase train, Batch 1108, Batch 351/1259, Loss: 2.1948750019073486\n",
            "Epoch 0, Phase train, Batch 346, Batch 352/1259, Loss: 2.3282628059387207\n",
            "Epoch 0, Phase train, Batch 346, Batch 352/1259, Loss: 2.1484639644622803\n",
            "Epoch 0, Phase train, Batch 1534, Batch 353/1259, Loss: 2.151940107345581\n",
            "Epoch 0, Phase train, Batch 1534, Batch 353/1259, Loss: 2.19023060798645\n",
            "Epoch 0, Phase train, Batch 1323, Batch 354/1259, Loss: 2.213433265686035\n",
            "Epoch 0, Phase train, Batch 1323, Batch 354/1259, Loss: 2.200474739074707\n",
            "Epoch 0, Phase train, Batch 59, Batch 355/1259, Loss: 2.3195242881774902\n",
            "Epoch 0, Phase train, Batch 59, Batch 355/1259, Loss: 2.3212528228759766\n",
            "Epoch 0, Phase train, Batch 1424, Batch 356/1259, Loss: 2.2954623699188232\n",
            "Epoch 0, Phase train, Batch 1424, Batch 356/1259, Loss: 2.2507128715515137\n",
            "Epoch 0, Phase train, Batch 652, Batch 357/1259, Loss: 2.3793203830718994\n",
            "Epoch 0, Phase train, Batch 652, Batch 357/1259, Loss: 2.257864236831665\n",
            "Epoch 0, Phase train, Batch 1535, Batch 358/1259, Loss: 2.165113925933838\n",
            "Epoch 0, Phase train, Batch 1535, Batch 358/1259, Loss: 2.133774757385254\n",
            "Epoch 0, Phase train, Batch 1172, Batch 359/1259, Loss: 2.2860727310180664\n",
            "Epoch 0, Phase train, Batch 1172, Batch 359/1259, Loss: 2.1063337326049805\n",
            "Epoch 0, Phase train, Batch 700, Batch 360/1259, Loss: 2.2371273040771484\n",
            "Epoch 0, Phase train, Batch 700, Batch 360/1259, Loss: 2.1912355422973633\n",
            "Epoch 0, Phase train, Batch 1036, Batch 361/1259, Loss: 2.299163818359375\n",
            "Epoch 0, Phase train, Batch 1036, Batch 361/1259, Loss: 2.1679959297180176\n",
            "Epoch 0, Phase train, Batch 340, Batch 362/1259, Loss: 2.224747657775879\n",
            "Epoch 0, Phase train, Batch 340, Batch 362/1259, Loss: 2.2066409587860107\n",
            "Epoch 0, Phase train, Batch 129, Batch 363/1259, Loss: 2.3284153938293457\n",
            "Epoch 0, Phase train, Batch 129, Batch 363/1259, Loss: 2.2381155490875244\n",
            "Epoch 0, Phase train, Batch 916, Batch 364/1259, Loss: 2.288950204849243\n",
            "Epoch 0, Phase train, Batch 916, Batch 364/1259, Loss: 2.283200263977051\n",
            "Epoch 0, Phase train, Batch 1021, Batch 365/1259, Loss: 2.278486967086792\n",
            "Epoch 0, Phase train, Batch 1021, Batch 365/1259, Loss: 2.2036943435668945\n",
            "Epoch 0, Phase train, Batch 675, Batch 366/1259, Loss: 2.2304043769836426\n",
            "Epoch 0, Phase train, Batch 675, Batch 366/1259, Loss: 2.411869525909424\n",
            "Epoch 0, Phase train, Batch 839, Batch 367/1259, Loss: 2.3495888710021973\n",
            "Epoch 0, Phase train, Batch 839, Batch 367/1259, Loss: 2.281144857406616\n",
            "Epoch 0, Phase train, Batch 459, Batch 368/1259, Loss: 2.2227907180786133\n",
            "Epoch 0, Phase train, Batch 459, Batch 368/1259, Loss: 2.111032009124756\n",
            "Epoch 0, Phase train, Batch 71, Batch 369/1259, Loss: 2.276132583618164\n",
            "Epoch 0, Phase train, Batch 71, Batch 369/1259, Loss: 2.1177444458007812\n",
            "Epoch 0, Phase train, Batch 1231, Batch 370/1259, Loss: 2.1562886238098145\n",
            "Epoch 0, Phase train, Batch 1231, Batch 370/1259, Loss: 2.299126148223877\n",
            "Epoch 0, Phase train, Batch 180, Batch 371/1259, Loss: 2.3078668117523193\n",
            "Epoch 0, Phase train, Batch 180, Batch 371/1259, Loss: 2.4468255043029785\n",
            "Epoch 0, Phase train, Batch 135, Batch 372/1259, Loss: 2.114426612854004\n",
            "Epoch 0, Phase train, Batch 135, Batch 372/1259, Loss: 2.0862319469451904\n",
            "Epoch 0, Phase train, Batch 183, Batch 373/1259, Loss: 2.040022850036621\n",
            "Epoch 0, Phase train, Batch 183, Batch 373/1259, Loss: 2.3234827518463135\n",
            "Epoch 0, Phase train, Batch 520, Batch 374/1259, Loss: 2.120272636413574\n",
            "Epoch 0, Phase train, Batch 520, Batch 374/1259, Loss: 2.0922319889068604\n",
            "Epoch 0, Phase train, Batch 164, Batch 375/1259, Loss: 2.213824510574341\n",
            "Epoch 0, Phase train, Batch 164, Batch 375/1259, Loss: 2.142702102661133\n",
            "Epoch 0, Phase train, Batch 1303, Batch 376/1259, Loss: 2.1394166946411133\n",
            "Epoch 0, Phase train, Batch 1303, Batch 376/1259, Loss: 2.144780397415161\n",
            "Epoch 0, Phase train, Batch 618, Batch 377/1259, Loss: 2.287691593170166\n",
            "Epoch 0, Phase train, Batch 618, Batch 377/1259, Loss: 2.3138370513916016\n",
            "Epoch 0, Phase train, Batch 1023, Batch 378/1259, Loss: 2.1357202529907227\n",
            "Epoch 0, Phase train, Batch 1023, Batch 378/1259, Loss: 2.1709072589874268\n",
            "Epoch 0, Phase train, Batch 1196, Batch 379/1259, Loss: 2.124556541442871\n",
            "Epoch 0, Phase train, Batch 1196, Batch 379/1259, Loss: 2.085092067718506\n",
            "Epoch 0, Phase train, Batch 209, Batch 380/1259, Loss: 2.191873073577881\n",
            "Epoch 0, Phase train, Batch 209, Batch 380/1259, Loss: 2.10402250289917\n",
            "Epoch 0, Phase train, Batch 1408, Batch 381/1259, Loss: 2.0728676319122314\n",
            "Epoch 0, Phase train, Batch 1408, Batch 381/1259, Loss: 2.220311403274536\n",
            "Epoch 0, Phase train, Batch 1372, Batch 382/1259, Loss: 2.1703081130981445\n",
            "Epoch 0, Phase train, Batch 1372, Batch 382/1259, Loss: 2.1202585697174072\n",
            "Epoch 0, Phase train, Batch 805, Batch 383/1259, Loss: 2.231027126312256\n",
            "Epoch 0, Phase train, Batch 805, Batch 383/1259, Loss: 2.2347404956817627\n",
            "Epoch 0, Phase train, Batch 701, Batch 384/1259, Loss: 2.2057175636291504\n",
            "Epoch 0, Phase train, Batch 701, Batch 384/1259, Loss: 2.1540863513946533\n",
            "Epoch 0, Phase train, Batch 1233, Batch 385/1259, Loss: 2.153517007827759\n",
            "Epoch 0, Phase train, Batch 1233, Batch 385/1259, Loss: 2.0479493141174316\n",
            "Epoch 0, Phase train, Batch 1255, Batch 386/1259, Loss: 2.2140824794769287\n",
            "Epoch 0, Phase train, Batch 1255, Batch 386/1259, Loss: 2.0872697830200195\n",
            "Epoch 0, Phase train, Batch 837, Batch 387/1259, Loss: 2.217393159866333\n",
            "Epoch 0, Phase train, Batch 837, Batch 387/1259, Loss: 2.1205062866210938\n",
            "Epoch 0, Phase train, Batch 781, Batch 388/1259, Loss: 2.3171801567077637\n",
            "Epoch 0, Phase train, Batch 781, Batch 388/1259, Loss: 2.1347954273223877\n",
            "Epoch 0, Phase train, Batch 345, Batch 389/1259, Loss: 2.2809269428253174\n",
            "Epoch 0, Phase train, Batch 345, Batch 389/1259, Loss: 1.9943755865097046\n",
            "Epoch 0, Phase train, Batch 451, Batch 390/1259, Loss: 2.1793882846832275\n",
            "Epoch 0, Phase train, Batch 451, Batch 390/1259, Loss: 2.1632115840911865\n",
            "Epoch 0, Phase train, Batch 1448, Batch 391/1259, Loss: 2.1818454265594482\n",
            "Epoch 0, Phase train, Batch 1448, Batch 391/1259, Loss: 2.0727105140686035\n",
            "Epoch 0, Phase train, Batch 877, Batch 392/1259, Loss: 2.1548333168029785\n",
            "Epoch 0, Phase train, Batch 877, Batch 392/1259, Loss: 2.149019241333008\n",
            "Epoch 0, Phase train, Batch 611, Batch 393/1259, Loss: 2.1703786849975586\n",
            "Epoch 0, Phase train, Batch 611, Batch 393/1259, Loss: 2.2130861282348633\n",
            "Epoch 0, Phase train, Batch 61, Batch 394/1259, Loss: 2.3351526260375977\n",
            "Epoch 0, Phase train, Batch 61, Batch 394/1259, Loss: 2.250802516937256\n",
            "Epoch 0, Phase train, Batch 1176, Batch 395/1259, Loss: 2.1442742347717285\n",
            "Epoch 0, Phase train, Batch 1176, Batch 395/1259, Loss: 2.2339272499084473\n",
            "Epoch 0, Phase train, Batch 1377, Batch 396/1259, Loss: 2.043046236038208\n",
            "Epoch 0, Phase train, Batch 1377, Batch 396/1259, Loss: 2.004801034927368\n",
            "Epoch 0, Phase train, Batch 844, Batch 397/1259, Loss: 2.1652472019195557\n",
            "Epoch 0, Phase train, Batch 844, Batch 397/1259, Loss: 2.0442256927490234\n",
            "Epoch 0, Phase train, Batch 954, Batch 398/1259, Loss: 2.2411160469055176\n",
            "Epoch 0, Phase train, Batch 954, Batch 398/1259, Loss: 2.357424259185791\n",
            "Epoch 0, Phase train, Batch 221, Batch 399/1259, Loss: 2.3047502040863037\n",
            "Epoch 0, Phase train, Batch 221, Batch 399/1259, Loss: 2.2748165130615234\n",
            "Epoch 0, Phase train, Batch 1570, Batch 400/1259, Loss: 2.0279123783111572\n",
            "Epoch 0, Phase train, Batch 1570, Batch 400/1259, Loss: 2.202526330947876\n",
            "Epoch 0, Phase train, Batch 1301, Batch 401/1259, Loss: 2.0562632083892822\n",
            "Epoch 0, Phase train, Batch 1301, Batch 401/1259, Loss: 2.26035213470459\n",
            "Epoch 0, Phase train, Batch 997, Batch 402/1259, Loss: 2.1727077960968018\n",
            "Epoch 0, Phase train, Batch 997, Batch 402/1259, Loss: 1.9852219820022583\n",
            "Epoch 0, Phase train, Batch 312, Batch 403/1259, Loss: 2.264476776123047\n",
            "Epoch 0, Phase train, Batch 312, Batch 403/1259, Loss: 2.3073203563690186\n",
            "Epoch 0, Phase train, Batch 1300, Batch 404/1259, Loss: 1.9905778169631958\n",
            "Epoch 0, Phase train, Batch 1300, Batch 404/1259, Loss: 2.2148475646972656\n",
            "Epoch 0, Phase train, Batch 283, Batch 405/1259, Loss: 2.289393424987793\n",
            "Epoch 0, Phase train, Batch 283, Batch 405/1259, Loss: 2.252631425857544\n",
            "Epoch 0, Phase train, Batch 326, Batch 406/1259, Loss: 2.105508327484131\n",
            "Epoch 0, Phase train, Batch 326, Batch 406/1259, Loss: 2.2378244400024414\n",
            "Epoch 0, Phase train, Batch 365, Batch 407/1259, Loss: 2.202535390853882\n",
            "Epoch 0, Phase train, Batch 365, Batch 407/1259, Loss: 2.258699417114258\n",
            "Epoch 0, Phase train, Batch 1140, Batch 408/1259, Loss: 2.2198243141174316\n",
            "Epoch 0, Phase train, Batch 1140, Batch 408/1259, Loss: 2.23093843460083\n",
            "Epoch 0, Phase train, Batch 111, Batch 409/1259, Loss: 2.3382959365844727\n",
            "Epoch 0, Phase train, Batch 111, Batch 409/1259, Loss: 2.2055816650390625\n",
            "Epoch 0, Phase train, Batch 1349, Batch 410/1259, Loss: 2.089841604232788\n",
            "Epoch 0, Phase train, Batch 1349, Batch 410/1259, Loss: 2.1464972496032715\n",
            "Epoch 0, Phase train, Batch 831, Batch 411/1259, Loss: 2.2301182746887207\n",
            "Epoch 0, Phase train, Batch 831, Batch 411/1259, Loss: 2.180079460144043\n",
            "Epoch 0, Phase train, Batch 4, Batch 412/1259, Loss: 2.232820749282837\n",
            "Epoch 0, Phase train, Batch 4, Batch 412/1259, Loss: 2.369516372680664\n",
            "Epoch 0, Phase train, Batch 1527, Batch 413/1259, Loss: 2.114262342453003\n",
            "Epoch 0, Phase train, Batch 1527, Batch 413/1259, Loss: 2.3021018505096436\n",
            "Epoch 0, Phase train, Batch 554, Batch 414/1259, Loss: 2.3797824382781982\n",
            "Epoch 0, Phase train, Batch 554, Batch 414/1259, Loss: 2.131063938140869\n",
            "Epoch 0, Phase train, Batch 981, Batch 415/1259, Loss: 2.409987688064575\n",
            "Epoch 0, Phase train, Batch 981, Batch 415/1259, Loss: 2.2484793663024902\n",
            "Epoch 0, Phase train, Batch 1371, Batch 416/1259, Loss: 2.104728937149048\n",
            "Epoch 0, Phase train, Batch 1371, Batch 416/1259, Loss: 2.101430654525757\n",
            "Epoch 0, Phase train, Batch 609, Batch 417/1259, Loss: 2.252559185028076\n",
            "Epoch 0, Phase train, Batch 609, Batch 417/1259, Loss: 2.0970866680145264\n",
            "Epoch 0, Phase train, Batch 487, Batch 418/1259, Loss: 2.2407314777374268\n",
            "Epoch 0, Phase train, Batch 487, Batch 418/1259, Loss: 2.1088528633117676\n",
            "Epoch 0, Phase train, Batch 462, Batch 419/1259, Loss: 2.195556402206421\n",
            "Epoch 0, Phase train, Batch 462, Batch 419/1259, Loss: 2.1990065574645996\n",
            "Epoch 0, Phase train, Batch 541, Batch 420/1259, Loss: 2.210792064666748\n",
            "Epoch 0, Phase train, Batch 541, Batch 420/1259, Loss: 2.1676523685455322\n",
            "Epoch 0, Phase train, Batch 824, Batch 421/1259, Loss: 1.9980148077011108\n",
            "Epoch 0, Phase train, Batch 824, Batch 421/1259, Loss: 2.2747652530670166\n",
            "Epoch 0, Phase train, Batch 1246, Batch 422/1259, Loss: 2.1625418663024902\n",
            "Epoch 0, Phase train, Batch 1246, Batch 422/1259, Loss: 2.122138738632202\n",
            "Epoch 0, Phase train, Batch 736, Batch 423/1259, Loss: 2.2955541610717773\n",
            "Epoch 0, Phase train, Batch 736, Batch 423/1259, Loss: 2.1576128005981445\n",
            "Epoch 0, Phase train, Batch 412, Batch 424/1259, Loss: 2.2089786529541016\n",
            "Epoch 0, Phase train, Batch 412, Batch 424/1259, Loss: 2.089757204055786\n",
            "Epoch 0, Phase train, Batch 1432, Batch 425/1259, Loss: 2.211632013320923\n",
            "Epoch 0, Phase train, Batch 1432, Batch 425/1259, Loss: 2.0206892490386963\n",
            "Epoch 0, Phase train, Batch 956, Batch 426/1259, Loss: 2.1242456436157227\n",
            "Epoch 0, Phase train, Batch 956, Batch 426/1259, Loss: 2.2587990760803223\n",
            "Epoch 0, Phase train, Batch 1385, Batch 427/1259, Loss: 2.205442190170288\n",
            "Epoch 0, Phase train, Batch 1385, Batch 427/1259, Loss: 2.0137627124786377\n",
            "Epoch 0, Phase train, Batch 1059, Batch 428/1259, Loss: 2.0832631587982178\n",
            "Epoch 0, Phase train, Batch 1059, Batch 428/1259, Loss: 2.283923864364624\n",
            "Epoch 0, Phase train, Batch 414, Batch 429/1259, Loss: 2.1942522525787354\n",
            "Epoch 0, Phase train, Batch 414, Batch 429/1259, Loss: 2.1369078159332275\n",
            "Epoch 0, Phase train, Batch 495, Batch 430/1259, Loss: 2.109722375869751\n",
            "Epoch 0, Phase train, Batch 495, Batch 430/1259, Loss: 2.1452043056488037\n",
            "Epoch 0, Phase train, Batch 994, Batch 431/1259, Loss: 2.1710312366485596\n",
            "Epoch 0, Phase train, Batch 994, Batch 431/1259, Loss: 2.1762869358062744\n",
            "Epoch 0, Phase train, Batch 22, Batch 432/1259, Loss: 2.1496191024780273\n",
            "Epoch 0, Phase train, Batch 22, Batch 432/1259, Loss: 2.325946092605591\n",
            "Epoch 0, Phase train, Batch 768, Batch 433/1259, Loss: 2.1141960620880127\n",
            "Epoch 0, Phase train, Batch 768, Batch 433/1259, Loss: 2.061209201812744\n",
            "Epoch 0, Phase train, Batch 1366, Batch 434/1259, Loss: 2.128596067428589\n",
            "Epoch 0, Phase train, Batch 1366, Batch 434/1259, Loss: 2.036709785461426\n",
            "Epoch 0, Phase train, Batch 1192, Batch 435/1259, Loss: 2.120115280151367\n",
            "Epoch 0, Phase train, Batch 1192, Batch 435/1259, Loss: 2.115046501159668\n",
            "Epoch 0, Phase train, Batch 965, Batch 436/1259, Loss: 2.045776605606079\n",
            "Epoch 0, Phase train, Batch 965, Batch 436/1259, Loss: 2.108123779296875\n",
            "Epoch 0, Phase train, Batch 165, Batch 437/1259, Loss: 2.2523014545440674\n",
            "Epoch 0, Phase train, Batch 165, Batch 437/1259, Loss: 2.104560613632202\n",
            "Epoch 0, Phase train, Batch 1055, Batch 438/1259, Loss: 2.163067579269409\n",
            "Epoch 0, Phase train, Batch 1055, Batch 438/1259, Loss: 2.250612258911133\n",
            "Epoch 0, Phase train, Batch 1304, Batch 439/1259, Loss: 2.1894540786743164\n",
            "Epoch 0, Phase train, Batch 1304, Batch 439/1259, Loss: 2.0566277503967285\n",
            "Epoch 0, Phase train, Batch 1358, Batch 440/1259, Loss: 2.0465152263641357\n",
            "Epoch 0, Phase train, Batch 1358, Batch 440/1259, Loss: 2.0851869583129883\n",
            "Epoch 0, Phase train, Batch 1175, Batch 441/1259, Loss: 2.1419501304626465\n",
            "Epoch 0, Phase train, Batch 1175, Batch 441/1259, Loss: 2.343630313873291\n",
            "Epoch 0, Phase train, Batch 74, Batch 442/1259, Loss: 2.2250921726226807\n",
            "Epoch 0, Phase train, Batch 74, Batch 442/1259, Loss: 2.2236883640289307\n",
            "Epoch 0, Phase train, Batch 235, Batch 443/1259, Loss: 2.11997127532959\n",
            "Epoch 0, Phase train, Batch 235, Batch 443/1259, Loss: 2.2236340045928955\n",
            "Epoch 0, Phase train, Batch 200, Batch 444/1259, Loss: 2.034294366836548\n",
            "Epoch 0, Phase train, Batch 200, Batch 444/1259, Loss: 2.1338043212890625\n",
            "Epoch 0, Phase train, Batch 1000, Batch 445/1259, Loss: 2.0012521743774414\n",
            "Epoch 0, Phase train, Batch 1000, Batch 445/1259, Loss: 2.2447242736816406\n",
            "Epoch 0, Phase train, Batch 198, Batch 446/1259, Loss: 2.1613359451293945\n",
            "Epoch 0, Phase train, Batch 198, Batch 446/1259, Loss: 2.2426462173461914\n",
            "Epoch 0, Phase train, Batch 827, Batch 447/1259, Loss: 2.0867490768432617\n",
            "Epoch 0, Phase train, Batch 827, Batch 447/1259, Loss: 2.2022886276245117\n",
            "Epoch 0, Phase train, Batch 250, Batch 448/1259, Loss: 2.2242989540100098\n",
            "Epoch 0, Phase train, Batch 250, Batch 448/1259, Loss: 2.1690855026245117\n",
            "Epoch 0, Phase train, Batch 592, Batch 449/1259, Loss: 2.2877228260040283\n",
            "Epoch 0, Phase train, Batch 592, Batch 449/1259, Loss: 2.2109901905059814\n",
            "Epoch 0, Phase train, Batch 396, Batch 450/1259, Loss: 2.060896635055542\n",
            "Epoch 0, Phase train, Batch 396, Batch 450/1259, Loss: 2.0788164138793945\n",
            "Epoch 0, Phase train, Batch 828, Batch 451/1259, Loss: 2.182605028152466\n",
            "Epoch 0, Phase train, Batch 828, Batch 451/1259, Loss: 2.2146599292755127\n",
            "Epoch 0, Phase train, Batch 268, Batch 452/1259, Loss: 2.1745057106018066\n",
            "Epoch 0, Phase train, Batch 268, Batch 452/1259, Loss: 2.0309500694274902\n",
            "Epoch 0, Phase train, Batch 931, Batch 453/1259, Loss: 2.250965118408203\n",
            "Epoch 0, Phase train, Batch 931, Batch 453/1259, Loss: 2.0645527839660645\n",
            "Epoch 0, Phase train, Batch 1014, Batch 454/1259, Loss: 2.223581314086914\n",
            "Epoch 0, Phase train, Batch 1014, Batch 454/1259, Loss: 2.1654112339019775\n",
            "Epoch 0, Phase train, Batch 151, Batch 455/1259, Loss: 2.1692497730255127\n",
            "Epoch 0, Phase train, Batch 151, Batch 455/1259, Loss: 2.306790828704834\n",
            "Epoch 0, Phase train, Batch 548, Batch 456/1259, Loss: 2.1674954891204834\n",
            "Epoch 0, Phase train, Batch 548, Batch 456/1259, Loss: 2.283905506134033\n",
            "Epoch 0, Phase train, Batch 616, Batch 457/1259, Loss: 2.2365529537200928\n",
            "Epoch 0, Phase train, Batch 616, Batch 457/1259, Loss: 2.2264599800109863\n",
            "Epoch 0, Phase train, Batch 724, Batch 458/1259, Loss: 2.1936426162719727\n",
            "Epoch 0, Phase train, Batch 724, Batch 458/1259, Loss: 1.953564167022705\n",
            "Epoch 0, Phase train, Batch 413, Batch 459/1259, Loss: 2.163376569747925\n",
            "Epoch 0, Phase train, Batch 413, Batch 459/1259, Loss: 2.0698139667510986\n",
            "Epoch 0, Phase train, Batch 510, Batch 460/1259, Loss: 1.9355385303497314\n",
            "Epoch 0, Phase train, Batch 510, Batch 460/1259, Loss: 1.9830152988433838\n",
            "Epoch 0, Phase train, Batch 780, Batch 461/1259, Loss: 2.147430658340454\n",
            "Epoch 0, Phase train, Batch 780, Batch 461/1259, Loss: 2.0834193229675293\n",
            "Epoch 0, Phase train, Batch 1227, Batch 462/1259, Loss: 1.936123251914978\n",
            "Epoch 0, Phase train, Batch 1227, Batch 462/1259, Loss: 2.0277116298675537\n",
            "Epoch 0, Phase train, Batch 185, Batch 463/1259, Loss: 2.097654104232788\n",
            "Epoch 0, Phase train, Batch 185, Batch 463/1259, Loss: 2.348370313644409\n",
            "Epoch 0, Phase train, Batch 955, Batch 464/1259, Loss: 1.969981074333191\n",
            "Epoch 0, Phase train, Batch 955, Batch 464/1259, Loss: 2.065707206726074\n",
            "Epoch 0, Phase train, Batch 756, Batch 465/1259, Loss: 2.2174501419067383\n",
            "Epoch 0, Phase train, Batch 756, Batch 465/1259, Loss: 2.117169141769409\n",
            "Epoch 0, Phase train, Batch 556, Batch 466/1259, Loss: 2.1527085304260254\n",
            "Epoch 0, Phase train, Batch 556, Batch 466/1259, Loss: 2.206881523132324\n",
            "Epoch 0, Phase train, Batch 1121, Batch 467/1259, Loss: 2.1484534740448\n",
            "Epoch 0, Phase train, Batch 1121, Batch 467/1259, Loss: 2.092775344848633\n",
            "Epoch 0, Phase train, Batch 483, Batch 468/1259, Loss: 1.987750768661499\n",
            "Epoch 0, Phase train, Batch 483, Batch 468/1259, Loss: 1.9334568977355957\n",
            "Epoch 0, Phase train, Batch 1297, Batch 469/1259, Loss: 2.053048610687256\n",
            "Epoch 0, Phase train, Batch 1297, Batch 469/1259, Loss: 2.143929958343506\n",
            "Epoch 0, Phase train, Batch 120, Batch 470/1259, Loss: 2.3418822288513184\n",
            "Epoch 0, Phase train, Batch 120, Batch 470/1259, Loss: 2.1108410358428955\n",
            "Epoch 0, Phase train, Batch 277, Batch 471/1259, Loss: 1.9994621276855469\n",
            "Epoch 0, Phase train, Batch 277, Batch 471/1259, Loss: 2.1609575748443604\n",
            "Epoch 0, Phase train, Batch 1470, Batch 472/1259, Loss: 2.100684881210327\n",
            "Epoch 0, Phase train, Batch 1470, Batch 472/1259, Loss: 2.0868990421295166\n",
            "Epoch 0, Phase train, Batch 5, Batch 473/1259, Loss: 2.2847378253936768\n",
            "Epoch 0, Phase train, Batch 5, Batch 473/1259, Loss: 2.2695837020874023\n",
            "Epoch 0, Phase train, Batch 721, Batch 474/1259, Loss: 2.1755263805389404\n",
            "Epoch 0, Phase train, Batch 721, Batch 474/1259, Loss: 2.1496663093566895\n",
            "Epoch 0, Phase train, Batch 1215, Batch 475/1259, Loss: 2.078701972961426\n",
            "Epoch 0, Phase train, Batch 1215, Batch 475/1259, Loss: 2.0908572673797607\n",
            "Epoch 0, Phase train, Batch 91, Batch 476/1259, Loss: 2.3470494747161865\n",
            "Epoch 0, Phase train, Batch 91, Batch 476/1259, Loss: 2.271202802658081\n",
            "Epoch 0, Phase train, Batch 1396, Batch 477/1259, Loss: 2.2865078449249268\n",
            "Epoch 0, Phase train, Batch 1396, Batch 477/1259, Loss: 1.9495238065719604\n",
            "Epoch 0, Phase train, Batch 655, Batch 478/1259, Loss: 2.123120069503784\n",
            "Epoch 0, Phase train, Batch 655, Batch 478/1259, Loss: 2.0886189937591553\n",
            "Epoch 0, Phase train, Batch 1306, Batch 479/1259, Loss: 2.0315988063812256\n",
            "Epoch 0, Phase train, Batch 1306, Batch 479/1259, Loss: 2.116889238357544\n",
            "Epoch 0, Phase train, Batch 442, Batch 480/1259, Loss: 2.312857151031494\n",
            "Epoch 0, Phase train, Batch 442, Batch 480/1259, Loss: 1.9671547412872314\n",
            "Epoch 0, Phase train, Batch 808, Batch 481/1259, Loss: 2.1228814125061035\n",
            "Epoch 0, Phase train, Batch 808, Batch 481/1259, Loss: 2.0322489738464355\n",
            "Epoch 0, Phase train, Batch 1437, Batch 482/1259, Loss: 2.2067947387695312\n",
            "Epoch 0, Phase train, Batch 1437, Batch 482/1259, Loss: 2.0982301235198975\n",
            "Epoch 0, Phase train, Batch 626, Batch 483/1259, Loss: 2.1258835792541504\n",
            "Epoch 0, Phase train, Batch 626, Batch 483/1259, Loss: 2.0751795768737793\n",
            "Epoch 0, Phase train, Batch 715, Batch 484/1259, Loss: 2.2596442699432373\n",
            "Epoch 0, Phase train, Batch 715, Batch 484/1259, Loss: 2.181511402130127\n",
            "Epoch 0, Phase train, Batch 466, Batch 485/1259, Loss: 1.9830719232559204\n",
            "Epoch 0, Phase train, Batch 466, Batch 485/1259, Loss: 2.1358110904693604\n",
            "Epoch 0, Phase train, Batch 938, Batch 486/1259, Loss: 2.0348634719848633\n",
            "Epoch 0, Phase train, Batch 938, Batch 486/1259, Loss: 2.083782196044922\n",
            "Epoch 0, Phase train, Batch 85, Batch 487/1259, Loss: 2.279585123062134\n",
            "Epoch 0, Phase train, Batch 85, Batch 487/1259, Loss: 2.261763334274292\n",
            "Epoch 0, Phase train, Batch 322, Batch 488/1259, Loss: 2.229421377182007\n",
            "Epoch 0, Phase train, Batch 322, Batch 488/1259, Loss: 2.113264799118042\n",
            "Epoch 0, Phase train, Batch 86, Batch 489/1259, Loss: 2.204477071762085\n",
            "Epoch 0, Phase train, Batch 86, Batch 489/1259, Loss: 2.037929058074951\n",
            "Epoch 0, Phase train, Batch 496, Batch 490/1259, Loss: 2.0210094451904297\n",
            "Epoch 0, Phase train, Batch 496, Batch 490/1259, Loss: 2.0190999507904053\n",
            "Epoch 0, Phase train, Batch 306, Batch 491/1259, Loss: 2.107722520828247\n",
            "Epoch 0, Phase train, Batch 306, Batch 491/1259, Loss: 2.1526412963867188\n",
            "Epoch 0, Phase train, Batch 750, Batch 492/1259, Loss: 1.9230903387069702\n",
            "Epoch 0, Phase train, Batch 750, Batch 492/1259, Loss: 2.0785021781921387\n",
            "Epoch 0, Phase train, Batch 570, Batch 493/1259, Loss: 2.3229737281799316\n",
            "Epoch 0, Phase train, Batch 570, Batch 493/1259, Loss: 2.4234631061553955\n",
            "Epoch 0, Phase train, Batch 1487, Batch 494/1259, Loss: 2.07181978225708\n",
            "Epoch 0, Phase train, Batch 1487, Batch 494/1259, Loss: 1.927345633506775\n",
            "Epoch 0, Phase train, Batch 1410, Batch 495/1259, Loss: 1.9562867879867554\n",
            "Epoch 0, Phase train, Batch 1410, Batch 495/1259, Loss: 2.0252435207366943\n",
            "Epoch 0, Phase train, Batch 978, Batch 496/1259, Loss: 1.9937442541122437\n",
            "Epoch 0, Phase train, Batch 978, Batch 496/1259, Loss: 2.171297311782837\n",
            "Epoch 0, Phase train, Batch 1419, Batch 497/1259, Loss: 2.095585346221924\n",
            "Epoch 0, Phase train, Batch 1419, Batch 497/1259, Loss: 2.0522518157958984\n",
            "Epoch 0, Phase train, Batch 585, Batch 498/1259, Loss: 2.180260419845581\n",
            "Epoch 0, Phase train, Batch 585, Batch 498/1259, Loss: 2.1589722633361816\n",
            "Epoch 0, Phase train, Batch 344, Batch 499/1259, Loss: 2.0290260314941406\n",
            "Epoch 0, Phase train, Batch 344, Batch 499/1259, Loss: 2.0789809226989746\n",
            "Epoch 0, Phase train, Batch 368, Batch 500/1259, Loss: 2.192401885986328\n",
            "Epoch 0, Phase train, Batch 368, Batch 500/1259, Loss: 2.0888965129852295\n",
            "Epoch 0, Phase train, Batch 1156, Batch 501/1259, Loss: 2.0705862045288086\n",
            "Epoch 0, Phase train, Batch 1156, Batch 501/1259, Loss: 2.2016708850860596\n",
            "Epoch 0, Phase train, Batch 964, Batch 502/1259, Loss: 2.0251877307891846\n",
            "Epoch 0, Phase train, Batch 964, Batch 502/1259, Loss: 2.0408740043640137\n",
            "Epoch 0, Phase train, Batch 1243, Batch 503/1259, Loss: 2.087244987487793\n",
            "Epoch 0, Phase train, Batch 1243, Batch 503/1259, Loss: 1.9115564823150635\n",
            "Epoch 0, Phase train, Batch 216, Batch 504/1259, Loss: 2.1027302742004395\n",
            "Epoch 0, Phase train, Batch 216, Batch 504/1259, Loss: 2.094578742980957\n",
            "Epoch 0, Phase train, Batch 206, Batch 505/1259, Loss: 2.0522186756134033\n",
            "Epoch 0, Phase train, Batch 206, Batch 505/1259, Loss: 2.0436739921569824\n",
            "Epoch 0, Phase train, Batch 278, Batch 506/1259, Loss: 2.0484964847564697\n",
            "Epoch 0, Phase train, Batch 278, Batch 506/1259, Loss: 2.253089427947998\n",
            "Epoch 0, Phase train, Batch 441, Batch 507/1259, Loss: 2.1499924659729004\n",
            "Epoch 0, Phase train, Batch 441, Batch 507/1259, Loss: 1.9474087953567505\n",
            "Epoch 0, Phase train, Batch 789, Batch 508/1259, Loss: 2.1699767112731934\n",
            "Epoch 0, Phase train, Batch 789, Batch 508/1259, Loss: 2.0187888145446777\n",
            "Epoch 0, Phase train, Batch 70, Batch 509/1259, Loss: 2.2276217937469482\n",
            "Epoch 0, Phase train, Batch 70, Batch 509/1259, Loss: 2.1680798530578613\n",
            "Epoch 0, Phase train, Batch 784, Batch 510/1259, Loss: 2.1142520904541016\n",
            "Epoch 0, Phase train, Batch 784, Batch 510/1259, Loss: 2.0529980659484863\n",
            "Epoch 0, Phase train, Batch 303, Batch 511/1259, Loss: 1.9608709812164307\n",
            "Epoch 0, Phase train, Batch 303, Batch 511/1259, Loss: 2.03422474861145\n",
            "Epoch 0, Phase train, Batch 587, Batch 512/1259, Loss: 2.0383224487304688\n",
            "Epoch 0, Phase train, Batch 587, Batch 512/1259, Loss: 2.1239967346191406\n",
            "Epoch 0, Phase train, Batch 1325, Batch 513/1259, Loss: 1.9894591569900513\n",
            "Epoch 0, Phase train, Batch 1325, Batch 513/1259, Loss: 2.038655996322632\n",
            "Epoch 0, Phase train, Batch 734, Batch 514/1259, Loss: 2.1076414585113525\n",
            "Epoch 0, Phase train, Batch 734, Batch 514/1259, Loss: 2.3031134605407715\n",
            "Epoch 0, Phase train, Batch 1205, Batch 515/1259, Loss: 1.89506995677948\n",
            "Epoch 0, Phase train, Batch 1205, Batch 515/1259, Loss: 2.0034561157226562\n",
            "Epoch 0, Phase train, Batch 1194, Batch 516/1259, Loss: 2.001948118209839\n",
            "Epoch 0, Phase train, Batch 1194, Batch 516/1259, Loss: 1.921046257019043\n",
            "Epoch 0, Phase train, Batch 1241, Batch 517/1259, Loss: 2.1347434520721436\n",
            "Epoch 0, Phase train, Batch 1241, Batch 517/1259, Loss: 2.059018135070801\n",
            "Epoch 0, Phase train, Batch 1262, Batch 518/1259, Loss: 2.00921368598938\n",
            "Epoch 0, Phase train, Batch 1262, Batch 518/1259, Loss: 1.9521278142929077\n",
            "Epoch 0, Phase train, Batch 1450, Batch 519/1259, Loss: 2.0268914699554443\n",
            "Epoch 0, Phase train, Batch 1450, Batch 519/1259, Loss: 2.0563724040985107\n",
            "Epoch 0, Phase train, Batch 804, Batch 520/1259, Loss: 2.064969301223755\n",
            "Epoch 0, Phase train, Batch 804, Batch 520/1259, Loss: 2.076113224029541\n",
            "Epoch 0, Phase train, Batch 799, Batch 521/1259, Loss: 2.2023887634277344\n",
            "Epoch 0, Phase train, Batch 799, Batch 521/1259, Loss: 2.2232139110565186\n",
            "Epoch 0, Phase train, Batch 475, Batch 522/1259, Loss: 2.1594972610473633\n",
            "Epoch 0, Phase train, Batch 475, Batch 522/1259, Loss: 2.0221078395843506\n",
            "Epoch 0, Phase train, Batch 1363, Batch 523/1259, Loss: 2.1118221282958984\n",
            "Epoch 0, Phase train, Batch 1363, Batch 523/1259, Loss: 1.9915244579315186\n",
            "Epoch 0, Phase train, Batch 752, Batch 524/1259, Loss: 2.148284673690796\n",
            "Epoch 0, Phase train, Batch 752, Batch 524/1259, Loss: 2.0328118801116943\n",
            "Epoch 0, Phase train, Batch 1080, Batch 525/1259, Loss: 2.080475330352783\n",
            "Epoch 0, Phase train, Batch 1080, Batch 525/1259, Loss: 2.084102153778076\n",
            "Epoch 0, Phase train, Batch 1127, Batch 526/1259, Loss: 2.13157320022583\n",
            "Epoch 0, Phase train, Batch 1127, Batch 526/1259, Loss: 1.976028561592102\n",
            "Epoch 0, Phase train, Batch 817, Batch 527/1259, Loss: 2.1524553298950195\n",
            "Epoch 0, Phase train, Batch 817, Batch 527/1259, Loss: 2.1082522869110107\n",
            "Epoch 0, Phase train, Batch 1211, Batch 528/1259, Loss: 1.811604380607605\n",
            "Epoch 0, Phase train, Batch 1211, Batch 528/1259, Loss: 1.977301001548767\n",
            "Epoch 0, Phase train, Batch 1344, Batch 529/1259, Loss: 1.8990970849990845\n",
            "Epoch 0, Phase train, Batch 1344, Batch 529/1259, Loss: 1.936050534248352\n",
            "Epoch 0, Phase train, Batch 1493, Batch 530/1259, Loss: 2.0405406951904297\n",
            "Epoch 0, Phase train, Batch 1493, Batch 530/1259, Loss: 1.8605464696884155\n",
            "Epoch 0, Phase train, Batch 894, Batch 531/1259, Loss: 2.161574125289917\n",
            "Epoch 0, Phase train, Batch 894, Batch 531/1259, Loss: 2.1819050312042236\n",
            "Epoch 0, Phase train, Batch 1152, Batch 532/1259, Loss: 2.1310198307037354\n",
            "Epoch 0, Phase train, Batch 1152, Batch 532/1259, Loss: 2.1290502548217773\n",
            "Epoch 0, Phase train, Batch 1452, Batch 533/1259, Loss: 2.1700665950775146\n",
            "Epoch 0, Phase train, Batch 1452, Batch 533/1259, Loss: 1.878454327583313\n",
            "Epoch 0, Phase train, Batch 1504, Batch 534/1259, Loss: 2.04306960105896\n",
            "Epoch 0, Phase train, Batch 1504, Batch 534/1259, Loss: 2.0350418090820312\n",
            "Epoch 0, Phase train, Batch 138, Batch 535/1259, Loss: 2.0247628688812256\n",
            "Epoch 0, Phase train, Batch 138, Batch 535/1259, Loss: 2.115093946456909\n",
            "Epoch 0, Phase train, Batch 761, Batch 536/1259, Loss: 2.069340705871582\n",
            "Epoch 0, Phase train, Batch 761, Batch 536/1259, Loss: 2.073003053665161\n",
            "Epoch 0, Phase train, Batch 879, Batch 537/1259, Loss: 2.233808755874634\n",
            "Epoch 0, Phase train, Batch 879, Batch 537/1259, Loss: 2.1721415519714355\n",
            "Epoch 0, Phase train, Batch 1508, Batch 538/1259, Loss: 2.030325412750244\n",
            "Epoch 0, Phase train, Batch 1508, Batch 538/1259, Loss: 1.9778188467025757\n",
            "Epoch 0, Phase train, Batch 933, Batch 539/1259, Loss: 1.9474647045135498\n",
            "Epoch 0, Phase train, Batch 933, Batch 539/1259, Loss: 2.0975515842437744\n",
            "Epoch 0, Phase train, Batch 3, Batch 540/1259, Loss: 2.4562010765075684\n",
            "Epoch 0, Phase train, Batch 3, Batch 540/1259, Loss: 2.1844959259033203\n",
            "Epoch 0, Phase train, Batch 1395, Batch 541/1259, Loss: 1.9089244604110718\n",
            "Epoch 0, Phase train, Batch 1395, Batch 541/1259, Loss: 1.9822605848312378\n",
            "Epoch 0, Phase train, Batch 697, Batch 542/1259, Loss: 2.0334296226501465\n",
            "Epoch 0, Phase train, Batch 697, Batch 542/1259, Loss: 2.1356894969940186\n",
            "Epoch 0, Phase train, Batch 550, Batch 543/1259, Loss: 2.2157933712005615\n",
            "Epoch 0, Phase train, Batch 550, Batch 543/1259, Loss: 2.1957550048828125\n",
            "Epoch 0, Phase train, Batch 603, Batch 544/1259, Loss: 2.1166577339172363\n",
            "Epoch 0, Phase train, Batch 603, Batch 544/1259, Loss: 2.1279749870300293\n",
            "Epoch 0, Phase train, Batch 422, Batch 545/1259, Loss: 2.095064640045166\n",
            "Epoch 0, Phase train, Batch 422, Batch 545/1259, Loss: 1.9922987222671509\n",
            "Epoch 0, Phase train, Batch 1076, Batch 546/1259, Loss: 2.1997714042663574\n",
            "Epoch 0, Phase train, Batch 1076, Batch 546/1259, Loss: 2.2149136066436768\n",
            "Epoch 0, Phase train, Batch 986, Batch 547/1259, Loss: 2.225522994995117\n",
            "Epoch 0, Phase train, Batch 986, Batch 547/1259, Loss: 2.130992889404297\n",
            "Epoch 0, Phase train, Batch 576, Batch 548/1259, Loss: 2.23866605758667\n",
            "Epoch 0, Phase train, Batch 576, Batch 548/1259, Loss: 2.3138020038604736\n",
            "Epoch 0, Phase train, Batch 511, Batch 549/1259, Loss: 1.9520503282546997\n",
            "Epoch 0, Phase train, Batch 511, Batch 549/1259, Loss: 1.9966212511062622\n",
            "Epoch 0, Phase train, Batch 67, Batch 550/1259, Loss: 2.2379846572875977\n",
            "Epoch 0, Phase train, Batch 67, Batch 550/1259, Loss: 2.085477113723755\n",
            "Epoch 0, Phase train, Batch 1213, Batch 551/1259, Loss: 1.9241286516189575\n",
            "Epoch 0, Phase train, Batch 1213, Batch 551/1259, Loss: 1.9203174114227295\n",
            "Epoch 0, Phase train, Batch 1514, Batch 552/1259, Loss: 1.9990907907485962\n",
            "Epoch 0, Phase train, Batch 1514, Batch 552/1259, Loss: 2.0408530235290527\n",
            "Epoch 0, Phase train, Batch 599, Batch 553/1259, Loss: 2.2883048057556152\n",
            "Epoch 0, Phase train, Batch 599, Batch 553/1259, Loss: 2.2385003566741943\n",
            "Epoch 0, Phase train, Batch 318, Batch 554/1259, Loss: 2.0772271156311035\n",
            "Epoch 0, Phase train, Batch 318, Batch 554/1259, Loss: 2.2090988159179688\n",
            "Epoch 0, Phase train, Batch 353, Batch 555/1259, Loss: 1.999171257019043\n",
            "Epoch 0, Phase train, Batch 353, Batch 555/1259, Loss: 2.0062599182128906\n",
            "Epoch 0, Phase train, Batch 15, Batch 556/1259, Loss: 2.2441744804382324\n",
            "Epoch 0, Phase train, Batch 15, Batch 556/1259, Loss: 2.0799288749694824\n",
            "Epoch 0, Phase train, Batch 561, Batch 557/1259, Loss: 2.2234160900115967\n",
            "Epoch 0, Phase train, Batch 561, Batch 557/1259, Loss: 2.225841760635376\n",
            "Epoch 0, Phase train, Batch 895, Batch 558/1259, Loss: 2.141822338104248\n",
            "Epoch 0, Phase train, Batch 895, Batch 558/1259, Loss: 2.083301305770874\n",
            "Epoch 0, Phase train, Batch 460, Batch 559/1259, Loss: 1.9878381490707397\n",
            "Epoch 0, Phase train, Batch 460, Batch 559/1259, Loss: 1.9228146076202393\n",
            "Epoch 0, Phase train, Batch 1465, Batch 560/1259, Loss: 2.1684086322784424\n",
            "Epoch 0, Phase train, Batch 1465, Batch 560/1259, Loss: 1.9604023694992065\n",
            "Epoch 0, Phase train, Batch 1084, Batch 561/1259, Loss: 2.0301311016082764\n",
            "Epoch 0, Phase train, Batch 1084, Batch 561/1259, Loss: 1.9415745735168457\n",
            "Epoch 0, Phase train, Batch 1490, Batch 562/1259, Loss: 1.9994261264801025\n",
            "Epoch 0, Phase train, Batch 1490, Batch 562/1259, Loss: 2.0911262035369873\n",
            "Epoch 0, Phase train, Batch 899, Batch 563/1259, Loss: 2.149109363555908\n",
            "Epoch 0, Phase train, Batch 899, Batch 563/1259, Loss: 1.9450199604034424\n",
            "Epoch 0, Phase train, Batch 809, Batch 564/1259, Loss: 2.08980655670166\n",
            "Epoch 0, Phase train, Batch 809, Batch 564/1259, Loss: 1.914508581161499\n",
            "Epoch 0, Phase train, Batch 81, Batch 565/1259, Loss: 2.2827999591827393\n",
            "Epoch 0, Phase train, Batch 81, Batch 565/1259, Loss: 2.2014546394348145\n",
            "Epoch 0, Phase train, Batch 439, Batch 566/1259, Loss: 2.0432591438293457\n",
            "Epoch 0, Phase train, Batch 439, Batch 566/1259, Loss: 2.059347152709961\n",
            "Epoch 0, Phase train, Batch 1472, Batch 567/1259, Loss: 2.0789129734039307\n",
            "Epoch 0, Phase train, Batch 1472, Batch 567/1259, Loss: 2.1091151237487793\n",
            "Epoch 0, Phase train, Batch 744, Batch 568/1259, Loss: 1.9173667430877686\n",
            "Epoch 0, Phase train, Batch 744, Batch 568/1259, Loss: 2.0601911544799805\n",
            "Epoch 0, Phase train, Batch 1079, Batch 569/1259, Loss: 1.978052020072937\n",
            "Epoch 0, Phase train, Batch 1079, Batch 569/1259, Loss: 2.1092143058776855\n",
            "Epoch 0, Phase train, Batch 1142, Batch 570/1259, Loss: 2.0163161754608154\n",
            "Epoch 0, Phase train, Batch 1142, Batch 570/1259, Loss: 1.981819748878479\n",
            "Epoch 0, Phase train, Batch 1058, Batch 571/1259, Loss: 2.040785312652588\n",
            "Epoch 0, Phase train, Batch 1058, Batch 571/1259, Loss: 2.3418374061584473\n",
            "Epoch 0, Phase train, Batch 661, Batch 572/1259, Loss: 2.055159330368042\n",
            "Epoch 0, Phase train, Batch 661, Batch 572/1259, Loss: 2.1003732681274414\n",
            "Epoch 0, Phase train, Batch 992, Batch 573/1259, Loss: 1.9331896305084229\n",
            "Epoch 0, Phase train, Batch 992, Batch 573/1259, Loss: 2.233327627182007\n",
            "Epoch 0, Phase train, Batch 1565, Batch 574/1259, Loss: 1.985937476158142\n",
            "Epoch 0, Phase train, Batch 1565, Batch 574/1259, Loss: 1.8737990856170654\n",
            "Epoch 0, Phase train, Batch 245, Batch 575/1259, Loss: 2.0808985233306885\n",
            "Epoch 0, Phase train, Batch 245, Batch 575/1259, Loss: 1.9901483058929443\n",
            "Epoch 0, Phase train, Batch 1418, Batch 576/1259, Loss: 1.937924861907959\n",
            "Epoch 0, Phase train, Batch 1418, Batch 576/1259, Loss: 2.144587278366089\n",
            "Epoch 0, Phase train, Batch 205, Batch 577/1259, Loss: 2.0157551765441895\n",
            "Epoch 0, Phase train, Batch 205, Batch 577/1259, Loss: 2.0147905349731445\n",
            "Epoch 0, Phase train, Batch 361, Batch 578/1259, Loss: 1.9891924858093262\n",
            "Epoch 0, Phase train, Batch 361, Batch 578/1259, Loss: 2.082392692565918\n",
            "Epoch 0, Phase train, Batch 1247, Batch 579/1259, Loss: 1.893506646156311\n",
            "Epoch 0, Phase train, Batch 1247, Batch 579/1259, Loss: 1.9943675994873047\n",
            "Epoch 0, Phase train, Batch 742, Batch 580/1259, Loss: 1.959526538848877\n",
            "Epoch 0, Phase train, Batch 742, Batch 580/1259, Loss: 2.1171586513519287\n",
            "Epoch 0, Phase train, Batch 692, Batch 581/1259, Loss: 2.1832432746887207\n",
            "Epoch 0, Phase train, Batch 692, Batch 581/1259, Loss: 2.1652991771698\n",
            "Epoch 0, Phase train, Batch 1309, Batch 582/1259, Loss: 2.0043692588806152\n",
            "Epoch 0, Phase train, Batch 1309, Batch 582/1259, Loss: 1.8775434494018555\n",
            "Epoch 0, Phase train, Batch 255, Batch 583/1259, Loss: 2.248979091644287\n",
            "Epoch 0, Phase train, Batch 255, Batch 583/1259, Loss: 2.0666017532348633\n",
            "Epoch 0, Phase train, Batch 657, Batch 584/1259, Loss: 1.9951573610305786\n",
            "Epoch 0, Phase train, Batch 657, Batch 584/1259, Loss: 2.0253167152404785\n",
            "Epoch 0, Phase train, Batch 285, Batch 585/1259, Loss: 2.000174045562744\n",
            "Epoch 0, Phase train, Batch 285, Batch 585/1259, Loss: 2.133535146713257\n",
            "Epoch 0, Phase train, Batch 1183, Batch 586/1259, Loss: 1.8948204517364502\n",
            "Epoch 0, Phase train, Batch 1183, Batch 586/1259, Loss: 2.0273211002349854\n",
            "Epoch 0, Phase train, Batch 197, Batch 587/1259, Loss: 1.9466863870620728\n",
            "Epoch 0, Phase train, Batch 197, Batch 587/1259, Loss: 2.068443536758423\n",
            "Epoch 0, Phase train, Batch 796, Batch 588/1259, Loss: 2.201026678085327\n",
            "Epoch 0, Phase train, Batch 796, Batch 588/1259, Loss: 2.0248775482177734\n",
            "Epoch 0, Phase train, Batch 77, Batch 589/1259, Loss: 2.2661497592926025\n",
            "Epoch 0, Phase train, Batch 77, Batch 589/1259, Loss: 2.3033227920532227\n",
            "Epoch 0, Phase train, Batch 946, Batch 590/1259, Loss: 1.958681344985962\n",
            "Epoch 0, Phase train, Batch 946, Batch 590/1259, Loss: 2.028357982635498\n",
            "Epoch 0, Phase train, Batch 632, Batch 591/1259, Loss: 1.982055425643921\n",
            "Epoch 0, Phase train, Batch 632, Batch 591/1259, Loss: 2.1309685707092285\n",
            "Epoch 0, Phase train, Batch 1342, Batch 592/1259, Loss: 1.9612398147583008\n",
            "Epoch 0, Phase train, Batch 1342, Batch 592/1259, Loss: 2.00351881980896\n",
            "Epoch 0, Phase train, Batch 818, Batch 593/1259, Loss: 2.0290441513061523\n",
            "Epoch 0, Phase train, Batch 818, Batch 593/1259, Loss: 2.0639889240264893\n",
            "Epoch 0, Phase train, Batch 646, Batch 594/1259, Loss: 2.2624387741088867\n",
            "Epoch 0, Phase train, Batch 646, Batch 594/1259, Loss: 2.1276450157165527\n",
            "Epoch 0, Phase train, Batch 1518, Batch 595/1259, Loss: 1.9679045677185059\n",
            "Epoch 0, Phase train, Batch 1518, Batch 595/1259, Loss: 2.0384867191314697\n",
            "Epoch 0, Phase train, Batch 624, Batch 596/1259, Loss: 2.127431869506836\n",
            "Epoch 0, Phase train, Batch 624, Batch 596/1259, Loss: 2.0086450576782227\n",
            "Epoch 0, Phase train, Batch 270, Batch 597/1259, Loss: 1.992708444595337\n",
            "Epoch 0, Phase train, Batch 270, Batch 597/1259, Loss: 1.9985179901123047\n",
            "Epoch 0, Phase train, Batch 52, Batch 598/1259, Loss: 2.2243988513946533\n",
            "Epoch 0, Phase train, Batch 52, Batch 598/1259, Loss: 2.2472264766693115\n",
            "Epoch 0, Phase train, Batch 355, Batch 599/1259, Loss: 1.9871199131011963\n",
            "Epoch 0, Phase train, Batch 355, Batch 599/1259, Loss: 2.0496718883514404\n",
            "Epoch 0, Phase train, Batch 144, Batch 600/1259, Loss: 2.013977289199829\n",
            "Epoch 0, Phase train, Batch 144, Batch 600/1259, Loss: 2.0036020278930664\n",
            "Epoch 0, Phase train, Batch 172, Batch 601/1259, Loss: 2.0772533416748047\n",
            "Epoch 0, Phase train, Batch 172, Batch 601/1259, Loss: 2.1185715198516846\n",
            "Epoch 0, Phase train, Batch 316, Batch 602/1259, Loss: 2.1063685417175293\n",
            "Epoch 0, Phase train, Batch 316, Batch 602/1259, Loss: 1.806227684020996\n",
            "Epoch 0, Phase train, Batch 1200, Batch 603/1259, Loss: 1.959989309310913\n",
            "Epoch 0, Phase train, Batch 1200, Batch 603/1259, Loss: 1.872329592704773\n",
            "Epoch 0, Phase train, Batch 1274, Batch 604/1259, Loss: 1.982153058052063\n",
            "Epoch 0, Phase train, Batch 1274, Batch 604/1259, Loss: 1.9242905378341675\n",
            "Epoch 0, Phase train, Batch 1132, Batch 605/1259, Loss: 1.957283854484558\n",
            "Epoch 0, Phase train, Batch 1132, Batch 605/1259, Loss: 1.938315749168396\n",
            "Epoch 0, Phase train, Batch 765, Batch 606/1259, Loss: 2.0738282203674316\n",
            "Epoch 0, Phase train, Batch 765, Batch 606/1259, Loss: 2.1199231147766113\n",
            "Epoch 0, Phase train, Batch 1035, Batch 607/1259, Loss: 1.96413254737854\n",
            "Epoch 0, Phase train, Batch 1035, Batch 607/1259, Loss: 2.0980093479156494\n",
            "Epoch 0, Phase train, Batch 1171, Batch 608/1259, Loss: 2.1015777587890625\n",
            "Epoch 0, Phase train, Batch 1171, Batch 608/1259, Loss: 1.9950830936431885\n",
            "Epoch 0, Phase train, Batch 335, Batch 609/1259, Loss: 2.1907880306243896\n",
            "Epoch 0, Phase train, Batch 335, Batch 609/1259, Loss: 2.023117780685425\n",
            "Epoch 0, Phase train, Batch 50, Batch 610/1259, Loss: 2.096099853515625\n",
            "Epoch 0, Phase train, Batch 50, Batch 610/1259, Loss: 2.2254204750061035\n",
            "Epoch 0, Phase train, Batch 311, Batch 611/1259, Loss: 1.8829257488250732\n",
            "Epoch 0, Phase train, Batch 311, Batch 611/1259, Loss: 2.005155563354492\n",
            "Epoch 0, Phase train, Batch 1446, Batch 612/1259, Loss: 1.9557431936264038\n",
            "Epoch 0, Phase train, Batch 1446, Batch 612/1259, Loss: 1.9028644561767578\n",
            "Epoch 0, Phase train, Batch 1245, Batch 613/1259, Loss: 1.946121335029602\n",
            "Epoch 0, Phase train, Batch 1245, Batch 613/1259, Loss: 1.9184108972549438\n",
            "Epoch 0, Phase train, Batch 1365, Batch 614/1259, Loss: 1.993918538093567\n",
            "Epoch 0, Phase train, Batch 1365, Batch 614/1259, Loss: 1.9814814329147339\n",
            "Epoch 0, Phase train, Batch 1144, Batch 615/1259, Loss: 2.0531516075134277\n",
            "Epoch 0, Phase train, Batch 1144, Batch 615/1259, Loss: 2.071920871734619\n",
            "Epoch 0, Phase train, Batch 463, Batch 616/1259, Loss: 1.8985583782196045\n",
            "Epoch 0, Phase train, Batch 463, Batch 616/1259, Loss: 2.091371536254883\n",
            "Epoch 0, Phase train, Batch 280, Batch 617/1259, Loss: 2.0424551963806152\n",
            "Epoch 0, Phase train, Batch 280, Batch 617/1259, Loss: 2.0928571224212646\n",
            "Epoch 0, Phase train, Batch 1353, Batch 618/1259, Loss: 2.004551887512207\n",
            "Epoch 0, Phase train, Batch 1353, Batch 618/1259, Loss: 1.8933151960372925\n",
            "Epoch 0, Phase train, Batch 1228, Batch 619/1259, Loss: 1.8853991031646729\n",
            "Epoch 0, Phase train, Batch 1228, Batch 619/1259, Loss: 1.929649829864502\n",
            "Epoch 0, Phase train, Batch 1442, Batch 620/1259, Loss: 2.0500457286834717\n",
            "Epoch 0, Phase train, Batch 1442, Batch 620/1259, Loss: 1.9221137762069702\n",
            "Epoch 0, Phase train, Batch 1038, Batch 621/1259, Loss: 2.1468257904052734\n",
            "Epoch 0, Phase train, Batch 1038, Batch 621/1259, Loss: 1.8967437744140625\n",
            "Epoch 0, Phase train, Batch 294, Batch 622/1259, Loss: 2.1269588470458984\n",
            "Epoch 0, Phase train, Batch 294, Batch 622/1259, Loss: 2.2064900398254395\n",
            "Epoch 0, Phase train, Batch 215, Batch 623/1259, Loss: 2.0085225105285645\n",
            "Epoch 0, Phase train, Batch 215, Batch 623/1259, Loss: 2.063324451446533\n",
            "Epoch 0, Phase train, Batch 822, Batch 624/1259, Loss: 2.0623626708984375\n",
            "Epoch 0, Phase train, Batch 822, Batch 624/1259, Loss: 1.9650646448135376\n",
            "Epoch 0, Phase train, Batch 1531, Batch 625/1259, Loss: 2.039064407348633\n",
            "Epoch 0, Phase train, Batch 1531, Batch 625/1259, Loss: 1.8416361808776855\n",
            "Epoch 0, Phase train, Batch 257, Batch 626/1259, Loss: 1.9091062545776367\n",
            "Epoch 0, Phase train, Batch 257, Batch 626/1259, Loss: 2.0315256118774414\n",
            "Epoch 0, Phase train, Batch 906, Batch 627/1259, Loss: 2.1389102935791016\n",
            "Epoch 0, Phase train, Batch 906, Batch 627/1259, Loss: 2.015080690383911\n",
            "Epoch 0, Phase train, Batch 251, Batch 628/1259, Loss: 1.965600609779358\n",
            "Epoch 0, Phase train, Batch 251, Batch 628/1259, Loss: 2.051666736602783\n",
            "Epoch 0, Phase train, Batch 874, Batch 629/1259, Loss: 1.758135437965393\n",
            "Epoch 0, Phase train, Batch 874, Batch 629/1259, Loss: 2.0154449939727783\n",
            "Epoch 0, Phase train, Batch 244, Batch 631/1259, Loss: 1.965040683746338\n",
            "Epoch 0, Phase train, Batch 244, Batch 631/1259, Loss: 2.1377182006835938\n",
            "Epoch 0, Phase train, Batch 1235, Batch 632/1259, Loss: 1.9298937320709229\n",
            "Epoch 0, Phase train, Batch 1235, Batch 632/1259, Loss: 1.8525538444519043\n",
            "Epoch 0, Phase train, Batch 515, Batch 633/1259, Loss: 2.071155548095703\n",
            "Epoch 0, Phase train, Batch 515, Batch 633/1259, Loss: 1.9544833898544312\n",
            "Epoch 0, Phase train, Batch 1512, Batch 634/1259, Loss: 1.9616925716400146\n",
            "Epoch 0, Phase train, Batch 1512, Batch 634/1259, Loss: 1.9266215562820435\n",
            "Epoch 0, Phase train, Batch 876, Batch 635/1259, Loss: 2.3095569610595703\n",
            "Epoch 0, Phase train, Batch 876, Batch 635/1259, Loss: 2.0525002479553223\n",
            "Epoch 0, Phase train, Batch 1086, Batch 636/1259, Loss: 2.0615274906158447\n",
            "Epoch 0, Phase train, Batch 1086, Batch 636/1259, Loss: 2.1858606338500977\n",
            "Epoch 0, Phase train, Batch 867, Batch 637/1259, Loss: 1.9967961311340332\n",
            "Epoch 0, Phase train, Batch 867, Batch 637/1259, Loss: 2.1074130535125732\n",
            "Epoch 0, Phase train, Batch 9, Batch 638/1259, Loss: 2.1619157791137695\n",
            "Epoch 0, Phase train, Batch 9, Batch 638/1259, Loss: 2.074474811553955\n",
            "Epoch 0, Phase train, Batch 305, Batch 639/1259, Loss: 2.1596031188964844\n",
            "Epoch 0, Phase train, Batch 305, Batch 639/1259, Loss: 2.054095506668091\n",
            "Epoch 0, Phase train, Batch 1455, Batch 640/1259, Loss: 1.9875274896621704\n",
            "Epoch 0, Phase train, Batch 1455, Batch 640/1259, Loss: 2.0268054008483887\n",
            "Epoch 0, Phase train, Batch 96, Batch 641/1259, Loss: 2.1076371669769287\n",
            "Epoch 0, Phase train, Batch 96, Batch 641/1259, Loss: 2.0661115646362305\n",
            "Epoch 0, Phase train, Batch 1329, Batch 642/1259, Loss: 1.9599733352661133\n",
            "Epoch 0, Phase train, Batch 1329, Batch 642/1259, Loss: 1.892881989479065\n",
            "Epoch 0, Phase train, Batch 272, Batch 643/1259, Loss: 1.9878841638565063\n",
            "Epoch 0, Phase train, Batch 272, Batch 643/1259, Loss: 2.046558141708374\n",
            "Epoch 0, Phase train, Batch 1129, Batch 644/1259, Loss: 1.9349627494812012\n",
            "Epoch 0, Phase train, Batch 1129, Batch 644/1259, Loss: 2.176129102706909\n",
            "Epoch 0, Phase train, Batch 208, Batch 645/1259, Loss: 1.937087059020996\n",
            "Epoch 0, Phase train, Batch 208, Batch 645/1259, Loss: 1.9978418350219727\n",
            "Epoch 0, Phase train, Batch 231, Batch 646/1259, Loss: 2.0997183322906494\n",
            "Epoch 0, Phase train, Batch 231, Batch 646/1259, Loss: 2.005657911300659\n",
            "Epoch 0, Phase train, Batch 1015, Batch 647/1259, Loss: 1.8949412107467651\n",
            "Epoch 0, Phase train, Batch 1015, Batch 647/1259, Loss: 2.091143846511841\n",
            "Epoch 0, Phase train, Batch 354, Batch 648/1259, Loss: 1.8044390678405762\n",
            "Epoch 0, Phase train, Batch 354, Batch 648/1259, Loss: 1.9718209505081177\n",
            "Epoch 0, Phase train, Batch 432, Batch 649/1259, Loss: 2.01749849319458\n",
            "Epoch 0, Phase train, Batch 432, Batch 649/1259, Loss: 1.9647126197814941\n",
            "Epoch 0, Phase train, Batch 139, Batch 650/1259, Loss: 1.9675318002700806\n",
            "Epoch 0, Phase train, Batch 139, Batch 650/1259, Loss: 2.229637861251831\n",
            "Epoch 0, Phase train, Batch 912, Batch 651/1259, Loss: 1.9304275512695312\n",
            "Epoch 0, Phase train, Batch 912, Batch 651/1259, Loss: 2.009690284729004\n",
            "Epoch 0, Phase train, Batch 160, Batch 652/1259, Loss: 1.9219626188278198\n",
            "Epoch 0, Phase train, Batch 160, Batch 652/1259, Loss: 2.015043020248413\n",
            "Epoch 0, Phase train, Batch 1420, Batch 653/1259, Loss: 2.0489349365234375\n",
            "Epoch 0, Phase train, Batch 1420, Batch 653/1259, Loss: 2.0163793563842773\n",
            "Epoch 0, Phase train, Batch 848, Batch 654/1259, Loss: 2.0210177898406982\n",
            "Epoch 0, Phase train, Batch 848, Batch 654/1259, Loss: 1.97181236743927\n",
            "Epoch 0, Phase train, Batch 1261, Batch 655/1259, Loss: 1.8463011980056763\n",
            "Epoch 0, Phase train, Batch 1261, Batch 655/1259, Loss: 1.8888970613479614\n",
            "Epoch 0, Phase train, Batch 429, Batch 656/1259, Loss: 1.9574912786483765\n",
            "Epoch 0, Phase train, Batch 429, Batch 656/1259, Loss: 1.9842678308486938\n",
            "Epoch 0, Phase train, Batch 1019, Batch 657/1259, Loss: 1.9686012268066406\n",
            "Epoch 0, Phase train, Batch 1019, Batch 657/1259, Loss: 2.075979709625244\n",
            "Epoch 0, Phase train, Batch 223, Batch 658/1259, Loss: 1.9019931554794312\n",
            "Epoch 0, Phase train, Batch 223, Batch 658/1259, Loss: 1.979547381401062\n",
            "Epoch 0, Phase train, Batch 1284, Batch 659/1259, Loss: 1.9418883323669434\n",
            "Epoch 0, Phase train, Batch 1284, Batch 659/1259, Loss: 1.9128361940383911\n",
            "Epoch 0, Phase train, Batch 667, Batch 660/1259, Loss: 2.0174612998962402\n",
            "Epoch 0, Phase train, Batch 667, Batch 660/1259, Loss: 1.8187403678894043\n",
            "Epoch 0, Phase train, Batch 1315, Batch 661/1259, Loss: 1.9707070589065552\n",
            "Epoch 0, Phase train, Batch 1315, Batch 661/1259, Loss: 1.986213207244873\n",
            "Epoch 0, Phase train, Batch 1155, Batch 662/1259, Loss: 1.9741277694702148\n",
            "Epoch 0, Phase train, Batch 1155, Batch 662/1259, Loss: 2.1808156967163086\n",
            "Epoch 0, Phase train, Batch 666, Batch 663/1259, Loss: 1.9802402257919312\n",
            "Epoch 0, Phase train, Batch 666, Batch 663/1259, Loss: 2.199002265930176\n",
            "Epoch 0, Phase train, Batch 660, Batch 664/1259, Loss: 2.1281399726867676\n",
            "Epoch 0, Phase train, Batch 660, Batch 664/1259, Loss: 2.2335729598999023\n",
            "Epoch 0, Phase train, Batch 1333, Batch 665/1259, Loss: 1.8531731367111206\n",
            "Epoch 0, Phase train, Batch 1333, Batch 665/1259, Loss: 2.030954122543335\n",
            "Epoch 0, Phase train, Batch 1087, Batch 666/1259, Loss: 1.9213813543319702\n",
            "Epoch 0, Phase train, Batch 1087, Batch 666/1259, Loss: 2.0646891593933105\n",
            "Epoch 0, Phase train, Batch 1054, Batch 667/1259, Loss: 2.149571657180786\n",
            "Epoch 0, Phase train, Batch 1054, Batch 667/1259, Loss: 2.0305633544921875\n",
            "Epoch 0, Phase train, Batch 1168, Batch 668/1259, Loss: 2.1271252632141113\n",
            "Epoch 0, Phase train, Batch 1168, Batch 668/1259, Loss: 1.9499825239181519\n",
            "Epoch 0, Phase train, Batch 687, Batch 669/1259, Loss: 2.078418016433716\n",
            "Epoch 0, Phase train, Batch 687, Batch 669/1259, Loss: 2.1495800018310547\n",
            "Epoch 0, Phase train, Batch 967, Batch 670/1259, Loss: 2.1776275634765625\n",
            "Epoch 0, Phase train, Batch 967, Batch 670/1259, Loss: 1.9580156803131104\n",
            "Epoch 0, Phase train, Batch 341, Batch 671/1259, Loss: 1.9925023317337036\n",
            "Epoch 0, Phase train, Batch 341, Batch 671/1259, Loss: 1.9574778079986572\n",
            "Epoch 0, Phase train, Batch 919, Batch 672/1259, Loss: 2.0100786685943604\n",
            "Epoch 0, Phase train, Batch 919, Batch 672/1259, Loss: 1.9551204442977905\n",
            "Epoch 0, Phase train, Batch 1026, Batch 673/1259, Loss: 2.1702113151550293\n",
            "Epoch 0, Phase train, Batch 1026, Batch 673/1259, Loss: 2.075094223022461\n",
            "Epoch 0, Phase train, Batch 792, Batch 674/1259, Loss: 2.030148983001709\n",
            "Epoch 0, Phase train, Batch 792, Batch 674/1259, Loss: 2.0024302005767822\n",
            "Epoch 0, Phase train, Batch 1237, Batch 675/1259, Loss: 1.8988111019134521\n",
            "Epoch 0, Phase train, Batch 1237, Batch 675/1259, Loss: 1.8899152278900146\n",
            "Epoch 0, Phase train, Batch 1483, Batch 676/1259, Loss: 2.016191005706787\n",
            "Epoch 0, Phase train, Batch 1483, Batch 676/1259, Loss: 1.9454677104949951\n",
            "Epoch 0, Phase train, Batch 543, Batch 677/1259, Loss: 2.110588550567627\n",
            "Epoch 0, Phase train, Batch 543, Batch 677/1259, Loss: 2.032264471054077\n",
            "Epoch 0, Phase train, Batch 612, Batch 678/1259, Loss: 1.9566688537597656\n",
            "Epoch 0, Phase train, Batch 612, Batch 678/1259, Loss: 2.0117380619049072\n",
            "Epoch 0, Phase train, Batch 1288, Batch 679/1259, Loss: 1.865602731704712\n",
            "Epoch 0, Phase train, Batch 1288, Batch 679/1259, Loss: 2.0169057846069336\n",
            "Epoch 0, Phase train, Batch 979, Batch 680/1259, Loss: 1.9866896867752075\n",
            "Epoch 0, Phase train, Batch 979, Batch 680/1259, Loss: 2.041015386581421\n",
            "Epoch 0, Phase train, Batch 847, Batch 681/1259, Loss: 1.9978117942810059\n",
            "Epoch 0, Phase train, Batch 847, Batch 681/1259, Loss: 1.961639165878296\n",
            "Epoch 0, Phase train, Batch 28, Batch 682/1259, Loss: 2.179194211959839\n",
            "Epoch 0, Phase train, Batch 28, Batch 682/1259, Loss: 1.9725433588027954\n",
            "Epoch 0, Phase train, Batch 406, Batch 683/1259, Loss: 2.018922805786133\n",
            "Epoch 0, Phase train, Batch 406, Batch 683/1259, Loss: 2.0209436416625977\n",
            "Epoch 0, Phase train, Batch 881, Batch 684/1259, Loss: 2.1081182956695557\n",
            "Epoch 0, Phase train, Batch 881, Batch 684/1259, Loss: 2.0781702995300293\n",
            "Epoch 0, Phase train, Batch 957, Batch 685/1259, Loss: 2.1128647327423096\n",
            "Epoch 0, Phase train, Batch 957, Batch 685/1259, Loss: 2.0371344089508057\n",
            "Epoch 0, Phase train, Batch 1392, Batch 686/1259, Loss: 1.9718565940856934\n",
            "Epoch 0, Phase train, Batch 1392, Batch 686/1259, Loss: 2.01794171333313\n",
            "Epoch 0, Phase train, Batch 1251, Batch 687/1259, Loss: 1.9841569662094116\n",
            "Epoch 0, Phase train, Batch 1251, Batch 687/1259, Loss: 1.95510995388031\n",
            "Epoch 0, Phase train, Batch 711, Batch 688/1259, Loss: 2.091609239578247\n",
            "Epoch 0, Phase train, Batch 711, Batch 688/1259, Loss: 2.0231168270111084\n",
            "Epoch 0, Phase train, Batch 878, Batch 689/1259, Loss: 1.9318066835403442\n",
            "Epoch 0, Phase train, Batch 878, Batch 689/1259, Loss: 1.9806441068649292\n",
            "Epoch 0, Phase train, Batch 677, Batch 690/1259, Loss: 2.122591495513916\n",
            "Epoch 0, Phase train, Batch 677, Batch 690/1259, Loss: 1.9519116878509521\n",
            "Epoch 0, Phase train, Batch 513, Batch 691/1259, Loss: 1.9008300304412842\n",
            "Epoch 0, Phase train, Batch 513, Batch 691/1259, Loss: 1.9183192253112793\n",
            "Epoch 0, Phase train, Batch 154, Batch 692/1259, Loss: 1.996303677558899\n",
            "Epoch 0, Phase train, Batch 154, Batch 692/1259, Loss: 2.046417474746704\n",
            "Epoch 0, Phase train, Batch 1263, Batch 693/1259, Loss: 1.9162805080413818\n",
            "Epoch 0, Phase train, Batch 1263, Batch 693/1259, Loss: 1.8581167459487915\n",
            "Epoch 0, Phase train, Batch 856, Batch 694/1259, Loss: 2.0570290088653564\n",
            "Epoch 0, Phase train, Batch 856, Batch 694/1259, Loss: 1.9849538803100586\n",
            "Epoch 0, Phase train, Batch 1022, Batch 695/1259, Loss: 1.8354518413543701\n",
            "Epoch 0, Phase train, Batch 1022, Batch 695/1259, Loss: 1.8717403411865234\n",
            "Epoch 0, Phase train, Batch 24, Batch 696/1259, Loss: 2.146806001663208\n",
            "Epoch 0, Phase train, Batch 24, Batch 696/1259, Loss: 2.1226179599761963\n",
            "Epoch 0, Phase train, Batch 53, Batch 697/1259, Loss: 2.254486322402954\n",
            "Epoch 0, Phase train, Batch 53, Batch 697/1259, Loss: 2.3002371788024902\n",
            "Epoch 0, Phase train, Batch 995, Batch 698/1259, Loss: 2.010990619659424\n",
            "Epoch 0, Phase train, Batch 995, Batch 698/1259, Loss: 1.920456051826477\n",
            "Epoch 0, Phase train, Batch 568, Batch 699/1259, Loss: 1.9387201070785522\n",
            "Epoch 0, Phase train, Batch 568, Batch 699/1259, Loss: 2.029554843902588\n",
            "Epoch 0, Phase train, Batch 1445, Batch 700/1259, Loss: 1.9526358842849731\n",
            "Epoch 0, Phase train, Batch 1445, Batch 700/1259, Loss: 1.8373112678527832\n",
            "Epoch 0, Phase train, Batch 1097, Batch 701/1259, Loss: 1.951374888420105\n",
            "Epoch 0, Phase train, Batch 1097, Batch 701/1259, Loss: 1.9059313535690308\n",
            "Epoch 0, Phase train, Batch 532, Batch 702/1259, Loss: 1.9815524816513062\n",
            "Epoch 0, Phase train, Batch 532, Batch 702/1259, Loss: 1.9595690965652466\n",
            "Epoch 0, Phase train, Batch 1052, Batch 703/1259, Loss: 1.9963017702102661\n",
            "Epoch 0, Phase train, Batch 1052, Batch 703/1259, Loss: 1.876097321510315\n",
            "Epoch 0, Phase train, Batch 758, Batch 704/1259, Loss: 1.8635059595108032\n",
            "Epoch 0, Phase train, Batch 758, Batch 704/1259, Loss: 2.0446860790252686\n",
            "Epoch 0, Phase train, Batch 663, Batch 705/1259, Loss: 1.9857913255691528\n",
            "Epoch 0, Phase train, Batch 663, Batch 705/1259, Loss: 1.934364914894104\n",
            "Epoch 0, Phase train, Batch 1390, Batch 706/1259, Loss: 1.8038794994354248\n",
            "Epoch 0, Phase train, Batch 1390, Batch 706/1259, Loss: 2.066099166870117\n",
            "Epoch 0, Phase train, Batch 275, Batch 707/1259, Loss: 1.9707996845245361\n",
            "Epoch 0, Phase train, Batch 275, Batch 707/1259, Loss: 1.8215289115905762\n",
            "Epoch 0, Phase train, Batch 1505, Batch 708/1259, Loss: 1.9528697729110718\n",
            "Epoch 0, Phase train, Batch 1505, Batch 708/1259, Loss: 2.0222604274749756\n",
            "Epoch 0, Phase train, Batch 696, Batch 709/1259, Loss: 1.9282852411270142\n",
            "Epoch 0, Phase train, Batch 696, Batch 709/1259, Loss: 2.0465993881225586\n",
            "Epoch 0, Phase train, Batch 1499, Batch 710/1259, Loss: 1.9809929132461548\n",
            "Epoch 0, Phase train, Batch 1499, Batch 710/1259, Loss: 1.8442586660385132\n",
            "Epoch 0, Phase train, Batch 759, Batch 711/1259, Loss: 1.9318469762802124\n",
            "Epoch 0, Phase train, Batch 759, Batch 711/1259, Loss: 2.0688061714172363\n",
            "Epoch 0, Phase train, Batch 947, Batch 712/1259, Loss: 1.9860384464263916\n",
            "Epoch 0, Phase train, Batch 947, Batch 712/1259, Loss: 1.916800856590271\n",
            "Epoch 0, Phase train, Batch 1425, Batch 713/1259, Loss: 2.0686330795288086\n",
            "Epoch 0, Phase train, Batch 1425, Batch 713/1259, Loss: 2.0332624912261963\n",
            "Epoch 0, Phase train, Batch 604, Batch 714/1259, Loss: 2.0823190212249756\n",
            "Epoch 0, Phase train, Batch 604, Batch 714/1259, Loss: 2.022495985031128\n",
            "Epoch 0, Phase train, Batch 892, Batch 716/1259, Loss: 1.9260042905807495\n",
            "Epoch 0, Phase train, Batch 892, Batch 716/1259, Loss: 2.109652280807495\n",
            "Epoch 0, Phase train, Batch 492, Batch 717/1259, Loss: 1.9763751029968262\n",
            "Epoch 0, Phase train, Batch 492, Batch 717/1259, Loss: 1.828867793083191\n",
            "Epoch 0, Phase train, Batch 373, Batch 718/1259, Loss: 2.015326738357544\n",
            "Epoch 0, Phase train, Batch 373, Batch 718/1259, Loss: 1.9202427864074707\n",
            "Epoch 0, Phase train, Batch 1124, Batch 719/1259, Loss: 1.896762490272522\n",
            "Epoch 0, Phase train, Batch 1124, Batch 719/1259, Loss: 2.005664110183716\n",
            "Epoch 0, Phase train, Batch 1409, Batch 720/1259, Loss: 2.0654423236846924\n",
            "Epoch 0, Phase train, Batch 1409, Batch 720/1259, Loss: 1.915030837059021\n",
            "Epoch 0, Phase train, Batch 192, Batch 721/1259, Loss: 2.0862061977386475\n",
            "Epoch 0, Phase train, Batch 192, Batch 721/1259, Loss: 2.036552667617798\n",
            "Epoch 0, Phase train, Batch 384, Batch 722/1259, Loss: 1.9915313720703125\n",
            "Epoch 0, Phase train, Batch 384, Batch 722/1259, Loss: 2.0685439109802246\n",
            "Epoch 0, Phase train, Batch 1075, Batch 723/1259, Loss: 1.9563077688217163\n",
            "Epoch 0, Phase train, Batch 1075, Batch 723/1259, Loss: 1.9417657852172852\n",
            "Epoch 0, Phase train, Batch 1057, Batch 724/1259, Loss: 1.9798632860183716\n",
            "Epoch 0, Phase train, Batch 1057, Batch 724/1259, Loss: 2.0286102294921875\n",
            "Epoch 0, Phase train, Batch 1269, Batch 725/1259, Loss: 1.8611479997634888\n",
            "Epoch 0, Phase train, Batch 1269, Batch 725/1259, Loss: 1.9586572647094727\n",
            "Epoch 0, Phase train, Batch 1538, Batch 726/1259, Loss: 1.912105917930603\n",
            "Epoch 0, Phase train, Batch 1538, Batch 726/1259, Loss: 1.8228559494018555\n",
            "Epoch 0, Phase train, Batch 107, Batch 727/1259, Loss: 2.0941455364227295\n",
            "Epoch 0, Phase train, Batch 107, Batch 727/1259, Loss: 2.1623098850250244\n",
            "Epoch 0, Phase train, Batch 1275, Batch 728/1259, Loss: 2.112590789794922\n",
            "Epoch 0, Phase train, Batch 1275, Batch 728/1259, Loss: 2.019423484802246\n",
            "Epoch 0, Phase train, Batch 527, Batch 729/1259, Loss: 1.9509023427963257\n",
            "Epoch 0, Phase train, Batch 527, Batch 729/1259, Loss: 1.9675570726394653\n",
            "Epoch 0, Phase train, Batch 1386, Batch 730/1259, Loss: 1.8664356470108032\n",
            "Epoch 0, Phase train, Batch 1386, Batch 730/1259, Loss: 1.9701381921768188\n",
            "Epoch 0, Phase train, Batch 747, Batch 731/1259, Loss: 2.061668634414673\n",
            "Epoch 0, Phase train, Batch 747, Batch 731/1259, Loss: 1.968598484992981\n",
            "Epoch 0, Phase train, Batch 1201, Batch 732/1259, Loss: 1.8488800525665283\n",
            "Epoch 0, Phase train, Batch 1201, Batch 732/1259, Loss: 1.970002293586731\n",
            "Epoch 0, Phase train, Batch 1478, Batch 733/1259, Loss: 1.816943645477295\n",
            "Epoch 0, Phase train, Batch 1478, Batch 733/1259, Loss: 1.967747688293457\n",
            "Epoch 0, Phase train, Batch 1537, Batch 734/1259, Loss: 1.9068284034729004\n",
            "Epoch 0, Phase train, Batch 1537, Batch 734/1259, Loss: 1.8333003520965576\n",
            "Epoch 0, Phase train, Batch 776, Batch 735/1259, Loss: 1.955273985862732\n",
            "Epoch 0, Phase train, Batch 776, Batch 735/1259, Loss: 2.0287623405456543\n",
            "Epoch 0, Phase train, Batch 607, Batch 736/1259, Loss: 2.0076472759246826\n",
            "Epoch 0, Phase train, Batch 607, Batch 736/1259, Loss: 2.0735082626342773\n",
            "Epoch 0, Phase train, Batch 474, Batch 737/1259, Loss: 2.0414886474609375\n",
            "Epoch 0, Phase train, Batch 474, Batch 737/1259, Loss: 1.9903839826583862\n",
            "Epoch 0, Phase train, Batch 299, Batch 738/1259, Loss: 1.9998879432678223\n",
            "Epoch 0, Phase train, Batch 299, Batch 738/1259, Loss: 2.003363609313965\n",
            "Epoch 0, Phase train, Batch 1413, Batch 739/1259, Loss: 1.7901774644851685\n",
            "Epoch 0, Phase train, Batch 1413, Batch 739/1259, Loss: 1.9309839010238647\n",
            "Epoch 0, Phase train, Batch 757, Batch 740/1259, Loss: 1.9642878770828247\n",
            "Epoch 0, Phase train, Batch 757, Batch 740/1259, Loss: 1.9457616806030273\n",
            "Epoch 0, Phase train, Batch 737, Batch 741/1259, Loss: 1.9039560556411743\n",
            "Epoch 0, Phase train, Batch 737, Batch 741/1259, Loss: 1.9592840671539307\n",
            "Epoch 0, Phase train, Batch 658, Batch 742/1259, Loss: 1.9530478715896606\n",
            "Epoch 0, Phase train, Batch 658, Batch 742/1259, Loss: 1.9526385068893433\n",
            "Epoch 0, Phase train, Batch 112, Batch 743/1259, Loss: 2.1074111461639404\n",
            "Epoch 0, Phase train, Batch 112, Batch 743/1259, Loss: 2.1480982303619385\n",
            "Epoch 0, Phase train, Batch 1226, Batch 744/1259, Loss: 1.8661060333251953\n",
            "Epoch 0, Phase train, Batch 1226, Batch 744/1259, Loss: 1.8919639587402344\n",
            "Epoch 0, Phase train, Batch 428, Batch 745/1259, Loss: 1.8514735698699951\n",
            "Epoch 0, Phase train, Batch 428, Batch 745/1259, Loss: 1.8592591285705566\n",
            "Epoch 0, Phase train, Batch 481, Batch 746/1259, Loss: 1.8880480527877808\n",
            "Epoch 0, Phase train, Batch 481, Batch 746/1259, Loss: 1.9605145454406738\n",
            "Epoch 0, Phase train, Batch 1454, Batch 747/1259, Loss: 1.88006591796875\n",
            "Epoch 0, Phase train, Batch 1454, Batch 747/1259, Loss: 1.902764081954956\n",
            "Epoch 0, Phase train, Batch 772, Batch 748/1259, Loss: 1.8167732954025269\n",
            "Epoch 0, Phase train, Batch 772, Batch 748/1259, Loss: 1.9192229509353638\n",
            "Epoch 0, Phase train, Batch 174, Batch 749/1259, Loss: 1.9867329597473145\n",
            "Epoch 0, Phase train, Batch 174, Batch 749/1259, Loss: 1.9123263359069824\n",
            "Epoch 0, Phase train, Batch 1294, Batch 750/1259, Loss: 1.7984201908111572\n",
            "Epoch 0, Phase train, Batch 1294, Batch 750/1259, Loss: 1.9712729454040527\n",
            "Epoch 0, Phase train, Batch 1345, Batch 751/1259, Loss: 1.7790091037750244\n",
            "Epoch 0, Phase train, Batch 1345, Batch 751/1259, Loss: 1.955222487449646\n",
            "Epoch 0, Phase train, Batch 78, Batch 752/1259, Loss: 2.1094579696655273\n",
            "Epoch 0, Phase train, Batch 78, Batch 752/1259, Loss: 2.0111474990844727\n",
            "Epoch 0, Phase train, Batch 464, Batch 753/1259, Loss: 1.901107907295227\n",
            "Epoch 0, Phase train, Batch 464, Batch 753/1259, Loss: 1.8170350790023804\n",
            "Epoch 0, Phase train, Batch 631, Batch 754/1259, Loss: 1.9165949821472168\n",
            "Epoch 0, Phase train, Batch 631, Batch 754/1259, Loss: 2.0616257190704346\n",
            "Epoch 0, Phase train, Batch 963, Batch 755/1259, Loss: 1.9272723197937012\n",
            "Epoch 0, Phase train, Batch 963, Batch 755/1259, Loss: 2.0231311321258545\n",
            "Epoch 0, Phase train, Batch 953, Batch 756/1259, Loss: 1.9112186431884766\n",
            "Epoch 0, Phase train, Batch 953, Batch 756/1259, Loss: 1.958971619606018\n",
            "Epoch 0, Phase train, Batch 552, Batch 757/1259, Loss: 1.984095573425293\n",
            "Epoch 0, Phase train, Batch 552, Batch 757/1259, Loss: 1.9978342056274414\n",
            "Epoch 0, Phase train, Batch 840, Batch 758/1259, Loss: 2.003202438354492\n",
            "Epoch 0, Phase train, Batch 840, Batch 758/1259, Loss: 1.9874409437179565\n",
            "Epoch 0, Phase train, Batch 864, Batch 759/1259, Loss: 2.0666778087615967\n",
            "Epoch 0, Phase train, Batch 864, Batch 759/1259, Loss: 1.9229354858398438\n",
            "Epoch 0, Phase train, Batch 1376, Batch 760/1259, Loss: 1.9565914869308472\n",
            "Epoch 0, Phase train, Batch 1376, Batch 760/1259, Loss: 1.8352973461151123\n",
            "Epoch 0, Phase train, Batch 584, Batch 761/1259, Loss: 2.1353490352630615\n",
            "Epoch 0, Phase train, Batch 584, Batch 761/1259, Loss: 2.028855323791504\n",
            "Epoch 0, Phase train, Batch 588, Batch 762/1259, Loss: 1.9351235628128052\n",
            "Epoch 0, Phase train, Batch 588, Batch 762/1259, Loss: 2.0133066177368164\n",
            "Epoch 0, Phase train, Batch 790, Batch 763/1259, Loss: 1.9923672676086426\n",
            "Epoch 0, Phase train, Batch 790, Batch 763/1259, Loss: 1.858843207359314\n",
            "Epoch 0, Phase train, Batch 904, Batch 764/1259, Loss: 1.8543673753738403\n",
            "Epoch 0, Phase train, Batch 904, Batch 764/1259, Loss: 1.9279422760009766\n",
            "Epoch 0, Phase train, Batch 237, Batch 765/1259, Loss: 2.119290590286255\n",
            "Epoch 0, Phase train, Batch 237, Batch 765/1259, Loss: 2.1395204067230225\n",
            "Epoch 0, Phase train, Batch 733, Batch 766/1259, Loss: 1.8306167125701904\n",
            "Epoch 0, Phase train, Batch 733, Batch 766/1259, Loss: 1.9894804954528809\n",
            "Epoch 0, Phase train, Batch 882, Batch 767/1259, Loss: 2.195493698120117\n",
            "Epoch 0, Phase train, Batch 882, Batch 767/1259, Loss: 1.9036812782287598\n",
            "Epoch 0, Phase train, Batch 1011, Batch 768/1259, Loss: 1.881187081336975\n",
            "Epoch 0, Phase train, Batch 1011, Batch 768/1259, Loss: 1.9921904802322388\n",
            "Epoch 0, Phase train, Batch 695, Batch 769/1259, Loss: 1.9638909101486206\n",
            "Epoch 0, Phase train, Batch 695, Batch 769/1259, Loss: 2.095604419708252\n",
            "Epoch 0, Phase train, Batch 787, Batch 770/1259, Loss: 1.9490538835525513\n",
            "Epoch 0, Phase train, Batch 787, Batch 770/1259, Loss: 1.9131262302398682\n",
            "Epoch 0, Phase train, Batch 816, Batch 771/1259, Loss: 2.026127815246582\n",
            "Epoch 0, Phase train, Batch 816, Batch 771/1259, Loss: 2.0399787425994873\n",
            "Epoch 0, Phase train, Batch 865, Batch 772/1259, Loss: 2.0321593284606934\n",
            "Epoch 0, Phase train, Batch 865, Batch 772/1259, Loss: 1.8730727434158325\n",
            "Epoch 0, Phase train, Batch 936, Batch 773/1259, Loss: 1.9237209558486938\n",
            "Epoch 0, Phase train, Batch 936, Batch 773/1259, Loss: 1.9737520217895508\n",
            "Epoch 0, Phase train, Batch 1360, Batch 774/1259, Loss: 1.9275181293487549\n",
            "Epoch 0, Phase train, Batch 1360, Batch 774/1259, Loss: 1.821406602859497\n",
            "Epoch 0, Phase train, Batch 234, Batch 775/1259, Loss: 2.062163829803467\n",
            "Epoch 0, Phase train, Batch 234, Batch 775/1259, Loss: 2.0382184982299805\n",
            "Epoch 0, Phase train, Batch 1541, Batch 776/1259, Loss: 2.0313642024993896\n",
            "Epoch 0, Phase train, Batch 1541, Batch 776/1259, Loss: 1.9563456773757935\n",
            "Epoch 0, Phase train, Batch 371, Batch 777/1259, Loss: 1.9602330923080444\n",
            "Epoch 0, Phase train, Batch 371, Batch 777/1259, Loss: 1.9208884239196777\n",
            "Epoch 0, Phase train, Batch 1516, Batch 778/1259, Loss: 1.8782566785812378\n",
            "Epoch 0, Phase train, Batch 1516, Batch 778/1259, Loss: 1.8144184350967407\n",
            "Epoch 0, Phase train, Batch 1550, Batch 779/1259, Loss: 1.8541905879974365\n",
            "Epoch 0, Phase train, Batch 1550, Batch 779/1259, Loss: 1.8908095359802246\n",
            "Epoch 0, Phase train, Batch 1443, Batch 780/1259, Loss: 1.883558988571167\n",
            "Epoch 0, Phase train, Batch 1443, Batch 780/1259, Loss: 1.9010151624679565\n",
            "Epoch 0, Phase train, Batch 598, Batch 781/1259, Loss: 2.103722333908081\n",
            "Epoch 0, Phase train, Batch 598, Batch 781/1259, Loss: 2.0919387340545654\n",
            "Epoch 0, Phase train, Batch 493, Batch 782/1259, Loss: 1.8203502893447876\n",
            "Epoch 0, Phase train, Batch 493, Batch 782/1259, Loss: 1.9530088901519775\n",
            "Epoch 0, Phase train, Batch 1190, Batch 783/1259, Loss: 1.8301737308502197\n",
            "Epoch 0, Phase train, Batch 1190, Batch 783/1259, Loss: 1.9074872732162476\n",
            "Epoch 0, Phase train, Batch 884, Batch 784/1259, Loss: 1.980850338935852\n",
            "Epoch 0, Phase train, Batch 884, Batch 784/1259, Loss: 2.0023579597473145\n",
            "Epoch 0, Phase train, Batch 184, Batch 785/1259, Loss: 2.035200357437134\n",
            "Epoch 0, Phase train, Batch 184, Batch 785/1259, Loss: 1.9142686128616333\n",
            "Epoch 0, Phase train, Batch 1380, Batch 786/1259, Loss: 1.8912197351455688\n",
            "Epoch 0, Phase train, Batch 1380, Batch 786/1259, Loss: 1.817854404449463\n",
            "Epoch 0, Phase train, Batch 1330, Batch 787/1259, Loss: 1.7643038034439087\n",
            "Epoch 0, Phase train, Batch 1330, Batch 787/1259, Loss: 1.8554962873458862\n",
            "Epoch 0, Phase train, Batch 573, Batch 788/1259, Loss: 2.1159439086914062\n",
            "Epoch 0, Phase train, Batch 573, Batch 788/1259, Loss: 2.181837558746338\n",
            "Epoch 0, Phase train, Batch 688, Batch 789/1259, Loss: 1.8361629247665405\n",
            "Epoch 0, Phase train, Batch 688, Batch 789/1259, Loss: 2.0753538608551025\n",
            "Epoch 0, Phase train, Batch 932, Batch 790/1259, Loss: 1.948860764503479\n",
            "Epoch 0, Phase train, Batch 932, Batch 790/1259, Loss: 1.9899340867996216\n",
            "Epoch 0, Phase train, Batch 1279, Batch 791/1259, Loss: 1.7624496221542358\n",
            "Epoch 0, Phase train, Batch 1279, Batch 791/1259, Loss: 1.9077874422073364\n",
            "Epoch 0, Phase train, Batch 903, Batch 792/1259, Loss: 1.8849701881408691\n",
            "Epoch 0, Phase train, Batch 903, Batch 792/1259, Loss: 2.079620361328125\n",
            "Epoch 0, Phase train, Batch 727, Batch 793/1259, Loss: 1.8894771337509155\n",
            "Epoch 0, Phase train, Batch 727, Batch 793/1259, Loss: 1.864919900894165\n",
            "Epoch 0, Phase train, Batch 859, Batch 794/1259, Loss: 2.1507015228271484\n",
            "Epoch 0, Phase train, Batch 859, Batch 794/1259, Loss: 1.825007677078247\n",
            "Epoch 0, Phase train, Batch 564, Batch 795/1259, Loss: 1.9542816877365112\n",
            "Epoch 0, Phase train, Batch 564, Batch 795/1259, Loss: 1.9542890787124634\n",
            "Epoch 0, Phase train, Batch 1109, Batch 796/1259, Loss: 1.910375952720642\n",
            "Epoch 0, Phase train, Batch 1109, Batch 796/1259, Loss: 1.9087066650390625\n",
            "Epoch 0, Phase train, Batch 39, Batch 797/1259, Loss: 2.0656464099884033\n",
            "Epoch 0, Phase train, Batch 39, Batch 797/1259, Loss: 2.076394557952881\n",
            "Epoch 0, Phase train, Batch 386, Batch 798/1259, Loss: 2.091369867324829\n",
            "Epoch 0, Phase train, Batch 386, Batch 798/1259, Loss: 1.9300000667572021\n",
            "Epoch 0, Phase train, Batch 1153, Batch 799/1259, Loss: 1.9374172687530518\n",
            "Epoch 0, Phase train, Batch 1153, Batch 799/1259, Loss: 1.9636799097061157\n",
            "Epoch 0, Phase train, Batch 437, Batch 800/1259, Loss: 1.9201561212539673\n",
            "Epoch 0, Phase train, Batch 437, Batch 800/1259, Loss: 1.9895371198654175\n",
            "Epoch 0, Phase train, Batch 982, Batch 801/1259, Loss: 2.086721897125244\n",
            "Epoch 0, Phase train, Batch 982, Batch 801/1259, Loss: 1.81797456741333\n",
            "Epoch 0, Phase train, Batch 794, Batch 802/1259, Loss: 1.8885021209716797\n",
            "Epoch 0, Phase train, Batch 794, Batch 802/1259, Loss: 1.9546716213226318\n",
            "Epoch 0, Phase train, Batch 829, Batch 803/1259, Loss: 1.9221830368041992\n",
            "Epoch 0, Phase train, Batch 829, Batch 803/1259, Loss: 1.8739466667175293\n",
            "Epoch 0, Phase train, Batch 1296, Batch 804/1259, Loss: 1.7837285995483398\n",
            "Epoch 0, Phase train, Batch 1296, Batch 804/1259, Loss: 1.7379870414733887\n",
            "Epoch 0, Phase train, Batch 31, Batch 805/1259, Loss: 2.159583806991577\n",
            "Epoch 0, Phase train, Batch 31, Batch 805/1259, Loss: 2.1088974475860596\n",
            "Epoch 0, Phase train, Batch 883, Batch 806/1259, Loss: 1.865103840827942\n",
            "Epoch 0, Phase train, Batch 883, Batch 806/1259, Loss: 2.006836414337158\n",
            "Epoch 0, Phase train, Batch 801, Batch 807/1259, Loss: 2.06198787689209\n",
            "Epoch 0, Phase train, Batch 801, Batch 807/1259, Loss: 2.0002524852752686\n",
            "Epoch 0, Phase train, Batch 166, Batch 808/1259, Loss: 1.8572032451629639\n",
            "Epoch 0, Phase train, Batch 166, Batch 808/1259, Loss: 1.9529725313186646\n",
            "Epoch 0, Phase train, Batch 25, Batch 809/1259, Loss: 2.0168862342834473\n",
            "Epoch 0, Phase train, Batch 25, Batch 809/1259, Loss: 2.024216651916504\n",
            "Epoch 0, Phase train, Batch 1524, Batch 810/1259, Loss: 1.8818548917770386\n",
            "Epoch 0, Phase train, Batch 1524, Batch 810/1259, Loss: 1.8433003425598145\n",
            "Epoch 0, Phase train, Batch 1320, Batch 811/1259, Loss: 1.9426193237304688\n",
            "Epoch 0, Phase train, Batch 1320, Batch 811/1259, Loss: 1.9245784282684326\n",
            "Epoch 0, Phase train, Batch 991, Batch 812/1259, Loss: 1.9374442100524902\n",
            "Epoch 0, Phase train, Batch 991, Batch 812/1259, Loss: 1.9147279262542725\n",
            "Epoch 0, Phase train, Batch 674, Batch 813/1259, Loss: 1.8713301420211792\n",
            "Epoch 0, Phase train, Batch 674, Batch 813/1259, Loss: 1.8951144218444824\n",
            "Epoch 0, Phase train, Batch 265, Batch 814/1259, Loss: 2.0303173065185547\n",
            "Epoch 0, Phase train, Batch 265, Batch 814/1259, Loss: 1.840860366821289\n",
            "Epoch 0, Phase train, Batch 1062, Batch 815/1259, Loss: 1.878543496131897\n",
            "Epoch 0, Phase train, Batch 1062, Batch 815/1259, Loss: 1.8735300302505493\n",
            "Epoch 0, Phase train, Batch 1161, Batch 816/1259, Loss: 1.9872207641601562\n",
            "Epoch 0, Phase train, Batch 1161, Batch 816/1259, Loss: 1.7688112258911133\n",
            "Epoch 0, Phase train, Batch 1005, Batch 817/1259, Loss: 1.9064509868621826\n",
            "Epoch 0, Phase train, Batch 1005, Batch 817/1259, Loss: 1.913054347038269\n",
            "Epoch 0, Phase train, Batch 800, Batch 818/1259, Loss: 1.899816632270813\n",
            "Epoch 0, Phase train, Batch 800, Batch 818/1259, Loss: 1.7729756832122803\n",
            "Epoch 0, Phase train, Batch 1513, Batch 819/1259, Loss: 1.9932093620300293\n",
            "Epoch 0, Phase train, Batch 1513, Batch 819/1259, Loss: 1.8894060850143433\n",
            "Epoch 0, Phase train, Batch 284, Batch 820/1259, Loss: 2.0455374717712402\n",
            "Epoch 0, Phase train, Batch 284, Batch 820/1259, Loss: 1.9390157461166382\n",
            "Epoch 0, Phase train, Batch 51, Batch 821/1259, Loss: 1.9267215728759766\n",
            "Epoch 0, Phase train, Batch 51, Batch 821/1259, Loss: 2.108572244644165\n",
            "Epoch 0, Phase train, Batch 222, Batch 822/1259, Loss: 2.1157422065734863\n",
            "Epoch 0, Phase train, Batch 222, Batch 822/1259, Loss: 2.0314888954162598\n",
            "Epoch 0, Phase train, Batch 1571, Batch 823/1259, Loss: 1.8263394832611084\n",
            "Epoch 0, Phase train, Batch 1571, Batch 823/1259, Loss: 1.9253129959106445\n",
            "Epoch 0, Phase train, Batch 1112, Batch 824/1259, Loss: 2.06376051902771\n",
            "Epoch 0, Phase train, Batch 1112, Batch 824/1259, Loss: 2.011035680770874\n",
            "Epoch 0, Phase train, Batch 707, Batch 825/1259, Loss: 1.9249056577682495\n",
            "Epoch 0, Phase train, Batch 707, Batch 825/1259, Loss: 1.9228166341781616\n",
            "Epoch 0, Phase train, Batch 649, Batch 826/1259, Loss: 1.9458845853805542\n",
            "Epoch 0, Phase train, Batch 649, Batch 826/1259, Loss: 1.9839706420898438\n",
            "Epoch 0, Phase train, Batch 92, Batch 827/1259, Loss: 2.0768091678619385\n",
            "Epoch 0, Phase train, Batch 92, Batch 827/1259, Loss: 1.9209868907928467\n",
            "Epoch 0, Phase train, Batch 653, Batch 828/1259, Loss: 1.9335466623306274\n",
            "Epoch 0, Phase train, Batch 653, Batch 828/1259, Loss: 2.0366523265838623\n",
            "Epoch 0, Phase train, Batch 911, Batch 829/1259, Loss: 1.9824249744415283\n",
            "Epoch 0, Phase train, Batch 911, Batch 829/1259, Loss: 1.8100837469100952\n",
            "Epoch 0, Phase train, Batch 227, Batch 830/1259, Loss: 1.9129470586776733\n",
            "Epoch 0, Phase train, Batch 227, Batch 830/1259, Loss: 1.8767752647399902\n",
            "Epoch 0, Phase train, Batch 1546, Batch 831/1259, Loss: 1.893034815788269\n",
            "Epoch 0, Phase train, Batch 1546, Batch 831/1259, Loss: 1.782942771911621\n",
            "Epoch 0, Phase train, Batch 1252, Batch 832/1259, Loss: 1.8274743556976318\n",
            "Epoch 0, Phase train, Batch 1252, Batch 832/1259, Loss: 1.7208365201950073\n",
            "Epoch 0, Phase train, Batch 545, Batch 833/1259, Loss: 2.0904805660247803\n",
            "Epoch 0, Phase train, Batch 545, Batch 833/1259, Loss: 2.1307075023651123\n",
            "Epoch 0, Phase train, Batch 602, Batch 834/1259, Loss: 2.0322020053863525\n",
            "Epoch 0, Phase train, Batch 602, Batch 834/1259, Loss: 1.818721055984497\n",
            "Epoch 0, Phase train, Batch 418, Batch 835/1259, Loss: 1.875619649887085\n",
            "Epoch 0, Phase train, Batch 418, Batch 835/1259, Loss: 1.9311788082122803\n",
            "Epoch 0, Phase train, Batch 875, Batch 836/1259, Loss: 2.045600414276123\n",
            "Epoch 0, Phase train, Batch 875, Batch 836/1259, Loss: 1.8752124309539795\n",
            "Epoch 0, Phase train, Batch 1162, Batch 837/1259, Loss: 1.8966484069824219\n",
            "Epoch 0, Phase train, Batch 1162, Batch 837/1259, Loss: 2.065082550048828\n",
            "Epoch 0, Phase train, Batch 179, Batch 838/1259, Loss: 1.877766489982605\n",
            "Epoch 0, Phase train, Batch 179, Batch 838/1259, Loss: 2.1883444786071777\n",
            "Epoch 0, Phase train, Batch 1265, Batch 839/1259, Loss: 1.8793381452560425\n",
            "Epoch 0, Phase train, Batch 1265, Batch 839/1259, Loss: 1.77777898311615\n",
            "Epoch 0, Phase train, Batch 1361, Batch 840/1259, Loss: 1.776174545288086\n",
            "Epoch 0, Phase train, Batch 1361, Batch 840/1259, Loss: 1.8864057064056396\n",
            "Epoch 0, Phase train, Batch 1292, Batch 841/1259, Loss: 1.7986130714416504\n",
            "Epoch 0, Phase train, Batch 1292, Batch 841/1259, Loss: 2.1122348308563232\n",
            "Epoch 0, Phase train, Batch 1179, Batch 842/1259, Loss: 1.6785622835159302\n",
            "Epoch 0, Phase train, Batch 1179, Batch 842/1259, Loss: 1.7975842952728271\n",
            "Epoch 0, Phase train, Batch 749, Batch 843/1259, Loss: 2.1867434978485107\n",
            "Epoch 0, Phase train, Batch 749, Batch 843/1259, Loss: 1.8724238872528076\n",
            "Epoch 0, Phase train, Batch 709, Batch 844/1259, Loss: 2.041807174682617\n",
            "Epoch 0, Phase train, Batch 709, Batch 844/1259, Loss: 1.90053391456604\n",
            "Epoch 0, Phase train, Batch 376, Batch 845/1259, Loss: 1.8276100158691406\n",
            "Epoch 0, Phase train, Batch 376, Batch 845/1259, Loss: 1.8921501636505127\n",
            "Epoch 0, Phase train, Batch 731, Batch 846/1259, Loss: 1.9692261219024658\n",
            "Epoch 0, Phase train, Batch 731, Batch 846/1259, Loss: 1.9526231288909912\n",
            "Epoch 0, Phase train, Batch 156, Batch 847/1259, Loss: 2.0128540992736816\n",
            "Epoch 0, Phase train, Batch 156, Batch 847/1259, Loss: 1.9777984619140625\n",
            "Epoch 0, Phase train, Batch 186, Batch 848/1259, Loss: 1.9772602319717407\n",
            "Epoch 0, Phase train, Batch 186, Batch 848/1259, Loss: 1.9824730157852173\n",
            "Epoch 0, Phase train, Batch 339, Batch 849/1259, Loss: 1.9173476696014404\n",
            "Epoch 0, Phase train, Batch 339, Batch 849/1259, Loss: 1.9642908573150635\n",
            "Epoch 0, Phase train, Batch 868, Batch 850/1259, Loss: 2.0783443450927734\n",
            "Epoch 0, Phase train, Batch 868, Batch 850/1259, Loss: 1.9137954711914062\n",
            "Epoch 0, Phase train, Batch 1214, Batch 851/1259, Loss: 1.8270138502120972\n",
            "Epoch 0, Phase train, Batch 1214, Batch 851/1259, Loss: 1.9041552543640137\n",
            "Epoch 0, Phase train, Batch 754, Batch 852/1259, Loss: 1.860992431640625\n",
            "Epoch 0, Phase train, Batch 754, Batch 852/1259, Loss: 1.985154628753662\n",
            "Epoch 0, Phase train, Batch 262, Batch 853/1259, Loss: 1.891520380973816\n",
            "Epoch 0, Phase train, Batch 262, Batch 853/1259, Loss: 2.047471284866333\n",
            "Epoch 0, Phase train, Batch 971, Batch 854/1259, Loss: 1.8093167543411255\n",
            "Epoch 0, Phase train, Batch 971, Batch 854/1259, Loss: 1.9146310091018677\n",
            "Epoch 0, Phase train, Batch 802, Batch 855/1259, Loss: 1.8929983377456665\n",
            "Epoch 0, Phase train, Batch 802, Batch 855/1259, Loss: 1.78481125831604\n",
            "Epoch 0, Phase train, Batch 64, Batch 856/1259, Loss: 2.086801052093506\n",
            "Epoch 0, Phase train, Batch 64, Batch 856/1259, Loss: 1.9614540338516235\n",
            "Epoch 0, Phase train, Batch 1198, Batch 857/1259, Loss: 1.6544098854064941\n",
            "Epoch 0, Phase train, Batch 1198, Batch 857/1259, Loss: 1.8795822858810425\n",
            "Epoch 0, Phase train, Batch 1102, Batch 858/1259, Loss: 2.1509475708007812\n",
            "Epoch 0, Phase train, Batch 1102, Batch 858/1259, Loss: 1.8431978225708008\n",
            "Epoch 0, Phase train, Batch 1391, Batch 859/1259, Loss: 1.8793541193008423\n",
            "Epoch 0, Phase train, Batch 1391, Batch 859/1259, Loss: 1.7947503328323364\n",
            "Epoch 0, Phase train, Batch 1340, Batch 860/1259, Loss: 1.9640247821807861\n",
            "Epoch 0, Phase train, Batch 1340, Batch 860/1259, Loss: 1.6844946146011353\n",
            "Epoch 0, Phase train, Batch 1316, Batch 861/1259, Loss: 1.9008264541625977\n",
            "Epoch 0, Phase train, Batch 1316, Batch 861/1259, Loss: 1.870535969734192\n",
            "Epoch 0, Phase train, Batch 415, Batch 862/1259, Loss: 1.7869161367416382\n",
            "Epoch 0, Phase train, Batch 415, Batch 862/1259, Loss: 1.9668060541152954\n",
            "Epoch 0, Phase train, Batch 1164, Batch 863/1259, Loss: 1.8496233224868774\n",
            "Epoch 0, Phase train, Batch 1164, Batch 863/1259, Loss: 1.9806740283966064\n",
            "Epoch 0, Phase train, Batch 1308, Batch 864/1259, Loss: 1.7775304317474365\n",
            "Epoch 0, Phase train, Batch 1308, Batch 864/1259, Loss: 1.694161295890808\n",
            "Epoch 0, Phase train, Batch 117, Batch 865/1259, Loss: 1.9411108493804932\n",
            "Epoch 0, Phase train, Batch 117, Batch 865/1259, Loss: 2.0025031566619873\n",
            "Epoch 0, Phase train, Batch 1321, Batch 866/1259, Loss: 1.7683042287826538\n",
            "Epoch 0, Phase train, Batch 1321, Batch 866/1259, Loss: 1.8952105045318604\n",
            "Epoch 0, Phase train, Batch 863, Batch 867/1259, Loss: 1.7957595586776733\n",
            "Epoch 0, Phase train, Batch 863, Batch 867/1259, Loss: 1.9155117273330688\n",
            "Epoch 0, Phase train, Batch 279, Batch 868/1259, Loss: 1.9035372734069824\n",
            "Epoch 0, Phase train, Batch 279, Batch 868/1259, Loss: 1.959357738494873\n",
            "Epoch 0, Phase train, Batch 1553, Batch 869/1259, Loss: 1.7058385610580444\n",
            "Epoch 0, Phase train, Batch 1553, Batch 869/1259, Loss: 1.9012229442596436\n",
            "Epoch 0, Phase train, Batch 999, Batch 870/1259, Loss: 1.9547967910766602\n",
            "Epoch 0, Phase train, Batch 999, Batch 870/1259, Loss: 1.8296256065368652\n",
            "Epoch 0, Phase train, Batch 1094, Batch 871/1259, Loss: 1.8290613889694214\n",
            "Epoch 0, Phase train, Batch 1094, Batch 871/1259, Loss: 1.9851809740066528\n",
            "Epoch 0, Phase train, Batch 1317, Batch 872/1259, Loss: 1.7249089479446411\n",
            "Epoch 0, Phase train, Batch 1317, Batch 872/1259, Loss: 1.72855806350708\n",
            "Epoch 0, Phase train, Batch 560, Batch 873/1259, Loss: 2.004213333129883\n",
            "Epoch 0, Phase train, Batch 560, Batch 873/1259, Loss: 2.1413772106170654\n",
            "Epoch 0, Phase train, Batch 1147, Batch 874/1259, Loss: 1.8068641424179077\n",
            "Epoch 0, Phase train, Batch 1147, Batch 874/1259, Loss: 1.9813165664672852\n",
            "Epoch 0, Phase train, Batch 1281, Batch 875/1259, Loss: 1.743351697921753\n",
            "Epoch 0, Phase train, Batch 1281, Batch 875/1259, Loss: 1.916671872138977\n",
            "Epoch 0, Phase train, Batch 1039, Batch 876/1259, Loss: 1.9481723308563232\n",
            "Epoch 0, Phase train, Batch 1039, Batch 876/1259, Loss: 1.6637451648712158\n",
            "Epoch 0, Phase train, Batch 436, Batch 877/1259, Loss: 1.805599570274353\n",
            "Epoch 0, Phase train, Batch 436, Batch 877/1259, Loss: 1.8767199516296387\n",
            "Epoch 0, Phase train, Batch 203, Batch 878/1259, Loss: 1.9875471591949463\n",
            "Epoch 0, Phase train, Batch 203, Batch 878/1259, Loss: 2.028945207595825\n",
            "Epoch 0, Phase train, Batch 1402, Batch 879/1259, Loss: 1.8722933530807495\n",
            "Epoch 0, Phase train, Batch 1402, Batch 879/1259, Loss: 1.812595009803772\n",
            "Epoch 0, Phase train, Batch 927, Batch 880/1259, Loss: 1.9237736463546753\n",
            "Epoch 0, Phase train, Batch 927, Batch 880/1259, Loss: 1.8965682983398438\n",
            "Epoch 0, Phase train, Batch 676, Batch 881/1259, Loss: 2.0265188217163086\n",
            "Epoch 0, Phase train, Batch 676, Batch 881/1259, Loss: 1.9568653106689453\n",
            "Epoch 0, Phase train, Batch 523, Batch 882/1259, Loss: 1.9964994192123413\n",
            "Epoch 0, Phase train, Batch 523, Batch 882/1259, Loss: 1.8517369031906128\n",
            "Epoch 0, Phase train, Batch 1515, Batch 883/1259, Loss: 1.8543707132339478\n",
            "Epoch 0, Phase train, Batch 1515, Batch 883/1259, Loss: 1.742588996887207\n",
            "Epoch 0, Phase train, Batch 1399, Batch 884/1259, Loss: 1.8527098894119263\n",
            "Epoch 0, Phase train, Batch 1399, Batch 884/1259, Loss: 1.8838709592819214\n",
            "Epoch 0, Phase train, Batch 659, Batch 885/1259, Loss: 2.08345627784729\n",
            "Epoch 0, Phase train, Batch 659, Batch 885/1259, Loss: 1.9607435464859009\n",
            "Epoch 0, Phase train, Batch 1016, Batch 886/1259, Loss: 1.7759861946105957\n",
            "Epoch 0, Phase train, Batch 1016, Batch 886/1259, Loss: 1.9588139057159424\n",
            "Epoch 0, Phase train, Batch 155, Batch 887/1259, Loss: 2.0189263820648193\n",
            "Epoch 0, Phase train, Batch 155, Batch 887/1259, Loss: 1.8656978607177734\n",
            "Epoch 0, Phase train, Batch 128, Batch 888/1259, Loss: 2.003108024597168\n",
            "Epoch 0, Phase train, Batch 128, Batch 888/1259, Loss: 1.8667103052139282\n",
            "Epoch 0, Phase train, Batch 641, Batch 889/1259, Loss: 1.9010432958602905\n",
            "Epoch 0, Phase train, Batch 641, Batch 889/1259, Loss: 1.9927749633789062\n",
            "Epoch 0, Phase train, Batch 1548, Batch 890/1259, Loss: 1.8037662506103516\n",
            "Epoch 0, Phase train, Batch 1548, Batch 890/1259, Loss: 1.6178194284439087\n",
            "Epoch 0, Phase train, Batch 73, Batch 891/1259, Loss: 1.8597217798233032\n",
            "Epoch 0, Phase train, Batch 73, Batch 891/1259, Loss: 2.011000871658325\n",
            "Epoch 0, Phase train, Batch 288, Batch 892/1259, Loss: 1.8306217193603516\n",
            "Epoch 0, Phase train, Batch 288, Batch 892/1259, Loss: 1.8726252317428589\n",
            "Epoch 0, Phase train, Batch 263, Batch 893/1259, Loss: 1.7927260398864746\n",
            "Epoch 0, Phase train, Batch 263, Batch 893/1259, Loss: 1.8452425003051758\n",
            "Epoch 0, Phase train, Batch 210, Batch 894/1259, Loss: 1.8254287242889404\n",
            "Epoch 0, Phase train, Batch 210, Batch 894/1259, Loss: 1.8398356437683105\n",
            "Epoch 0, Phase train, Batch 546, Batch 895/1259, Loss: 2.175581216812134\n",
            "Epoch 0, Phase train, Batch 546, Batch 895/1259, Loss: 1.9609301090240479\n",
            "Epoch 0, Phase train, Batch 36, Batch 896/1259, Loss: 2.056365966796875\n",
            "Epoch 0, Phase train, Batch 36, Batch 896/1259, Loss: 2.077620267868042\n",
            "Epoch 0, Phase train, Batch 534, Batch 897/1259, Loss: 1.7351607084274292\n",
            "Epoch 0, Phase train, Batch 534, Batch 897/1259, Loss: 1.6867780685424805\n",
            "Epoch 0, Phase train, Batch 665, Batch 898/1259, Loss: 1.7291908264160156\n",
            "Epoch 0, Phase train, Batch 665, Batch 898/1259, Loss: 1.927669882774353\n",
            "Epoch 0, Phase train, Batch 625, Batch 899/1259, Loss: 2.0000970363616943\n",
            "Epoch 0, Phase train, Batch 625, Batch 899/1259, Loss: 2.110700845718384\n",
            "Epoch 0, Phase train, Batch 499, Batch 900/1259, Loss: 1.7482234239578247\n",
            "Epoch 0, Phase train, Batch 499, Batch 900/1259, Loss: 1.9130555391311646\n",
            "Epoch 0, Phase train, Batch 845, Batch 901/1259, Loss: 1.8196347951889038\n",
            "Epoch 0, Phase train, Batch 845, Batch 901/1259, Loss: 1.8523048162460327\n",
            "Epoch 0, Phase train, Batch 407, Batch 902/1259, Loss: 1.8051776885986328\n",
            "Epoch 0, Phase train, Batch 407, Batch 902/1259, Loss: 1.9380193948745728\n",
            "Epoch 0, Phase train, Batch 391, Batch 903/1259, Loss: 1.8875230550765991\n",
            "Epoch 0, Phase train, Batch 391, Batch 903/1259, Loss: 1.8254971504211426\n",
            "Epoch 0, Phase train, Batch 1204, Batch 904/1259, Loss: 1.9008029699325562\n",
            "Epoch 0, Phase train, Batch 1204, Batch 904/1259, Loss: 1.7765392065048218\n",
            "Epoch 0, Phase train, Batch 851, Batch 905/1259, Loss: 1.8465559482574463\n",
            "Epoch 0, Phase train, Batch 851, Batch 905/1259, Loss: 1.914901614189148\n",
            "Epoch 0, Phase train, Batch 1060, Batch 906/1259, Loss: 2.166269063949585\n",
            "Epoch 0, Phase train, Batch 1060, Batch 906/1259, Loss: 2.0684831142425537\n",
            "Epoch 0, Phase train, Batch 333, Batch 907/1259, Loss: 1.997559666633606\n",
            "Epoch 0, Phase train, Batch 333, Batch 907/1259, Loss: 1.8998538255691528\n",
            "Epoch 0, Phase train, Batch 134, Batch 908/1259, Loss: 1.98317289352417\n",
            "Epoch 0, Phase train, Batch 134, Batch 908/1259, Loss: 1.8426756858825684\n",
            "Epoch 0, Phase train, Batch 273, Batch 909/1259, Loss: 1.8666027784347534\n",
            "Epoch 0, Phase train, Batch 273, Batch 909/1259, Loss: 1.933179259300232\n",
            "Epoch 0, Phase train, Batch 213, Batch 910/1259, Loss: 1.7814617156982422\n",
            "Epoch 0, Phase train, Batch 213, Batch 910/1259, Loss: 1.9402357339859009\n",
            "Epoch 0, Phase train, Batch 204, Batch 911/1259, Loss: 1.8579224348068237\n",
            "Epoch 0, Phase train, Batch 204, Batch 911/1259, Loss: 1.9279717206954956\n",
            "Epoch 0, Phase train, Batch 211, Batch 912/1259, Loss: 1.968698501586914\n",
            "Epoch 0, Phase train, Batch 211, Batch 912/1259, Loss: 2.03979754447937\n",
            "Epoch 0, Phase train, Batch 93, Batch 913/1259, Loss: 2.0671863555908203\n",
            "Epoch 0, Phase train, Batch 93, Batch 913/1259, Loss: 1.8268929719924927\n",
            "Epoch 0, Phase train, Batch 149, Batch 914/1259, Loss: 1.8205703496932983\n",
            "Epoch 0, Phase train, Batch 149, Batch 914/1259, Loss: 1.8074971437454224\n",
            "Epoch 0, Phase train, Batch 260, Batch 915/1259, Loss: 1.8688644170761108\n",
            "Epoch 0, Phase train, Batch 260, Batch 915/1259, Loss: 2.066199779510498\n",
            "Epoch 0, Phase train, Batch 177, Batch 916/1259, Loss: 1.900312900543213\n",
            "Epoch 0, Phase train, Batch 177, Batch 916/1259, Loss: 1.8700605630874634\n",
            "Epoch 0, Phase train, Batch 38, Batch 917/1259, Loss: 2.039163589477539\n",
            "Epoch 0, Phase train, Batch 38, Batch 917/1259, Loss: 1.918054461479187\n",
            "Epoch 0, Phase train, Batch 708, Batch 918/1259, Loss: 1.755672574043274\n",
            "Epoch 0, Phase train, Batch 708, Batch 918/1259, Loss: 1.8542141914367676\n",
            "Epoch 0, Phase train, Batch 689, Batch 919/1259, Loss: 1.878352403640747\n",
            "Epoch 0, Phase train, Batch 689, Batch 919/1259, Loss: 1.8722593784332275\n",
            "Epoch 0, Phase train, Batch 1017, Batch 920/1259, Loss: 1.927165150642395\n",
            "Epoch 0, Phase train, Batch 1017, Batch 920/1259, Loss: 1.8550492525100708\n",
            "Epoch 0, Phase train, Batch 1322, Batch 921/1259, Loss: 1.821087121963501\n",
            "Epoch 0, Phase train, Batch 1322, Batch 921/1259, Loss: 1.9036941528320312\n",
            "Epoch 0, Phase train, Batch 133, Batch 922/1259, Loss: 1.920986294746399\n",
            "Epoch 0, Phase train, Batch 133, Batch 922/1259, Loss: 2.0539567470550537\n",
            "Epoch 0, Phase train, Batch 694, Batch 923/1259, Loss: 1.9398515224456787\n",
            "Epoch 0, Phase train, Batch 694, Batch 923/1259, Loss: 1.990395426750183\n",
            "Epoch 0, Phase train, Batch 679, Batch 924/1259, Loss: 1.967437505722046\n",
            "Epoch 0, Phase train, Batch 679, Batch 924/1259, Loss: 1.9331848621368408\n",
            "Epoch 0, Phase train, Batch 810, Batch 925/1259, Loss: 1.9442377090454102\n",
            "Epoch 0, Phase train, Batch 810, Batch 925/1259, Loss: 1.7823131084442139\n",
            "Epoch 0, Phase train, Batch 929, Batch 926/1259, Loss: 1.9871057271957397\n",
            "Epoch 0, Phase train, Batch 929, Batch 926/1259, Loss: 2.029350996017456\n",
            "Epoch 0, Phase train, Batch 1486, Batch 927/1259, Loss: 1.8809576034545898\n",
            "Epoch 0, Phase train, Batch 1486, Batch 927/1259, Loss: 1.8749440908432007\n",
            "Epoch 0, Phase train, Batch 514, Batch 928/1259, Loss: 1.7883561849594116\n",
            "Epoch 0, Phase train, Batch 514, Batch 928/1259, Loss: 1.8495306968688965\n",
            "Epoch 0, Phase train, Batch 1492, Batch 929/1259, Loss: 1.9012473821640015\n",
            "Epoch 0, Phase train, Batch 1492, Batch 929/1259, Loss: 1.7968188524246216\n",
            "Epoch 0, Phase train, Batch 324, Batch 930/1259, Loss: 1.982000470161438\n",
            "Epoch 0, Phase train, Batch 324, Batch 930/1259, Loss: 1.7980177402496338\n",
            "Epoch 0, Phase train, Batch 95, Batch 931/1259, Loss: 1.9334379434585571\n",
            "Epoch 0, Phase train, Batch 95, Batch 931/1259, Loss: 2.0808510780334473\n",
            "Epoch 0, Phase train, Batch 83, Batch 932/1259, Loss: 1.9145723581314087\n",
            "Epoch 0, Phase train, Batch 83, Batch 932/1259, Loss: 1.9228769540786743\n",
            "Epoch 0, Phase train, Batch 1046, Batch 933/1259, Loss: 2.025320053100586\n",
            "Epoch 0, Phase train, Batch 1046, Batch 933/1259, Loss: 1.9583624601364136\n",
            "Epoch 0, Phase train, Batch 712, Batch 934/1259, Loss: 1.8765274286270142\n",
            "Epoch 0, Phase train, Batch 712, Batch 934/1259, Loss: 1.9412283897399902\n",
            "Epoch 0, Phase train, Batch 745, Batch 935/1259, Loss: 1.700493335723877\n",
            "Epoch 0, Phase train, Batch 745, Batch 935/1259, Loss: 1.8685672283172607\n",
            "Epoch 0, Phase train, Batch 449, Batch 936/1259, Loss: 1.799983263015747\n",
            "Epoch 0, Phase train, Batch 449, Batch 936/1259, Loss: 1.8159552812576294\n",
            "Epoch 0, Phase train, Batch 1356, Batch 937/1259, Loss: 1.7680939435958862\n",
            "Epoch 0, Phase train, Batch 1356, Batch 937/1259, Loss: 1.7397068738937378\n",
            "Epoch 0, Phase train, Batch 0, Batch 938/1259, Loss: 2.1133174896240234\n",
            "Epoch 0, Phase train, Batch 0, Batch 938/1259, Loss: 1.8837579488754272\n",
            "Epoch 0, Phase train, Batch 103, Batch 939/1259, Loss: 1.996163249015808\n",
            "Epoch 0, Phase train, Batch 103, Batch 939/1259, Loss: 1.9772701263427734\n",
            "Epoch 0, Phase train, Batch 382, Batch 940/1259, Loss: 1.838698387145996\n",
            "Epoch 0, Phase train, Batch 382, Batch 940/1259, Loss: 1.9840513467788696\n",
            "Epoch 0, Phase train, Batch 291, Batch 941/1259, Loss: 1.8208959102630615\n",
            "Epoch 0, Phase train, Batch 291, Batch 941/1259, Loss: 1.8807722330093384\n",
            "Epoch 0, Phase train, Batch 1299, Batch 942/1259, Loss: 1.7176291942596436\n",
            "Epoch 0, Phase train, Batch 1299, Batch 942/1259, Loss: 1.717392086982727\n",
            "Epoch 0, Phase train, Batch 1384, Batch 943/1259, Loss: 1.7920151948928833\n",
            "Epoch 0, Phase train, Batch 1384, Batch 943/1259, Loss: 1.8393367528915405\n",
            "Epoch 0, Phase train, Batch 1043, Batch 944/1259, Loss: 1.895827054977417\n",
            "Epoch 0, Phase train, Batch 1043, Batch 944/1259, Loss: 1.833998680114746\n",
            "Epoch 0, Phase train, Batch 286, Batch 945/1259, Loss: 1.924285888671875\n",
            "Epoch 0, Phase train, Batch 286, Batch 945/1259, Loss: 2.003109931945801\n",
            "Epoch 0, Phase train, Batch 1382, Batch 946/1259, Loss: 1.8435187339782715\n",
            "Epoch 0, Phase train, Batch 1382, Batch 946/1259, Loss: 1.7616387605667114\n",
            "Epoch 0, Phase train, Batch 735, Batch 947/1259, Loss: 2.0054261684417725\n",
            "Epoch 0, Phase train, Batch 735, Batch 947/1259, Loss: 1.9025968313217163\n",
            "Epoch 0, Phase train, Batch 411, Batch 948/1259, Loss: 1.8884419202804565\n",
            "Epoch 0, Phase train, Batch 411, Batch 948/1259, Loss: 1.7885937690734863\n",
            "Epoch 0, Phase train, Batch 403, Batch 949/1259, Loss: 1.8773488998413086\n",
            "Epoch 0, Phase train, Batch 403, Batch 949/1259, Loss: 1.8505604267120361\n",
            "Epoch 0, Phase train, Batch 136, Batch 950/1259, Loss: 1.8906292915344238\n",
            "Epoch 0, Phase train, Batch 136, Batch 950/1259, Loss: 1.7395005226135254\n",
            "Epoch 0, Phase train, Batch 193, Batch 951/1259, Loss: 1.8891414403915405\n",
            "Epoch 0, Phase train, Batch 193, Batch 951/1259, Loss: 1.797008991241455\n",
            "Epoch 0, Phase train, Batch 1143, Batch 952/1259, Loss: 1.982602834701538\n",
            "Epoch 0, Phase train, Batch 1143, Batch 952/1259, Loss: 1.7943193912506104\n",
            "Epoch 0, Phase train, Batch 1030, Batch 953/1259, Loss: 1.9820201396942139\n",
            "Epoch 0, Phase train, Batch 1030, Batch 953/1259, Loss: 1.7853307723999023\n",
            "Epoch 0, Phase train, Batch 158, Batch 954/1259, Loss: 1.7671188116073608\n",
            "Epoch 0, Phase train, Batch 158, Batch 954/1259, Loss: 1.7569416761398315\n",
            "Epoch 0, Phase train, Batch 232, Batch 955/1259, Loss: 1.8808537721633911\n",
            "Epoch 0, Phase train, Batch 232, Batch 955/1259, Loss: 1.9009060859680176\n",
            "Epoch 0, Phase train, Batch 6, Batch 956/1259, Loss: 2.097564935684204\n",
            "Epoch 0, Phase train, Batch 6, Batch 956/1259, Loss: 2.0843632221221924\n",
            "Epoch 0, Phase train, Batch 1337, Batch 957/1259, Loss: 1.7317535877227783\n",
            "Epoch 0, Phase train, Batch 1337, Batch 957/1259, Loss: 1.836102843284607\n",
            "Epoch 0, Phase train, Batch 1407, Batch 958/1259, Loss: 1.7582074403762817\n",
            "Epoch 0, Phase train, Batch 1407, Batch 958/1259, Loss: 1.9088478088378906\n",
            "Epoch 0, Phase train, Batch 1249, Batch 959/1259, Loss: 1.856554388999939\n",
            "Epoch 0, Phase train, Batch 1249, Batch 959/1259, Loss: 1.949588656425476\n",
            "Epoch 0, Phase train, Batch 812, Batch 960/1259, Loss: 1.7695449590682983\n",
            "Epoch 0, Phase train, Batch 812, Batch 960/1259, Loss: 1.8446097373962402\n",
            "Epoch 0, Phase train, Batch 1554, Batch 961/1259, Loss: 1.8589982986450195\n",
            "Epoch 0, Phase train, Batch 1554, Batch 961/1259, Loss: 1.7232259511947632\n",
            "Epoch 0, Phase train, Batch 1532, Batch 962/1259, Loss: 1.8382500410079956\n",
            "Epoch 0, Phase train, Batch 1532, Batch 962/1259, Loss: 1.9626485109329224\n",
            "Epoch 0, Phase train, Batch 760, Batch 963/1259, Loss: 1.9717576503753662\n",
            "Epoch 0, Phase train, Batch 760, Batch 963/1259, Loss: 1.8257222175598145\n",
            "Epoch 0, Phase train, Batch 431, Batch 964/1259, Loss: 1.922796368598938\n",
            "Epoch 0, Phase train, Batch 431, Batch 964/1259, Loss: 1.8288525342941284\n",
            "Epoch 0, Phase train, Batch 106, Batch 965/1259, Loss: 1.9259394407272339\n",
            "Epoch 0, Phase train, Batch 106, Batch 965/1259, Loss: 2.040391206741333\n",
            "Epoch 0, Phase train, Batch 1157, Batch 966/1259, Loss: 1.9082895517349243\n",
            "Epoch 0, Phase train, Batch 1157, Batch 966/1259, Loss: 1.9259427785873413\n",
            "Epoch 0, Phase train, Batch 1276, Batch 967/1259, Loss: 1.7700163125991821\n",
            "Epoch 0, Phase train, Batch 1276, Batch 967/1259, Loss: 1.7713154554367065\n",
            "Epoch 0, Phase train, Batch 961, Batch 968/1259, Loss: 1.957809567451477\n",
            "Epoch 0, Phase train, Batch 961, Batch 968/1259, Loss: 1.9224138259887695\n",
            "Epoch 0, Phase train, Batch 764, Batch 969/1259, Loss: 1.790501594543457\n",
            "Epoch 0, Phase train, Batch 764, Batch 969/1259, Loss: 1.865882158279419\n",
            "Epoch 0, Phase train, Batch 866, Batch 970/1259, Loss: 2.515404224395752\n",
            "Epoch 0, Phase train, Batch 866, Batch 970/1259, Loss: 2.5834157466888428\n",
            "Epoch 0, Phase train, Batch 937, Batch 971/1259, Loss: 1.8940911293029785\n",
            "Epoch 0, Phase train, Batch 937, Batch 971/1259, Loss: 1.8390384912490845\n",
            "Epoch 0, Phase train, Batch 559, Batch 972/1259, Loss: 1.9588390588760376\n",
            "Epoch 0, Phase train, Batch 559, Batch 972/1259, Loss: 1.9686435461044312\n",
            "Epoch 0, Phase train, Batch 1099, Batch 973/1259, Loss: 1.9739978313446045\n",
            "Epoch 0, Phase train, Batch 1099, Batch 973/1259, Loss: 1.8231892585754395\n",
            "Epoch 0, Phase train, Batch 913, Batch 974/1259, Loss: 1.9092607498168945\n",
            "Epoch 0, Phase train, Batch 913, Batch 974/1259, Loss: 1.8829773664474487\n",
            "Epoch 0, Phase train, Batch 476, Batch 975/1259, Loss: 1.9101877212524414\n",
            "Epoch 0, Phase train, Batch 476, Batch 975/1259, Loss: 1.84970223903656\n",
            "Epoch 0, Phase train, Batch 1338, Batch 976/1259, Loss: 1.6422748565673828\n",
            "Epoch 0, Phase train, Batch 1338, Batch 976/1259, Loss: 1.6711156368255615\n",
            "Epoch 0, Phase train, Batch 907, Batch 977/1259, Loss: 1.7839878797531128\n",
            "Epoch 0, Phase train, Batch 907, Batch 977/1259, Loss: 2.1777195930480957\n",
            "Epoch 0, Phase train, Batch 583, Batch 978/1259, Loss: 1.8300871849060059\n",
            "Epoch 0, Phase train, Batch 583, Batch 978/1259, Loss: 2.0262556076049805\n",
            "Epoch 0, Phase train, Batch 461, Batch 979/1259, Loss: 1.7284491062164307\n",
            "Epoch 0, Phase train, Batch 461, Batch 979/1259, Loss: 1.7272977828979492\n",
            "Epoch 0, Phase train, Batch 898, Batch 980/1259, Loss: 2.0438425540924072\n",
            "Epoch 0, Phase train, Batch 898, Batch 980/1259, Loss: 1.9386829137802124\n",
            "Epoch 0, Phase train, Batch 1511, Batch 981/1259, Loss: 1.9195963144302368\n",
            "Epoch 0, Phase train, Batch 1511, Batch 981/1259, Loss: 1.9065226316452026\n",
            "Epoch 0, Phase train, Batch 1187, Batch 982/1259, Loss: 1.7978124618530273\n",
            "Epoch 0, Phase train, Batch 1187, Batch 982/1259, Loss: 1.6895978450775146\n",
            "Epoch 0, Phase train, Batch 173, Batch 983/1259, Loss: 1.7187695503234863\n",
            "Epoch 0, Phase train, Batch 173, Batch 983/1259, Loss: 1.816556692123413\n",
            "Epoch 0, Phase train, Batch 385, Batch 984/1259, Loss: 1.6931486129760742\n",
            "Epoch 0, Phase train, Batch 385, Batch 984/1259, Loss: 1.97713041305542\n",
            "Epoch 0, Phase train, Batch 82, Batch 985/1259, Loss: 2.015920639038086\n",
            "Epoch 0, Phase train, Batch 82, Batch 985/1259, Loss: 1.6502991914749146\n",
            "Epoch 0, Phase train, Batch 1328, Batch 986/1259, Loss: 2.008855104446411\n",
            "Epoch 0, Phase train, Batch 1328, Batch 986/1259, Loss: 2.0629467964172363\n",
            "Epoch 0, Phase train, Batch 1481, Batch 987/1259, Loss: 1.8436863422393799\n",
            "Epoch 0, Phase train, Batch 1481, Batch 987/1259, Loss: 1.8643649816513062\n",
            "Epoch 0, Phase train, Batch 960, Batch 988/1259, Loss: 1.8268793821334839\n",
            "Epoch 0, Phase train, Batch 960, Batch 988/1259, Loss: 1.8114378452301025\n",
            "Epoch 0, Phase train, Batch 105, Batch 989/1259, Loss: 2.033243417739868\n",
            "Epoch 0, Phase train, Batch 105, Batch 989/1259, Loss: 2.0182294845581055\n",
            "Epoch 0, Phase train, Batch 440, Batch 990/1259, Loss: 1.9023473262786865\n",
            "Epoch 0, Phase train, Batch 440, Batch 990/1259, Loss: 1.8064751625061035\n",
            "Epoch 0, Phase train, Batch 21, Batch 991/1259, Loss: 1.9501477479934692\n",
            "Epoch 0, Phase train, Batch 21, Batch 991/1259, Loss: 1.8627116680145264\n",
            "Epoch 0, Phase train, Batch 1332, Batch 992/1259, Loss: 1.806105613708496\n",
            "Epoch 0, Phase train, Batch 1332, Batch 992/1259, Loss: 1.726603388786316\n",
            "Epoch 0, Phase train, Batch 854, Batch 993/1259, Loss: 2.0915591716766357\n",
            "Epoch 0, Phase train, Batch 854, Batch 993/1259, Loss: 1.900121808052063\n",
            "Epoch 0, Phase train, Batch 1295, Batch 994/1259, Loss: 1.8231192827224731\n",
            "Epoch 0, Phase train, Batch 1295, Batch 994/1259, Loss: 1.7700430154800415\n",
            "Epoch 0, Phase train, Batch 922, Batch 995/1259, Loss: 1.9589813947677612\n",
            "Epoch 0, Phase train, Batch 922, Batch 995/1259, Loss: 1.8634753227233887\n",
            "Epoch 0, Phase train, Batch 480, Batch 996/1259, Loss: 1.745970368385315\n",
            "Epoch 0, Phase train, Batch 480, Batch 996/1259, Loss: 1.730607032775879\n",
            "Epoch 0, Phase train, Batch 996, Batch 997/1259, Loss: 1.9076439142227173\n",
            "Epoch 0, Phase train, Batch 996, Batch 997/1259, Loss: 2.007707118988037\n",
            "Epoch 0, Phase train, Batch 958, Batch 998/1259, Loss: 1.6480528116226196\n",
            "Epoch 0, Phase train, Batch 958, Batch 998/1259, Loss: 1.971583604812622\n",
            "Epoch 0, Phase train, Batch 295, Batch 999/1259, Loss: 2.0291950702667236\n",
            "Epoch 0, Phase train, Batch 295, Batch 999/1259, Loss: 1.8246058225631714\n",
            "Epoch 0, Phase train, Batch 785, Batch 1000/1259, Loss: 1.764262318611145\n",
            "Epoch 0, Phase train, Batch 785, Batch 1000/1259, Loss: 1.943181037902832\n",
            "Epoch 0, Phase train, Batch 1370, Batch 1001/1259, Loss: 1.7027950286865234\n",
            "Epoch 0, Phase train, Batch 1370, Batch 1001/1259, Loss: 1.7999298572540283\n",
            "Epoch 0, Phase train, Batch 987, Batch 1002/1259, Loss: 1.8981293439865112\n",
            "Epoch 0, Phase train, Batch 987, Batch 1002/1259, Loss: 1.9869227409362793\n",
            "Epoch 0, Phase train, Batch 769, Batch 1003/1259, Loss: 1.8719735145568848\n",
            "Epoch 0, Phase train, Batch 769, Batch 1003/1259, Loss: 1.9129985570907593\n",
            "Epoch 0, Phase train, Batch 521, Batch 1004/1259, Loss: 1.7570600509643555\n",
            "Epoch 0, Phase train, Batch 521, Batch 1004/1259, Loss: 1.8174941539764404\n",
            "Epoch 0, Phase train, Batch 292, Batch 1005/1259, Loss: 1.888785481452942\n",
            "Epoch 0, Phase train, Batch 292, Batch 1005/1259, Loss: 2.049463987350464\n",
            "Epoch 0, Phase train, Batch 44, Batch 1006/1259, Loss: 2.1406168937683105\n",
            "Epoch 0, Phase train, Batch 44, Batch 1006/1259, Loss: 2.0475778579711914\n",
            "Epoch 0, Phase train, Batch 830, Batch 1007/1259, Loss: 1.8033761978149414\n",
            "Epoch 0, Phase train, Batch 830, Batch 1007/1259, Loss: 1.847677230834961\n",
            "Epoch 0, Phase train, Batch 444, Batch 1008/1259, Loss: 1.7690821886062622\n",
            "Epoch 0, Phase train, Batch 444, Batch 1008/1259, Loss: 1.8021941184997559\n",
            "Epoch 0, Phase train, Batch 1025, Batch 1009/1259, Loss: 1.7215490341186523\n",
            "Epoch 0, Phase train, Batch 1025, Batch 1009/1259, Loss: 1.894690990447998\n",
            "Epoch 0, Phase train, Batch 497, Batch 1010/1259, Loss: 1.772605061531067\n",
            "Epoch 0, Phase train, Batch 497, Batch 1010/1259, Loss: 1.781012773513794\n",
            "Epoch 0, Phase train, Batch 1426, Batch 1011/1259, Loss: 1.924936056137085\n",
            "Epoch 0, Phase train, Batch 1426, Batch 1011/1259, Loss: 1.7616961002349854\n",
            "Epoch 0, Phase train, Batch 1100, Batch 1012/1259, Loss: 1.9953745603561401\n",
            "Epoch 0, Phase train, Batch 1100, Batch 1012/1259, Loss: 1.8763220310211182\n",
            "Epoch 0, Phase train, Batch 364, Batch 1013/1259, Loss: 1.9354108572006226\n",
            "Epoch 0, Phase train, Batch 364, Batch 1013/1259, Loss: 1.930389404296875\n",
            "Epoch 0, Phase train, Batch 142, Batch 1014/1259, Loss: 1.9218111038208008\n",
            "Epoch 0, Phase train, Batch 142, Batch 1014/1259, Loss: 1.8180218935012817\n",
            "Epoch 0, Phase train, Batch 246, Batch 1015/1259, Loss: 1.8832381963729858\n",
            "Epoch 0, Phase train, Batch 246, Batch 1015/1259, Loss: 1.7881743907928467\n",
            "Epoch 0, Phase train, Batch 1028, Batch 1016/1259, Loss: 1.8919724225997925\n",
            "Epoch 0, Phase train, Batch 1028, Batch 1016/1259, Loss: 1.8054916858673096\n",
            "Epoch 0, Phase train, Batch 1012, Batch 1017/1259, Loss: 1.92059326171875\n",
            "Epoch 0, Phase train, Batch 1012, Batch 1017/1259, Loss: 1.9669475555419922\n",
            "Epoch 0, Phase train, Batch 1359, Batch 1018/1259, Loss: 1.77689528465271\n",
            "Epoch 0, Phase train, Batch 1359, Batch 1018/1259, Loss: 1.8889604806900024\n",
            "Epoch 0, Phase train, Batch 1305, Batch 1019/1259, Loss: 1.7144519090652466\n",
            "Epoch 0, Phase train, Batch 1305, Batch 1019/1259, Loss: 1.879523754119873\n",
            "Epoch 0, Phase train, Batch 256, Batch 1020/1259, Loss: 1.93539297580719\n",
            "Epoch 0, Phase train, Batch 256, Batch 1020/1259, Loss: 1.908789038658142\n",
            "Epoch 0, Phase train, Batch 540, Batch 1021/1259, Loss: 1.8771898746490479\n",
            "Epoch 0, Phase train, Batch 540, Batch 1021/1259, Loss: 2.102048397064209\n",
            "Epoch 0, Phase train, Batch 970, Batch 1022/1259, Loss: 1.9677329063415527\n",
            "Epoch 0, Phase train, Batch 970, Batch 1022/1259, Loss: 1.8876632452011108\n",
            "Epoch 0, Phase train, Batch 506, Batch 1023/1259, Loss: 1.6440138816833496\n",
            "Epoch 0, Phase train, Batch 506, Batch 1023/1259, Loss: 1.711340308189392\n",
            "Epoch 0, Phase train, Batch 1277, Batch 1024/1259, Loss: 1.7139892578125\n",
            "Epoch 0, Phase train, Batch 1277, Batch 1024/1259, Loss: 1.8568518161773682\n",
            "Epoch 0, Phase train, Batch 1406, Batch 1025/1259, Loss: 1.8175421953201294\n",
            "Epoch 0, Phase train, Batch 1406, Batch 1025/1259, Loss: 1.844883680343628\n",
            "Epoch 0, Phase train, Batch 479, Batch 1026/1259, Loss: 1.7800921201705933\n",
            "Epoch 0, Phase train, Batch 479, Batch 1026/1259, Loss: 1.7544466257095337\n",
            "Epoch 0, Phase train, Batch 1383, Batch 1027/1259, Loss: 1.7657179832458496\n",
            "Epoch 0, Phase train, Batch 1383, Batch 1027/1259, Loss: 1.8136729001998901\n",
            "Epoch 0, Phase train, Batch 1113, Batch 1028/1259, Loss: 2.065091371536255\n",
            "Epoch 0, Phase train, Batch 1113, Batch 1028/1259, Loss: 1.9090949296951294\n",
            "Epoch 0, Phase train, Batch 841, Batch 1029/1259, Loss: 1.7857472896575928\n",
            "Epoch 0, Phase train, Batch 841, Batch 1029/1259, Loss: 1.7439393997192383\n",
            "Epoch 0, Phase train, Batch 1336, Batch 1030/1259, Loss: 1.7719953060150146\n",
            "Epoch 0, Phase train, Batch 1336, Batch 1030/1259, Loss: 1.7543562650680542\n",
            "Epoch 0, Phase train, Batch 606, Batch 1031/1259, Loss: 1.8140003681182861\n",
            "Epoch 0, Phase train, Batch 606, Batch 1031/1259, Loss: 1.8314156532287598\n",
            "Epoch 0, Phase train, Batch 187, Batch 1032/1259, Loss: 1.794389009475708\n",
            "Epoch 0, Phase train, Batch 187, Batch 1032/1259, Loss: 1.8872069120407104\n",
            "Epoch 0, Phase train, Batch 901, Batch 1033/1259, Loss: 1.972830057144165\n",
            "Epoch 0, Phase train, Batch 901, Batch 1033/1259, Loss: 1.8351484537124634\n",
            "Epoch 0, Phase train, Batch 126, Batch 1034/1259, Loss: 1.904245376586914\n",
            "Epoch 0, Phase train, Batch 126, Batch 1034/1259, Loss: 1.8271105289459229\n",
            "Epoch 0, Phase train, Batch 66, Batch 1035/1259, Loss: 1.992672324180603\n",
            "Epoch 0, Phase train, Batch 66, Batch 1035/1259, Loss: 2.101593255996704\n",
            "Epoch 0, Phase train, Batch 161, Batch 1036/1259, Loss: 1.815927505493164\n",
            "Epoch 0, Phase train, Batch 161, Batch 1036/1259, Loss: 1.847718596458435\n",
            "Epoch 0, Phase train, Batch 40, Batch 1037/1259, Loss: 1.9788895845413208\n",
            "Epoch 0, Phase train, Batch 40, Batch 1037/1259, Loss: 1.9387730360031128\n",
            "Epoch 0, Phase train, Batch 293, Batch 1038/1259, Loss: 1.9476356506347656\n",
            "Epoch 0, Phase train, Batch 293, Batch 1038/1259, Loss: 1.8895775079727173\n",
            "Epoch 0, Phase train, Batch 224, Batch 1039/1259, Loss: 1.9242229461669922\n",
            "Epoch 0, Phase train, Batch 224, Batch 1039/1259, Loss: 1.7923274040222168\n",
            "Epoch 0, Phase train, Batch 1397, Batch 1040/1259, Loss: 1.7870510816574097\n",
            "Epoch 0, Phase train, Batch 1397, Batch 1040/1259, Loss: 1.9850687980651855\n",
            "Epoch 0, Phase train, Batch 1401, Batch 1041/1259, Loss: 1.7886486053466797\n",
            "Epoch 0, Phase train, Batch 1401, Batch 1041/1259, Loss: 1.9403300285339355\n",
            "Epoch 0, Phase train, Batch 1311, Batch 1042/1259, Loss: 1.696048378944397\n",
            "Epoch 0, Phase train, Batch 1311, Batch 1042/1259, Loss: 1.6788853406906128\n",
            "Epoch 0, Phase train, Batch 1146, Batch 1043/1259, Loss: 1.8014694452285767\n",
            "Epoch 0, Phase train, Batch 1146, Batch 1043/1259, Loss: 1.762582540512085\n",
            "Epoch 0, Phase train, Batch 298, Batch 1044/1259, Loss: 1.7934330701828003\n",
            "Epoch 0, Phase train, Batch 298, Batch 1044/1259, Loss: 1.8751769065856934\n",
            "Epoch 0, Phase train, Batch 1556, Batch 1045/1259, Loss: 1.8415666818618774\n",
            "Epoch 0, Phase train, Batch 1556, Batch 1045/1259, Loss: 1.7686198949813843\n",
            "Epoch 0, Phase train, Batch 572, Batch 1046/1259, Loss: 1.8783862590789795\n",
            "Epoch 0, Phase train, Batch 572, Batch 1046/1259, Loss: 1.899550199508667\n",
            "Epoch 0, Phase train, Batch 634, Batch 1047/1259, Loss: 1.9171241521835327\n",
            "Epoch 0, Phase train, Batch 634, Batch 1047/1259, Loss: 1.866814136505127\n",
            "Epoch 0, Phase train, Batch 1441, Batch 1048/1259, Loss: 1.8818471431732178\n",
            "Epoch 0, Phase train, Batch 1441, Batch 1048/1259, Loss: 1.9892349243164062\n",
            "Epoch 0, Phase train, Batch 896, Batch 1049/1259, Loss: 1.8173015117645264\n",
            "Epoch 0, Phase train, Batch 896, Batch 1049/1259, Loss: 1.8534514904022217\n",
            "Epoch 0, Phase train, Batch 58, Batch 1050/1259, Loss: 1.959485650062561\n",
            "Epoch 0, Phase train, Batch 58, Batch 1050/1259, Loss: 2.1562254428863525\n",
            "Epoch 0, Phase train, Batch 470, Batch 1051/1259, Loss: 1.6662288904190063\n",
            "Epoch 0, Phase train, Batch 470, Batch 1051/1259, Loss: 1.787413477897644\n",
            "Epoch 0, Phase train, Batch 873, Batch 1052/1259, Loss: 1.824332594871521\n",
            "Epoch 0, Phase train, Batch 873, Batch 1052/1259, Loss: 1.9465305805206299\n",
            "Epoch 0, Phase train, Batch 310, Batch 1053/1259, Loss: 1.7764447927474976\n",
            "Epoch 0, Phase train, Batch 310, Batch 1053/1259, Loss: 1.9388631582260132\n",
            "Epoch 0, Phase train, Batch 1178, Batch 1054/1259, Loss: 1.822871446609497\n",
            "Epoch 0, Phase train, Batch 1178, Batch 1054/1259, Loss: 1.7502351999282837\n",
            "Epoch 0, Phase train, Batch 167, Batch 1055/1259, Loss: 1.8326162099838257\n",
            "Epoch 0, Phase train, Batch 167, Batch 1055/1259, Loss: 1.8703103065490723\n",
            "Epoch 0, Phase train, Batch 12, Batch 1056/1259, Loss: 2.1204957962036133\n",
            "Epoch 0, Phase train, Batch 12, Batch 1056/1259, Loss: 2.0221900939941406\n",
            "Epoch 0, Phase train, Batch 1136, Batch 1057/1259, Loss: 1.9241949319839478\n",
            "Epoch 0, Phase train, Batch 1136, Batch 1057/1259, Loss: 1.9493131637573242\n",
            "Epoch 0, Phase train, Batch 629, Batch 1058/1259, Loss: 2.0069899559020996\n",
            "Epoch 0, Phase train, Batch 629, Batch 1058/1259, Loss: 1.8116399049758911\n",
            "Epoch 0, Phase train, Batch 1219, Batch 1059/1259, Loss: 1.5515360832214355\n",
            "Epoch 0, Phase train, Batch 1219, Batch 1059/1259, Loss: 1.7972021102905273\n",
            "Epoch 0, Phase train, Batch 309, Batch 1060/1259, Loss: 1.7670484781265259\n",
            "Epoch 0, Phase train, Batch 309, Batch 1060/1259, Loss: 1.7899211645126343\n",
            "Epoch 0, Phase train, Batch 1191, Batch 1061/1259, Loss: 1.6422827243804932\n",
            "Epoch 0, Phase train, Batch 1191, Batch 1061/1259, Loss: 1.7372978925704956\n",
            "Epoch 0, Phase train, Batch 797, Batch 1062/1259, Loss: 1.9193565845489502\n",
            "Epoch 0, Phase train, Batch 797, Batch 1062/1259, Loss: 1.8722813129425049\n",
            "Epoch 0, Phase train, Batch 976, Batch 1063/1259, Loss: 1.8207721710205078\n",
            "Epoch 0, Phase train, Batch 976, Batch 1063/1259, Loss: 1.7563793659210205\n",
            "Epoch 0, Phase train, Batch 897, Batch 1064/1259, Loss: 1.8130807876586914\n",
            "Epoch 0, Phase train, Batch 897, Batch 1064/1259, Loss: 1.8760042190551758\n",
            "Epoch 0, Phase train, Batch 1552, Batch 1065/1259, Loss: 1.9820979833602905\n",
            "Epoch 0, Phase train, Batch 1552, Batch 1065/1259, Loss: 1.6791986227035522\n",
            "Epoch 0, Phase train, Batch 1165, Batch 1066/1259, Loss: 1.861425757408142\n",
            "Epoch 0, Phase train, Batch 1165, Batch 1066/1259, Loss: 1.8567240238189697\n",
            "Epoch 0, Phase train, Batch 1463, Batch 1067/1259, Loss: 1.9646183252334595\n",
            "Epoch 0, Phase train, Batch 1463, Batch 1067/1259, Loss: 1.669083595275879\n",
            "Epoch 0, Phase train, Batch 704, Batch 1068/1259, Loss: 1.9170317649841309\n",
            "Epoch 0, Phase train, Batch 704, Batch 1068/1259, Loss: 2.079195499420166\n",
            "Epoch 0, Phase train, Batch 1427, Batch 1069/1259, Loss: 1.9305697679519653\n",
            "Epoch 0, Phase train, Batch 1427, Batch 1069/1259, Loss: 1.7237894535064697\n",
            "Epoch 0, Phase train, Batch 590, Batch 1070/1259, Loss: 1.9856581687927246\n",
            "Epoch 0, Phase train, Batch 590, Batch 1070/1259, Loss: 1.7242423295974731\n",
            "Epoch 0, Phase train, Batch 46, Batch 1071/1259, Loss: 1.9643629789352417\n",
            "Epoch 0, Phase train, Batch 46, Batch 1071/1259, Loss: 2.154470682144165\n",
            "Epoch 0, Phase train, Batch 182, Batch 1072/1259, Loss: 1.79677414894104\n",
            "Epoch 0, Phase train, Batch 182, Batch 1072/1259, Loss: 1.923989176750183\n",
            "Epoch 0, Phase train, Batch 336, Batch 1073/1259, Loss: 1.8403533697128296\n",
            "Epoch 0, Phase train, Batch 336, Batch 1073/1259, Loss: 1.8070317506790161\n",
            "Epoch 0, Phase train, Batch 1461, Batch 1074/1259, Loss: 1.821671962738037\n",
            "Epoch 0, Phase train, Batch 1461, Batch 1074/1259, Loss: 1.807820200920105\n",
            "Epoch 0, Phase train, Batch 1458, Batch 1075/1259, Loss: 1.6903409957885742\n",
            "Epoch 0, Phase train, Batch 1458, Batch 1075/1259, Loss: 1.8234597444534302\n",
            "Epoch 0, Phase train, Batch 108, Batch 1076/1259, Loss: 1.8705886602401733\n",
            "Epoch 0, Phase train, Batch 108, Batch 1076/1259, Loss: 1.9595292806625366\n",
            "Epoch 0, Phase train, Batch 1169, Batch 1077/1259, Loss: 1.9266382455825806\n",
            "Epoch 0, Phase train, Batch 1169, Batch 1077/1259, Loss: 1.8483532667160034\n",
            "Epoch 0, Phase train, Batch 468, Batch 1078/1259, Loss: 1.7756181955337524\n",
            "Epoch 0, Phase train, Batch 468, Batch 1078/1259, Loss: 1.7401659488677979\n",
            "Epoch 0, Phase train, Batch 458, Batch 1079/1259, Loss: 1.8336929082870483\n",
            "Epoch 0, Phase train, Batch 458, Batch 1079/1259, Loss: 1.7711725234985352\n",
            "Epoch 0, Phase train, Batch 1373, Batch 1080/1259, Loss: 1.671837568283081\n",
            "Epoch 0, Phase train, Batch 1373, Batch 1080/1259, Loss: 1.805601954460144\n",
            "Epoch 0, Phase train, Batch 348, Batch 1081/1259, Loss: 1.7691435813903809\n",
            "Epoch 0, Phase train, Batch 348, Batch 1081/1259, Loss: 1.796006679534912\n",
            "Epoch 0, Phase train, Batch 505, Batch 1082/1259, Loss: 1.7672632932662964\n",
            "Epoch 0, Phase train, Batch 505, Batch 1082/1259, Loss: 1.8413640260696411\n",
            "Epoch 0, Phase train, Batch 1503, Batch 1083/1259, Loss: 1.6230229139328003\n",
            "Epoch 0, Phase train, Batch 1503, Batch 1083/1259, Loss: 1.7940140962600708\n",
            "Epoch 0, Phase train, Batch 401, Batch 1084/1259, Loss: 1.878915548324585\n",
            "Epoch 0, Phase train, Batch 401, Batch 1084/1259, Loss: 1.84368896484375\n",
            "Epoch 0, Phase train, Batch 176, Batch 1085/1259, Loss: 1.8212087154388428\n",
            "Epoch 0, Phase train, Batch 176, Batch 1085/1259, Loss: 1.9179365634918213\n",
            "Epoch 0, Phase train, Batch 798, Batch 1086/1259, Loss: 1.7146638631820679\n",
            "Epoch 0, Phase train, Batch 798, Batch 1086/1259, Loss: 1.8179538249969482\n",
            "Epoch 0, Phase train, Batch 190, Batch 1087/1259, Loss: 1.978472113609314\n",
            "Epoch 0, Phase train, Batch 190, Batch 1087/1259, Loss: 1.827038049697876\n",
            "Epoch 0, Phase train, Batch 1185, Batch 1088/1259, Loss: 1.8737519979476929\n",
            "Epoch 0, Phase train, Batch 1185, Batch 1088/1259, Loss: 1.6193821430206299\n",
            "Epoch 0, Phase train, Batch 786, Batch 1089/1259, Loss: 1.9374250173568726\n",
            "Epoch 0, Phase train, Batch 786, Batch 1089/1259, Loss: 1.9131404161453247\n",
            "Epoch 0, Phase train, Batch 1357, Batch 1090/1259, Loss: 1.6341643333435059\n",
            "Epoch 0, Phase train, Batch 1357, Batch 1090/1259, Loss: 1.8252062797546387\n",
            "Epoch 0, Phase train, Batch 1234, Batch 1091/1259, Loss: 1.5951076745986938\n",
            "Epoch 0, Phase train, Batch 1234, Batch 1091/1259, Loss: 1.6602762937545776\n",
            "Epoch 0, Phase train, Batch 48, Batch 1092/1259, Loss: 1.8514597415924072\n",
            "Epoch 0, Phase train, Batch 48, Batch 1092/1259, Loss: 2.061328887939453\n",
            "Epoch 0, Phase train, Batch 392, Batch 1093/1259, Loss: 1.8050938844680786\n",
            "Epoch 0, Phase train, Batch 392, Batch 1093/1259, Loss: 1.7493667602539062\n",
            "Epoch 0, Phase train, Batch 1149, Batch 1094/1259, Loss: 2.380035400390625\n",
            "Epoch 0, Phase train, Batch 1149, Batch 1094/1259, Loss: 2.313967704772949\n",
            "Epoch 0, Phase train, Batch 1415, Batch 1095/1259, Loss: 1.7531166076660156\n",
            "Epoch 0, Phase train, Batch 1415, Batch 1095/1259, Loss: 1.7923357486724854\n",
            "Epoch 0, Phase train, Batch 630, Batch 1096/1259, Loss: 1.9921709299087524\n",
            "Epoch 0, Phase train, Batch 630, Batch 1096/1259, Loss: 2.106822967529297\n",
            "Epoch 0, Phase train, Batch 1456, Batch 1097/1259, Loss: 2.32226824760437\n",
            "Epoch 0, Phase train, Batch 1456, Batch 1097/1259, Loss: 2.414896011352539\n",
            "Epoch 0, Phase train, Batch 730, Batch 1098/1259, Loss: 1.8881521224975586\n",
            "Epoch 0, Phase train, Batch 730, Batch 1098/1259, Loss: 1.7191599607467651\n",
            "Epoch 0, Phase train, Batch 1447, Batch 1099/1259, Loss: 1.8591891527175903\n",
            "Epoch 0, Phase train, Batch 1447, Batch 1099/1259, Loss: 1.6981019973754883\n",
            "Epoch 0, Phase train, Batch 30, Batch 1100/1259, Loss: 2.138221263885498\n",
            "Epoch 0, Phase train, Batch 30, Batch 1100/1259, Loss: 1.977586030960083\n",
            "Epoch 0, Phase train, Batch 17, Batch 1101/1259, Loss: 1.9824845790863037\n",
            "Epoch 0, Phase train, Batch 17, Batch 1101/1259, Loss: 1.86296808719635\n",
            "Epoch 0, Phase train, Batch 1488, Batch 1102/1259, Loss: 1.7710191011428833\n",
            "Epoch 0, Phase train, Batch 1488, Batch 1102/1259, Loss: 1.7537038326263428\n",
            "Epoch 0, Phase train, Batch 29, Batch 1103/1259, Loss: 2.228484869003296\n",
            "Epoch 0, Phase train, Batch 29, Batch 1103/1259, Loss: 2.2518913745880127\n",
            "Epoch 0, Phase train, Batch 1453, Batch 1104/1259, Loss: 1.8007185459136963\n",
            "Epoch 0, Phase train, Batch 1453, Batch 1104/1259, Loss: 1.9784607887268066\n",
            "Epoch 0, Phase train, Batch 88, Batch 1105/1259, Loss: 1.9119821786880493\n",
            "Epoch 0, Phase train, Batch 88, Batch 1105/1259, Loss: 1.9045183658599854\n",
            "Epoch 0, Phase train, Batch 725, Batch 1106/1259, Loss: 1.8891215324401855\n",
            "Epoch 0, Phase train, Batch 725, Batch 1106/1259, Loss: 2.087968587875366\n",
            "Epoch 0, Phase train, Batch 100, Batch 1107/1259, Loss: 1.888685941696167\n",
            "Epoch 0, Phase train, Batch 100, Batch 1107/1259, Loss: 2.040574312210083\n",
            "Epoch 0, Phase train, Batch 705, Batch 1108/1259, Loss: 1.9698611497879028\n",
            "Epoch 0, Phase train, Batch 705, Batch 1108/1259, Loss: 2.0321567058563232\n",
            "Epoch 0, Phase train, Batch 276, Batch 1109/1259, Loss: 1.7535353899002075\n",
            "Epoch 0, Phase train, Batch 276, Batch 1109/1259, Loss: 1.8639893531799316\n",
            "Epoch 0, Phase train, Batch 706, Batch 1110/1259, Loss: 2.0065088272094727\n",
            "Epoch 0, Phase train, Batch 706, Batch 1110/1259, Loss: 2.053908586502075\n",
            "Epoch 0, Phase train, Batch 519, Batch 1111/1259, Loss: 1.8270528316497803\n",
            "Epoch 0, Phase train, Batch 519, Batch 1111/1259, Loss: 1.952514886856079\n",
            "Epoch 0, Phase train, Batch 1250, Batch 1112/1259, Loss: 1.9143908023834229\n",
            "Epoch 0, Phase train, Batch 1250, Batch 1112/1259, Loss: 1.6676247119903564\n",
            "Epoch 0, Phase train, Batch 1210, Batch 1113/1259, Loss: 4.319897651672363\n",
            "Epoch 0, Phase train, Batch 1210, Batch 1113/1259, Loss: 4.322935581207275\n",
            "Epoch 0, Phase train, Batch 504, Batch 1114/1259, Loss: 1.7368072271347046\n",
            "Epoch 0, Phase train, Batch 504, Batch 1114/1259, Loss: 1.6660064458847046\n",
            "Epoch 0, Phase train, Batch 1459, Batch 1115/1259, Loss: 1.8966790437698364\n",
            "Epoch 0, Phase train, Batch 1459, Batch 1115/1259, Loss: 1.9356411695480347\n",
            "Epoch 0, Phase train, Batch 650, Batch 1116/1259, Loss: 2.360661745071411\n",
            "Epoch 0, Phase train, Batch 650, Batch 1116/1259, Loss: 2.2832562923431396\n",
            "Epoch 0, Phase train, Batch 425, Batch 1117/1259, Loss: 1.8030706644058228\n",
            "Epoch 0, Phase train, Batch 425, Batch 1117/1259, Loss: 1.8003848791122437\n",
            "Epoch 0, Phase train, Batch 1509, Batch 1118/1259, Loss: 1.8079413175582886\n",
            "Epoch 0, Phase train, Batch 1509, Batch 1118/1259, Loss: 1.8874170780181885\n",
            "Epoch 0, Phase train, Batch 1387, Batch 1119/1259, Loss: 1.7479103803634644\n",
            "Epoch 0, Phase train, Batch 1387, Batch 1119/1259, Loss: 1.8076214790344238\n",
            "Epoch 0, Phase train, Batch 1066, Batch 1120/1259, Loss: 1.8513928651809692\n",
            "Epoch 0, Phase train, Batch 1066, Batch 1120/1259, Loss: 1.711660623550415\n",
            "Epoch 0, Phase train, Batch 1236, Batch 1121/1259, Loss: 1.7766448259353638\n",
            "Epoch 0, Phase train, Batch 1236, Batch 1121/1259, Loss: 1.7028764486312866\n",
            "Epoch 0, Phase train, Batch 1095, Batch 1122/1259, Loss: 1.7476075887680054\n",
            "Epoch 0, Phase train, Batch 1095, Batch 1122/1259, Loss: 1.9487850666046143\n",
            "Epoch 0, Phase train, Batch 691, Batch 1123/1259, Loss: 2.2112536430358887\n",
            "Epoch 0, Phase train, Batch 691, Batch 1123/1259, Loss: 2.1061899662017822\n",
            "Epoch 0, Phase train, Batch 838, Batch 1124/1259, Loss: 1.7956753969192505\n",
            "Epoch 0, Phase train, Batch 838, Batch 1124/1259, Loss: 1.9508086442947388\n",
            "Epoch 0, Phase train, Batch 477, Batch 1125/1259, Loss: 1.7480237483978271\n",
            "Epoch 0, Phase train, Batch 477, Batch 1125/1259, Loss: 1.6941392421722412\n",
            "Epoch 0, Phase train, Batch 1114, Batch 1126/1259, Loss: 1.7053924798965454\n",
            "Epoch 0, Phase train, Batch 1114, Batch 1126/1259, Loss: 1.996416687965393\n",
            "Epoch 0, Phase train, Batch 454, Batch 1127/1259, Loss: 1.8167184591293335\n",
            "Epoch 0, Phase train, Batch 454, Batch 1127/1259, Loss: 1.6767030954360962\n",
            "Epoch 0, Phase train, Batch 1268, Batch 1128/1259, Loss: 1.9574801921844482\n",
            "Epoch 0, Phase train, Batch 1268, Batch 1128/1259, Loss: 2.0310521125793457\n",
            "Epoch 0, Phase train, Batch 1429, Batch 1129/1259, Loss: 1.8908073902130127\n",
            "Epoch 0, Phase train, Batch 1429, Batch 1129/1259, Loss: 1.8052161931991577\n",
            "Epoch 0, Phase train, Batch 1018, Batch 1130/1259, Loss: 1.6862753629684448\n",
            "Epoch 0, Phase train, Batch 1018, Batch 1130/1259, Loss: 2.06231427192688\n",
            "Epoch 0, Phase train, Batch 387, Batch 1131/1259, Loss: 1.7296996116638184\n",
            "Epoch 0, Phase train, Batch 387, Batch 1131/1259, Loss: 1.8285064697265625\n",
            "Epoch 0, Phase train, Batch 718, Batch 1132/1259, Loss: 1.834365963935852\n",
            "Epoch 0, Phase train, Batch 718, Batch 1132/1259, Loss: 1.8998359441757202\n",
            "Epoch 0, Phase train, Batch 1, Batch 1133/1259, Loss: 1.8903061151504517\n",
            "Epoch 0, Phase train, Batch 1, Batch 1133/1259, Loss: 2.1450130939483643\n",
            "Epoch 0, Phase train, Batch 1290, Batch 1134/1259, Loss: 1.7656903266906738\n",
            "Epoch 0, Phase train, Batch 1290, Batch 1134/1259, Loss: 1.691226840019226\n",
            "Epoch 0, Phase train, Batch 571, Batch 1135/1259, Loss: 2.785140037536621\n",
            "Epoch 0, Phase train, Batch 571, Batch 1135/1259, Loss: 2.6728713512420654\n",
            "Epoch 0, Phase train, Batch 562, Batch 1136/1259, Loss: 2.145658493041992\n",
            "Epoch 0, Phase train, Batch 562, Batch 1136/1259, Loss: 2.083465576171875\n",
            "Epoch 0, Phase train, Batch 1495, Batch 1137/1259, Loss: 1.8831062316894531\n",
            "Epoch 0, Phase train, Batch 1495, Batch 1137/1259, Loss: 1.8859106302261353\n",
            "Epoch 0, Phase train, Batch 524, Batch 1138/1259, Loss: 1.7605218887329102\n",
            "Epoch 0, Phase train, Batch 524, Batch 1138/1259, Loss: 1.6935328245162964\n",
            "Epoch 0, Phase train, Batch 317, Batch 1139/1259, Loss: 1.9574711322784424\n",
            "Epoch 0, Phase train, Batch 317, Batch 1139/1259, Loss: 1.8938592672348022\n",
            "Epoch 0, Phase train, Batch 72, Batch 1140/1259, Loss: 2.042154312133789\n",
            "Epoch 0, Phase train, Batch 72, Batch 1140/1259, Loss: 1.8561207056045532\n",
            "Epoch 0, Phase train, Batch 1457, Batch 1141/1259, Loss: 1.999687671661377\n",
            "Epoch 0, Phase train, Batch 1457, Batch 1141/1259, Loss: 1.9787421226501465\n",
            "Epoch 0, Phase train, Batch 486, Batch 1142/1259, Loss: 1.9527570009231567\n",
            "Epoch 0, Phase train, Batch 486, Batch 1142/1259, Loss: 2.0395777225494385\n",
            "Epoch 0, Phase train, Batch 952, Batch 1143/1259, Loss: 1.8546555042266846\n",
            "Epoch 0, Phase train, Batch 952, Batch 1143/1259, Loss: 1.9665218591690063\n",
            "Epoch 0, Phase train, Batch 259, Batch 1144/1259, Loss: 2.2779359817504883\n",
            "Epoch 0, Phase train, Batch 259, Batch 1144/1259, Loss: 2.4863383769989014\n",
            "Epoch 0, Phase train, Batch 321, Batch 1145/1259, Loss: 1.918838381767273\n",
            "Epoch 0, Phase train, Batch 321, Batch 1145/1259, Loss: 1.9867898225784302\n",
            "Epoch 0, Phase train, Batch 871, Batch 1146/1259, Loss: 1.908252239227295\n",
            "Epoch 0, Phase train, Batch 871, Batch 1146/1259, Loss: 1.9723159074783325\n",
            "Epoch 0, Phase train, Batch 63, Batch 1147/1259, Loss: 1.8425168991088867\n",
            "Epoch 0, Phase train, Batch 63, Batch 1147/1259, Loss: 2.026745319366455\n",
            "Epoch 0, Phase train, Batch 1327, Batch 1148/1259, Loss: 1.7303788661956787\n",
            "Epoch 0, Phase train, Batch 1327, Batch 1148/1259, Loss: 1.5863866806030273\n",
            "Epoch 0, Phase train, Batch 323, Batch 1149/1259, Loss: 1.9676647186279297\n",
            "Epoch 0, Phase train, Batch 323, Batch 1149/1259, Loss: 1.8120441436767578\n",
            "Epoch 0, Phase train, Batch 910, Batch 1150/1259, Loss: 1.8339589834213257\n",
            "Epoch 0, Phase train, Batch 910, Batch 1150/1259, Loss: 1.7574760913848877\n",
            "Epoch 0, Phase train, Batch 1545, Batch 1151/1259, Loss: 1.7875086069107056\n",
            "Epoch 0, Phase train, Batch 1545, Batch 1151/1259, Loss: 1.6416295766830444\n",
            "Epoch 0, Phase train, Batch 1521, Batch 1152/1259, Loss: 1.6205464601516724\n",
            "Epoch 0, Phase train, Batch 1521, Batch 1152/1259, Loss: 1.8058907985687256\n",
            "Epoch 0, Phase train, Batch 683, Batch 1153/1259, Loss: 1.9275269508361816\n",
            "Epoch 0, Phase train, Batch 683, Batch 1153/1259, Loss: 1.8311772346496582\n",
            "Epoch 0, Phase train, Batch 247, Batch 1154/1259, Loss: 2.0974135398864746\n",
            "Epoch 0, Phase train, Batch 247, Batch 1154/1259, Loss: 2.1739635467529297\n",
            "Epoch 0, Phase train, Batch 1517, Batch 1155/1259, Loss: 1.8326826095581055\n",
            "Epoch 0, Phase train, Batch 1517, Batch 1155/1259, Loss: 1.7554974555969238\n",
            "Epoch 0, Phase train, Batch 533, Batch 1156/1259, Loss: 1.856716275215149\n",
            "Epoch 0, Phase train, Batch 533, Batch 1156/1259, Loss: 1.83109712600708\n",
            "Epoch 0, Phase train, Batch 1048, Batch 1157/1259, Loss: 1.8598345518112183\n",
            "Epoch 0, Phase train, Batch 1048, Batch 1157/1259, Loss: 1.8943017721176147\n",
            "Epoch 0, Phase train, Batch 1533, Batch 1158/1259, Loss: 1.729791283607483\n",
            "Epoch 0, Phase train, Batch 1533, Batch 1158/1259, Loss: 1.9307615756988525\n",
            "Epoch 0, Phase train, Batch 579, Batch 1159/1259, Loss: 1.7886613607406616\n",
            "Epoch 0, Phase train, Batch 579, Batch 1159/1259, Loss: 1.956693172454834\n",
            "Epoch 0, Phase train, Batch 774, Batch 1160/1259, Loss: 1.982145071029663\n",
            "Epoch 0, Phase train, Batch 774, Batch 1160/1259, Loss: 1.955818772315979\n",
            "Epoch 0, Phase train, Batch 225, Batch 1161/1259, Loss: 1.9858258962631226\n",
            "Epoch 0, Phase train, Batch 225, Batch 1161/1259, Loss: 2.0912132263183594\n",
            "Epoch 0, Phase train, Batch 826, Batch 1162/1259, Loss: 1.7980319261550903\n",
            "Epoch 0, Phase train, Batch 826, Batch 1162/1259, Loss: 1.6294593811035156\n",
            "Epoch 0, Phase train, Batch 1343, Batch 1163/1259, Loss: 1.8452953100204468\n",
            "Epoch 0, Phase train, Batch 1343, Batch 1163/1259, Loss: 1.988280177116394\n",
            "Epoch 0, Phase train, Batch 1195, Batch 1164/1259, Loss: 1.7609070539474487\n",
            "Epoch 0, Phase train, Batch 1195, Batch 1164/1259, Loss: 1.8517580032348633\n",
            "Epoch 0, Phase train, Batch 821, Batch 1165/1259, Loss: 1.8411850929260254\n",
            "Epoch 0, Phase train, Batch 821, Batch 1165/1259, Loss: 1.9465266466140747\n",
            "Epoch 0, Phase train, Batch 1078, Batch 1166/1259, Loss: 1.9022512435913086\n",
            "Epoch 0, Phase train, Batch 1078, Batch 1166/1259, Loss: 1.6523630619049072\n",
            "Epoch 0, Phase train, Batch 472, Batch 1167/1259, Loss: 1.8895177841186523\n",
            "Epoch 0, Phase train, Batch 472, Batch 1167/1259, Loss: 1.7549656629562378\n",
            "Epoch 0, Phase train, Batch 201, Batch 1168/1259, Loss: 1.7777076959609985\n",
            "Epoch 0, Phase train, Batch 201, Batch 1168/1259, Loss: 1.8284156322479248\n",
            "Epoch 0, Phase train, Batch 1154, Batch 1169/1259, Loss: 1.7811939716339111\n",
            "Epoch 0, Phase train, Batch 1154, Batch 1169/1259, Loss: 1.8410898447036743\n",
            "Epoch 0, Phase train, Batch 153, Batch 1170/1259, Loss: 1.7054165601730347\n",
            "Epoch 0, Phase train, Batch 153, Batch 1170/1259, Loss: 1.7832576036453247\n",
            "Epoch 0, Phase train, Batch 791, Batch 1171/1259, Loss: 1.826749563217163\n",
            "Epoch 0, Phase train, Batch 791, Batch 1171/1259, Loss: 1.8626139163970947\n",
            "Epoch 0, Phase train, Batch 880, Batch 1172/1259, Loss: 2.0060067176818848\n",
            "Epoch 0, Phase train, Batch 880, Batch 1172/1259, Loss: 1.9201382398605347\n",
            "Epoch 0, Phase train, Batch 1216, Batch 1173/1259, Loss: 1.761505365371704\n",
            "Epoch 0, Phase train, Batch 1216, Batch 1173/1259, Loss: 1.6590529680252075\n",
            "Epoch 0, Phase train, Batch 605, Batch 1174/1259, Loss: 1.8725517988204956\n",
            "Epoch 0, Phase train, Batch 605, Batch 1174/1259, Loss: 1.847985029220581\n",
            "Epoch 0, Phase train, Batch 473, Batch 1175/1259, Loss: 1.9138250350952148\n",
            "Epoch 0, Phase train, Batch 473, Batch 1175/1259, Loss: 1.6455553770065308\n",
            "Epoch 0, Phase train, Batch 14, Batch 1176/1259, Loss: 2.0129613876342773\n",
            "Epoch 0, Phase train, Batch 14, Batch 1176/1259, Loss: 1.872634768486023\n",
            "Epoch 0, Phase train, Batch 1319, Batch 1177/1259, Loss: 1.9553993940353394\n",
            "Epoch 0, Phase train, Batch 1319, Batch 1177/1259, Loss: 1.5950801372528076\n",
            "Epoch 0, Phase train, Batch 788, Batch 1178/1259, Loss: 1.793166160583496\n",
            "Epoch 0, Phase train, Batch 788, Batch 1178/1259, Loss: 1.7369465827941895\n",
            "Epoch 0, Phase train, Batch 94, Batch 1179/1259, Loss: 1.9375581741333008\n",
            "Epoch 0, Phase train, Batch 94, Batch 1179/1259, Loss: 1.9832627773284912\n",
            "Epoch 0, Phase train, Batch 1110, Batch 1180/1259, Loss: 1.899693489074707\n",
            "Epoch 0, Phase train, Batch 1110, Batch 1180/1259, Loss: 1.821540117263794\n",
            "Epoch 0, Phase train, Batch 928, Batch 1181/1259, Loss: 1.9885601997375488\n",
            "Epoch 0, Phase train, Batch 928, Batch 1181/1259, Loss: 1.9045120477676392\n",
            "Epoch 0, Phase train, Batch 1189, Batch 1182/1259, Loss: 1.6441316604614258\n",
            "Epoch 0, Phase train, Batch 1189, Batch 1182/1259, Loss: 1.6991623640060425\n",
            "Epoch 0, Phase train, Batch 915, Batch 1183/1259, Loss: 1.7599161863327026\n",
            "Epoch 0, Phase train, Batch 915, Batch 1183/1259, Loss: 1.9372071027755737\n",
            "Epoch 0, Phase train, Batch 307, Batch 1184/1259, Loss: 1.8990892171859741\n",
            "Epoch 0, Phase train, Batch 307, Batch 1184/1259, Loss: 1.9766337871551514\n",
            "Epoch 0, Phase train, Batch 1020, Batch 1185/1259, Loss: 1.8445346355438232\n",
            "Epoch 0, Phase train, Batch 1020, Batch 1185/1259, Loss: 1.7589484453201294\n",
            "Epoch 0, Phase train, Batch 1460, Batch 1186/1259, Loss: 1.6383869647979736\n",
            "Epoch 0, Phase train, Batch 1460, Batch 1186/1259, Loss: 1.7904188632965088\n",
            "Epoch 0, Phase train, Batch 319, Batch 1187/1259, Loss: 1.8952853679656982\n",
            "Epoch 0, Phase train, Batch 319, Batch 1187/1259, Loss: 1.8601099252700806\n",
            "Epoch 0, Phase train, Batch 998, Batch 1188/1259, Loss: 1.9414294958114624\n",
            "Epoch 0, Phase train, Batch 998, Batch 1188/1259, Loss: 1.718036413192749\n",
            "Epoch 0, Phase train, Batch 751, Batch 1189/1259, Loss: 1.9165585041046143\n",
            "Epoch 0, Phase train, Batch 751, Batch 1189/1259, Loss: 1.784580945968628\n",
            "Epoch 0, Phase train, Batch 1530, Batch 1190/1259, Loss: 1.8210408687591553\n",
            "Epoch 0, Phase train, Batch 1530, Batch 1190/1259, Loss: 1.7904080152511597\n",
            "Epoch 0, Phase train, Batch 87, Batch 1191/1259, Loss: 2.0600569248199463\n",
            "Epoch 0, Phase train, Batch 87, Batch 1191/1259, Loss: 1.9230461120605469\n",
            "Epoch 0, Phase train, Batch 729, Batch 1192/1259, Loss: 1.964720368385315\n",
            "Epoch 0, Phase train, Batch 729, Batch 1192/1259, Loss: 1.8997710943222046\n",
            "Epoch 0, Phase train, Batch 600, Batch 1193/1259, Loss: 1.8609811067581177\n",
            "Epoch 0, Phase train, Batch 600, Batch 1193/1259, Loss: 1.8306350708007812\n",
            "Epoch 0, Phase train, Batch 1091, Batch 1194/1259, Loss: 2.0271098613739014\n",
            "Epoch 0, Phase train, Batch 1091, Batch 1194/1259, Loss: 1.767999291419983\n",
            "Epoch 0, Phase train, Batch 1225, Batch 1195/1259, Loss: 1.6630491018295288\n",
            "Epoch 0, Phase train, Batch 1225, Batch 1195/1259, Loss: 1.789385437965393\n",
            "Epoch 0, Phase train, Batch 337, Batch 1196/1259, Loss: 2.3174490928649902\n",
            "Epoch 0, Phase train, Batch 337, Batch 1196/1259, Loss: 2.378152847290039\n",
            "Epoch 0, Phase train, Batch 1520, Batch 1197/1259, Loss: 1.6893094778060913\n",
            "Epoch 0, Phase train, Batch 1520, Batch 1197/1259, Loss: 1.735456943511963\n",
            "Epoch 0, Phase train, Batch 448, Batch 1198/1259, Loss: 1.8783190250396729\n",
            "Epoch 0, Phase train, Batch 448, Batch 1198/1259, Loss: 1.876024842262268\n",
            "Epoch 0, Phase train, Batch 621, Batch 1199/1259, Loss: 1.863410234451294\n",
            "Epoch 0, Phase train, Batch 621, Batch 1199/1259, Loss: 1.7561863660812378\n",
            "Epoch 0, Phase train, Batch 983, Batch 1200/1259, Loss: 1.8970448970794678\n",
            "Epoch 0, Phase train, Batch 983, Batch 1200/1259, Loss: 1.8626480102539062\n",
            "Epoch 0, Phase train, Batch 490, Batch 1201/1259, Loss: 1.72678542137146\n",
            "Epoch 0, Phase train, Batch 490, Batch 1201/1259, Loss: 1.72072434425354\n",
            "Epoch 0, Phase train, Batch 1403, Batch 1202/1259, Loss: 2.1137211322784424\n",
            "Epoch 0, Phase train, Batch 1403, Batch 1202/1259, Loss: 1.917022705078125\n",
            "Epoch 0, Phase train, Batch 485, Batch 1203/1259, Loss: 1.817367672920227\n",
            "Epoch 0, Phase train, Batch 485, Batch 1203/1259, Loss: 2.0927884578704834\n",
            "Epoch 0, Phase train, Batch 1082, Batch 1204/1259, Loss: 2.0103604793548584\n",
            "Epoch 0, Phase train, Batch 1082, Batch 1204/1259, Loss: 1.9473881721496582\n",
            "Epoch 0, Phase train, Batch 405, Batch 1205/1259, Loss: 1.8204727172851562\n",
            "Epoch 0, Phase train, Batch 405, Batch 1205/1259, Loss: 1.8932777643203735\n",
            "Epoch 0, Phase train, Batch 229, Batch 1206/1259, Loss: 1.731633186340332\n",
            "Epoch 0, Phase train, Batch 229, Batch 1206/1259, Loss: 1.962381362915039\n",
            "Epoch 0, Phase train, Batch 1523, Batch 1207/1259, Loss: 1.8593913316726685\n",
            "Epoch 0, Phase train, Batch 1523, Batch 1207/1259, Loss: 1.8615580797195435\n",
            "Epoch 0, Phase train, Batch 79, Batch 1208/1259, Loss: 2.0545506477355957\n",
            "Epoch 0, Phase train, Batch 79, Batch 1208/1259, Loss: 1.8781532049179077\n",
            "Epoch 0, Phase train, Batch 1013, Batch 1209/1259, Loss: 1.814197063446045\n",
            "Epoch 0, Phase train, Batch 1013, Batch 1209/1259, Loss: 1.9814618825912476\n",
            "Epoch 0, Phase train, Batch 443, Batch 1210/1259, Loss: 1.9406459331512451\n",
            "Epoch 0, Phase train, Batch 443, Batch 1210/1259, Loss: 1.8055328130722046\n",
            "Epoch 0, Phase train, Batch 132, Batch 1211/1259, Loss: 1.9019535779953003\n",
            "Epoch 0, Phase train, Batch 132, Batch 1211/1259, Loss: 1.8793575763702393\n",
            "Epoch 0, Phase train, Batch 531, Batch 1212/1259, Loss: 1.7559430599212646\n",
            "Epoch 0, Phase train, Batch 531, Batch 1212/1259, Loss: 1.9474103450775146\n",
            "Epoch 0, Phase train, Batch 1197, Batch 1213/1259, Loss: 1.7976711988449097\n",
            "Epoch 0, Phase train, Batch 1197, Batch 1213/1259, Loss: 1.7839263677597046\n",
            "Epoch 0, Phase train, Batch 1394, Batch 1214/1259, Loss: 1.7543288469314575\n",
            "Epoch 0, Phase train, Batch 1394, Batch 1214/1259, Loss: 1.63581383228302\n",
            "Epoch 0, Phase train, Batch 374, Batch 1215/1259, Loss: 1.8988299369812012\n",
            "Epoch 0, Phase train, Batch 374, Batch 1215/1259, Loss: 1.9060066938400269\n",
            "Epoch 0, Phase train, Batch 168, Batch 1216/1259, Loss: 1.7292931079864502\n",
            "Epoch 0, Phase train, Batch 168, Batch 1216/1259, Loss: 1.8531385660171509\n",
            "Epoch 0, Phase train, Batch 1117, Batch 1217/1259, Loss: 1.923579216003418\n",
            "Epoch 0, Phase train, Batch 1117, Batch 1217/1259, Loss: 1.917533278465271\n",
            "Epoch 0, Phase train, Batch 127, Batch 1218/1259, Loss: 1.8502272367477417\n",
            "Epoch 0, Phase train, Batch 127, Batch 1218/1259, Loss: 1.8704307079315186\n",
            "Epoch 0, Phase train, Batch 32, Batch 1219/1259, Loss: 2.0945780277252197\n",
            "Epoch 0, Phase train, Batch 32, Batch 1219/1259, Loss: 2.182757616043091\n",
            "Epoch 0, Phase train, Batch 1182, Batch 1220/1259, Loss: 1.5784999132156372\n",
            "Epoch 0, Phase train, Batch 1182, Batch 1220/1259, Loss: 1.805190086364746\n",
            "Epoch 0, Phase train, Batch 8, Batch 1221/1259, Loss: 2.1020092964172363\n",
            "Epoch 0, Phase train, Batch 8, Batch 1221/1259, Loss: 2.0050840377807617\n",
            "Epoch 0, Phase train, Batch 684, Batch 1222/1259, Loss: 1.8211264610290527\n",
            "Epoch 0, Phase train, Batch 684, Batch 1222/1259, Loss: 1.8875172138214111\n",
            "Epoch 0, Phase train, Batch 1400, Batch 1223/1259, Loss: 1.6396639347076416\n",
            "Epoch 0, Phase train, Batch 1400, Batch 1223/1259, Loss: 1.807773470878601\n",
            "Epoch 0, Phase train, Batch 1451, Batch 1224/1259, Loss: 1.6355551481246948\n",
            "Epoch 0, Phase train, Batch 1451, Batch 1224/1259, Loss: 1.7782466411590576\n",
            "Epoch 0, Phase train, Batch 528, Batch 1225/1259, Loss: 1.7853455543518066\n",
            "Epoch 0, Phase train, Batch 528, Batch 1225/1259, Loss: 1.7250934839248657\n",
            "Epoch 0, Phase train, Batch 212, Batch 1226/1259, Loss: 1.8075205087661743\n",
            "Epoch 0, Phase train, Batch 212, Batch 1226/1259, Loss: 1.9886434078216553\n",
            "Epoch 0, Phase train, Batch 628, Batch 1227/1259, Loss: 1.8903824090957642\n",
            "Epoch 0, Phase train, Batch 628, Batch 1227/1259, Loss: 1.926365613937378\n",
            "Epoch 0, Phase train, Batch 914, Batch 1228/1259, Loss: 1.925890326499939\n",
            "Epoch 0, Phase train, Batch 914, Batch 1228/1259, Loss: 1.8959531784057617\n",
            "Epoch 0, Phase train, Batch 591, Batch 1229/1259, Loss: 1.872276782989502\n",
            "Epoch 0, Phase train, Batch 591, Batch 1229/1259, Loss: 1.8219040632247925\n",
            "Epoch 0, Phase train, Batch 738, Batch 1230/1259, Loss: 1.7140008211135864\n",
            "Epoch 0, Phase train, Batch 738, Batch 1230/1259, Loss: 1.7907469272613525\n",
            "Epoch 0, Phase train, Batch 1166, Batch 1231/1259, Loss: 1.8574986457824707\n",
            "Epoch 0, Phase train, Batch 1166, Batch 1231/1259, Loss: 1.7146300077438354\n",
            "Epoch 0, Phase train, Batch 1221, Batch 1232/1259, Loss: 1.7057145833969116\n",
            "Epoch 0, Phase train, Batch 1221, Batch 1232/1259, Loss: 1.688590407371521\n",
            "Epoch 0, Phase train, Batch 577, Batch 1233/1259, Loss: 1.8217384815216064\n",
            "Epoch 0, Phase train, Batch 577, Batch 1233/1259, Loss: 1.7654346227645874\n",
            "Epoch 0, Phase train, Batch 377, Batch 1234/1259, Loss: 1.6576383113861084\n",
            "Epoch 0, Phase train, Batch 377, Batch 1234/1259, Loss: 1.7125484943389893\n",
            "Epoch 0, Phase train, Batch 635, Batch 1235/1259, Loss: 1.9541308879852295\n",
            "Epoch 0, Phase train, Batch 635, Batch 1235/1259, Loss: 1.8910349607467651\n",
            "Epoch 0, Phase train, Batch 1362, Batch 1236/1259, Loss: 1.686893343925476\n",
            "Epoch 0, Phase train, Batch 1362, Batch 1236/1259, Loss: 1.7523136138916016\n",
            "Epoch 0, Phase train, Batch 1498, Batch 1237/1259, Loss: 1.7755331993103027\n",
            "Epoch 0, Phase train, Batch 1498, Batch 1237/1259, Loss: 1.5434480905532837\n",
            "Epoch 0, Phase train, Batch 1135, Batch 1238/1259, Loss: 1.7022203207015991\n",
            "Epoch 0, Phase train, Batch 1135, Batch 1238/1259, Loss: 1.760933518409729\n",
            "Epoch 0, Phase train, Batch 860, Batch 1239/1259, Loss: 1.92942476272583\n",
            "Epoch 0, Phase train, Batch 860, Batch 1239/1259, Loss: 1.6179254055023193\n",
            "Epoch 0, Phase train, Batch 1051, Batch 1240/1259, Loss: 1.7411739826202393\n",
            "Epoch 0, Phase train, Batch 1051, Batch 1240/1259, Loss: 1.7760264873504639\n",
            "Epoch 0, Phase train, Batch 974, Batch 1241/1259, Loss: 1.8000637292861938\n",
            "Epoch 0, Phase train, Batch 974, Batch 1241/1259, Loss: 1.8580464124679565\n",
            "Epoch 0, Phase train, Batch 20, Batch 1242/1259, Loss: 1.7582718133926392\n",
            "Epoch 0, Phase train, Batch 20, Batch 1242/1259, Loss: 1.9833717346191406\n",
            "Epoch 0, Phase train, Batch 1123, Batch 1243/1259, Loss: 1.9505360126495361\n",
            "Epoch 0, Phase train, Batch 1123, Batch 1243/1259, Loss: 1.758249044418335\n",
            "Epoch 0, Phase train, Batch 508, Batch 1244/1259, Loss: 1.790209174156189\n",
            "Epoch 0, Phase train, Batch 508, Batch 1244/1259, Loss: 1.8634600639343262\n",
            "Epoch 0, Phase train, Batch 1180, Batch 1245/1259, Loss: 1.6702762842178345\n",
            "Epoch 0, Phase train, Batch 1180, Batch 1245/1259, Loss: 1.62892746925354\n",
            "Epoch 0, Phase train, Batch 1160, Batch 1246/1259, Loss: 1.7663545608520508\n",
            "Epoch 0, Phase train, Batch 1160, Batch 1246/1259, Loss: 1.5578323602676392\n",
            "Epoch 0, Phase train, Batch 366, Batch 1247/1259, Loss: 1.7971769571304321\n",
            "Epoch 0, Phase train, Batch 366, Batch 1247/1259, Loss: 1.765551209449768\n",
            "Epoch 0, Phase train, Batch 1405, Batch 1248/1259, Loss: 1.8035943508148193\n",
            "Epoch 0, Phase train, Batch 1405, Batch 1248/1259, Loss: 1.7077569961547852\n",
            "Epoch 0, Phase train, Batch 330, Batch 1249/1259, Loss: 1.8691765069961548\n",
            "Epoch 0, Phase train, Batch 330, Batch 1249/1259, Loss: 1.9047815799713135\n",
            "Epoch 0, Phase train, Batch 518, Batch 1250/1259, Loss: 1.826927661895752\n",
            "Epoch 0, Phase train, Batch 518, Batch 1250/1259, Loss: 1.817007303237915\n",
            "Epoch 0, Phase train, Batch 842, Batch 1251/1259, Loss: 1.7812200784683228\n",
            "Epoch 0, Phase train, Batch 842, Batch 1251/1259, Loss: 1.6731057167053223\n",
            "Epoch 0, Phase train, Batch 668, Batch 1252/1259, Loss: 1.7869590520858765\n",
            "Epoch 0, Phase train, Batch 668, Batch 1252/1259, Loss: 1.9093025922775269\n",
            "Epoch 0, Phase train, Batch 843, Batch 1253/1259, Loss: 1.6842142343521118\n",
            "Epoch 0, Phase train, Batch 843, Batch 1253/1259, Loss: 1.8082443475723267\n",
            "Epoch 0, Phase train, Batch 264, Batch 1254/1259, Loss: 1.7999707460403442\n",
            "Epoch 0, Phase train, Batch 264, Batch 1254/1259, Loss: 1.6226038932800293\n",
            "Epoch 0, Phase train, Batch 1270, Batch 1255/1259, Loss: 1.6801906824111938\n",
            "Epoch 0, Phase train, Batch 1270, Batch 1255/1259, Loss: 1.6206697225570679\n",
            "Epoch 0, Phase train, Batch 1244, Batch 1256/1259, Loss: 1.9277915954589844\n",
            "Epoch 0, Phase train, Batch 1244, Batch 1256/1259, Loss: 1.792306661605835\n",
            "Epoch 0, Phase train, Batch 502, Batch 1257/1259, Loss: 1.8144627809524536\n",
            "Epoch 0, Phase train, Batch 502, Batch 1257/1259, Loss: 1.9145164489746094\n",
            "Epoch 0, Phase train, Batch 1044, Batch 1258/1259, Loss: 1.9177852869033813\n",
            "Epoch 0, Phase train, Batch 1044, Batch 1258/1259, Loss: 1.8567070960998535\n",
            "Epoch 0, Phase train, Batch 243, Batch 1259/1259, Loss: 1.718267798423767\n",
            "Epoch 0, Phase train, Batch 243, Batch 1259/1259, Loss: 1.90419602394104\n",
            "Epoch 0, Phase train completed.\n",
            "Epoch 0, Phase val, Batch 1148, Batch 1/314, Loss: 1.9206984043121338\n",
            "Epoch 0, Phase val, Batch 1148, Batch 1/314, Loss: 1.9854276180267334\n",
            "Epoch 0, Phase val, Batch 1469, Batch 2/314, Loss: 1.7954944372177124\n",
            "Epoch 0, Phase val, Batch 1469, Batch 2/314, Loss: 1.7476271390914917\n",
            "Epoch 0, Phase val, Batch 1001, Batch 3/314, Loss: 1.7744532823562622\n",
            "Epoch 0, Phase val, Batch 1001, Batch 3/314, Loss: 1.7242316007614136\n",
            "Epoch 0, Phase val, Batch 855, Batch 4/314, Loss: 1.8591140508651733\n",
            "Epoch 0, Phase val, Batch 855, Batch 4/314, Loss: 1.796402096748352\n",
            "Epoch 0, Phase val, Batch 248, Batch 5/314, Loss: 1.7764948606491089\n",
            "Epoch 0, Phase val, Batch 248, Batch 5/314, Loss: 1.827620267868042\n",
            "Epoch 0, Phase val, Batch 300, Batch 6/314, Loss: 1.8417311906814575\n",
            "Epoch 0, Phase val, Batch 300, Batch 6/314, Loss: 1.8053958415985107\n",
            "Epoch 0, Phase val, Batch 1350, Batch 7/314, Loss: 1.7008975744247437\n",
            "Epoch 0, Phase val, Batch 1350, Batch 7/314, Loss: 1.7549842596054077\n",
            "Epoch 0, Phase val, Batch 671, Batch 8/314, Loss: 1.8066169023513794\n",
            "Epoch 0, Phase val, Batch 671, Batch 8/314, Loss: 1.8673710823059082\n",
            "Epoch 0, Phase val, Batch 1053, Batch 9/314, Loss: 1.9525952339172363\n",
            "Epoch 0, Phase val, Batch 1053, Batch 9/314, Loss: 1.8504273891448975\n",
            "Epoch 0, Phase val, Batch 1354, Batch 10/314, Loss: 1.6228041648864746\n",
            "Epoch 0, Phase val, Batch 1354, Batch 10/314, Loss: 1.628647804260254\n",
            "Epoch 0, Phase val, Batch 512, Batch 11/314, Loss: 1.9267631769180298\n",
            "Epoch 0, Phase val, Batch 512, Batch 11/314, Loss: 1.7466309070587158\n",
            "Epoch 0, Phase val, Batch 34, Batch 12/314, Loss: 1.9246373176574707\n",
            "Epoch 0, Phase val, Batch 34, Batch 12/314, Loss: 2.077406644821167\n",
            "Epoch 0, Phase val, Batch 49, Batch 13/314, Loss: 2.020033121109009\n",
            "Epoch 0, Phase val, Batch 49, Batch 13/314, Loss: 2.0010828971862793\n",
            "Epoch 0, Phase val, Batch 456, Batch 14/314, Loss: 1.7628791332244873\n",
            "Epoch 0, Phase val, Batch 456, Batch 14/314, Loss: 1.6342785358428955\n",
            "Epoch 0, Phase val, Batch 1049, Batch 15/314, Loss: 1.9018239974975586\n",
            "Epoch 0, Phase val, Batch 1049, Batch 15/314, Loss: 1.9163795709609985\n",
            "Epoch 0, Phase val, Batch 453, Batch 16/314, Loss: 1.731636643409729\n",
            "Epoch 0, Phase val, Batch 453, Batch 16/314, Loss: 1.748412847518921\n",
            "Epoch 0, Phase val, Batch 563, Batch 17/314, Loss: 1.8147562742233276\n",
            "Epoch 0, Phase val, Batch 563, Batch 17/314, Loss: 1.9459949731826782\n",
            "Epoch 0, Phase val, Batch 1558, Batch 18/314, Loss: 1.7093560695648193\n",
            "Epoch 0, Phase val, Batch 1558, Batch 18/314, Loss: 1.6368098258972168\n",
            "Epoch 0, Phase val, Batch 509, Batch 19/314, Loss: 1.887986183166504\n",
            "Epoch 0, Phase val, Batch 509, Batch 19/314, Loss: 1.7971599102020264\n",
            "Epoch 0, Phase val, Batch 363, Batch 20/314, Loss: 1.6772541999816895\n",
            "Epoch 0, Phase val, Batch 363, Batch 20/314, Loss: 1.7706754207611084\n",
            "Epoch 0, Phase val, Batch 140, Batch 21/314, Loss: 1.9343159198760986\n",
            "Epoch 0, Phase val, Batch 140, Batch 21/314, Loss: 1.8544145822525024\n",
            "Epoch 0, Phase val, Batch 424, Batch 22/314, Loss: 1.8862825632095337\n",
            "Epoch 0, Phase val, Batch 424, Batch 22/314, Loss: 1.782824158668518\n",
            "Epoch 0, Phase val, Batch 1496, Batch 23/314, Loss: 1.702759027481079\n",
            "Epoch 0, Phase val, Batch 1496, Batch 23/314, Loss: 1.686899185180664\n",
            "Epoch 0, Phase val, Batch 710, Batch 24/314, Loss: 1.824985146522522\n",
            "Epoch 0, Phase val, Batch 710, Batch 24/314, Loss: 1.7370710372924805\n",
            "Epoch 0, Phase val, Batch 1069, Batch 25/314, Loss: 1.856858253479004\n",
            "Epoch 0, Phase val, Batch 1069, Batch 25/314, Loss: 1.8119864463806152\n",
            "Epoch 0, Phase val, Batch 656, Batch 26/314, Loss: 1.7110954523086548\n",
            "Epoch 0, Phase val, Batch 656, Batch 26/314, Loss: 1.7794052362442017\n",
            "Epoch 0, Phase val, Batch 1471, Batch 27/314, Loss: 1.903560757637024\n",
            "Epoch 0, Phase val, Batch 1471, Batch 27/314, Loss: 1.738465666770935\n",
            "Epoch 0, Phase val, Batch 1378, Batch 28/314, Loss: 1.857325792312622\n",
            "Epoch 0, Phase val, Batch 1378, Batch 28/314, Loss: 1.6542186737060547\n",
            "Epoch 0, Phase val, Batch 853, Batch 29/314, Loss: 1.7286536693572998\n",
            "Epoch 0, Phase val, Batch 853, Batch 29/314, Loss: 1.6283599138259888\n",
            "Epoch 0, Phase val, Batch 1449, Batch 30/314, Loss: 1.662638545036316\n",
            "Epoch 0, Phase val, Batch 1449, Batch 30/314, Loss: 1.6669424772262573\n",
            "Epoch 0, Phase val, Batch 395, Batch 31/314, Loss: 1.6822429895401\n",
            "Epoch 0, Phase val, Batch 395, Batch 31/314, Loss: 2.052929401397705\n",
            "Epoch 0, Phase val, Batch 654, Batch 32/314, Loss: 1.8816194534301758\n",
            "Epoch 0, Phase val, Batch 654, Batch 32/314, Loss: 1.8564555644989014\n",
            "Epoch 0, Phase val, Batch 1254, Batch 33/314, Loss: 1.6272656917572021\n",
            "Epoch 0, Phase val, Batch 1254, Batch 33/314, Loss: 1.8049150705337524\n",
            "Epoch 0, Phase val, Batch 988, Batch 34/314, Loss: 1.8421038389205933\n",
            "Epoch 0, Phase val, Batch 988, Batch 34/314, Loss: 1.6916530132293701\n",
            "Epoch 0, Phase val, Batch 1500, Batch 35/314, Loss: 1.7236963510513306\n",
            "Epoch 0, Phase val, Batch 1500, Batch 35/314, Loss: 1.7141814231872559\n",
            "Epoch 0, Phase val, Batch 178, Batch 36/314, Loss: 1.9610059261322021\n",
            "Epoch 0, Phase val, Batch 178, Batch 36/314, Loss: 1.8256115913391113\n",
            "Epoch 0, Phase val, Batch 969, Batch 37/314, Loss: 1.8517520427703857\n",
            "Epoch 0, Phase val, Batch 969, Batch 37/314, Loss: 1.8395432233810425\n",
            "Epoch 0, Phase val, Batch 430, Batch 38/314, Loss: 1.797145128250122\n",
            "Epoch 0, Phase val, Batch 430, Batch 38/314, Loss: 1.7793941497802734\n",
            "Epoch 0, Phase val, Batch 1522, Batch 39/314, Loss: 1.8411736488342285\n",
            "Epoch 0, Phase val, Batch 1522, Batch 39/314, Loss: 1.8932832479476929\n",
            "Epoch 0, Phase val, Batch 1475, Batch 40/314, Loss: 1.857059359550476\n",
            "Epoch 0, Phase val, Batch 1475, Batch 40/314, Loss: 1.7476670742034912\n",
            "Epoch 0, Phase val, Batch 846, Batch 41/314, Loss: 1.8215011358261108\n",
            "Epoch 0, Phase val, Batch 846, Batch 41/314, Loss: 1.878721833229065\n",
            "Epoch 0, Phase val, Batch 608, Batch 42/314, Loss: 1.9256235361099243\n",
            "Epoch 0, Phase val, Batch 608, Batch 42/314, Loss: 1.965079665184021\n",
            "Epoch 0, Phase val, Batch 589, Batch 43/314, Loss: 2.0807785987854004\n",
            "Epoch 0, Phase val, Batch 589, Batch 43/314, Loss: 1.831800937652588\n",
            "Epoch 0, Phase val, Batch 1313, Batch 44/314, Loss: 1.6568561792373657\n",
            "Epoch 0, Phase val, Batch 1313, Batch 44/314, Loss: 1.7481417655944824\n",
            "Epoch 0, Phase val, Batch 1286, Batch 45/314, Loss: 1.7123441696166992\n",
            "Epoch 0, Phase val, Batch 1286, Batch 45/314, Loss: 1.8261501789093018\n",
            "Epoch 0, Phase val, Batch 1193, Batch 46/314, Loss: 1.7151756286621094\n",
            "Epoch 0, Phase val, Batch 1193, Batch 46/314, Loss: 1.700667381286621\n",
            "Epoch 0, Phase val, Batch 1106, Batch 47/314, Loss: 1.9175468683242798\n",
            "Epoch 0, Phase val, Batch 1106, Batch 47/314, Loss: 1.6912496089935303\n",
            "Epoch 0, Phase val, Batch 465, Batch 48/314, Loss: 1.8248591423034668\n",
            "Epoch 0, Phase val, Batch 465, Batch 48/314, Loss: 1.7152414321899414\n",
            "Epoch 0, Phase val, Batch 1347, Batch 49/314, Loss: 1.6810475587844849\n",
            "Epoch 0, Phase val, Batch 1347, Batch 49/314, Loss: 1.6984657049179077\n",
            "Epoch 0, Phase val, Batch 1422, Batch 50/314, Loss: 1.6913554668426514\n",
            "Epoch 0, Phase val, Batch 1422, Batch 50/314, Loss: 1.7999898195266724\n",
            "Epoch 0, Phase val, Batch 1547, Batch 51/314, Loss: 1.725380778312683\n",
            "Epoch 0, Phase val, Batch 1547, Batch 51/314, Loss: 1.8129395246505737\n",
            "Epoch 0, Phase val, Batch 1159, Batch 52/314, Loss: 1.869495153427124\n",
            "Epoch 0, Phase val, Batch 1159, Batch 52/314, Loss: 1.8099356889724731\n",
            "Epoch 0, Phase val, Batch 242, Batch 53/314, Loss: 1.9877634048461914\n",
            "Epoch 0, Phase val, Batch 242, Batch 53/314, Loss: 1.899296760559082\n",
            "Epoch 0, Phase val, Batch 819, Batch 54/314, Loss: 1.8097256422042847\n",
            "Epoch 0, Phase val, Batch 819, Batch 54/314, Loss: 2.0032384395599365\n",
            "Epoch 0, Phase val, Batch 16, Batch 55/314, Loss: 1.9824074506759644\n",
            "Epoch 0, Phase val, Batch 16, Batch 55/314, Loss: 1.9789533615112305\n",
            "Epoch 0, Phase val, Batch 888, Batch 56/314, Loss: 1.9254459142684937\n",
            "Epoch 0, Phase val, Batch 888, Batch 56/314, Loss: 1.6894502639770508\n",
            "Epoch 0, Phase val, Batch 538, Batch 57/314, Loss: 1.760758876800537\n",
            "Epoch 0, Phase val, Batch 538, Batch 57/314, Loss: 1.9076820611953735\n",
            "Epoch 0, Phase val, Batch 417, Batch 58/314, Loss: 1.6865664720535278\n",
            "Epoch 0, Phase val, Batch 417, Batch 58/314, Loss: 1.7636663913726807\n",
            "Epoch 0, Phase val, Batch 743, Batch 59/314, Loss: 1.8722888231277466\n",
            "Epoch 0, Phase val, Batch 743, Batch 59/314, Loss: 1.8266823291778564\n",
            "Epoch 0, Phase val, Batch 41, Batch 60/314, Loss: 1.9534492492675781\n",
            "Epoch 0, Phase val, Batch 41, Batch 60/314, Loss: 1.909075140953064\n",
            "Epoch 0, Phase val, Batch 1331, Batch 61/314, Loss: 1.683948040008545\n",
            "Epoch 0, Phase val, Batch 1331, Batch 61/314, Loss: 1.6994060277938843\n",
            "Epoch 0, Phase val, Batch 1119, Batch 62/314, Loss: 1.8931937217712402\n",
            "Epoch 0, Phase val, Batch 1119, Batch 62/314, Loss: 1.8558090925216675\n",
            "Epoch 0, Phase val, Batch 887, Batch 63/314, Loss: 1.7171261310577393\n",
            "Epoch 0, Phase val, Batch 887, Batch 63/314, Loss: 1.863818645477295\n",
            "Epoch 0, Phase val, Batch 427, Batch 64/314, Loss: 1.6967487335205078\n",
            "Epoch 0, Phase val, Batch 427, Batch 64/314, Loss: 1.72894287109375\n",
            "Epoch 0, Phase val, Batch 1540, Batch 65/314, Loss: 1.6570188999176025\n",
            "Epoch 0, Phase val, Batch 1540, Batch 65/314, Loss: 1.7044423818588257\n",
            "Epoch 0, Phase val, Batch 349, Batch 66/314, Loss: 1.7972859144210815\n",
            "Epoch 0, Phase val, Batch 349, Batch 66/314, Loss: 1.7166458368301392\n",
            "Epoch 0, Phase val, Batch 1222, Batch 67/314, Loss: 1.6330697536468506\n",
            "Epoch 0, Phase val, Batch 1222, Batch 67/314, Loss: 1.7186676263809204\n",
            "Epoch 0, Phase val, Batch 1032, Batch 68/314, Loss: 1.9441947937011719\n",
            "Epoch 0, Phase val, Batch 1032, Batch 68/314, Loss: 1.7051541805267334\n",
            "Epoch 0, Phase val, Batch 169, Batch 69/314, Loss: 1.7372640371322632\n",
            "Epoch 0, Phase val, Batch 169, Batch 69/314, Loss: 1.898605465888977\n",
            "Epoch 0, Phase val, Batch 89, Batch 70/314, Loss: 1.8214945793151855\n",
            "Epoch 0, Phase val, Batch 89, Batch 70/314, Loss: 1.9027591943740845\n",
            "Epoch 0, Phase val, Batch 1310, Batch 71/314, Loss: 1.7510759830474854\n",
            "Epoch 0, Phase val, Batch 1310, Batch 71/314, Loss: 1.7745471000671387\n",
            "Epoch 0, Phase val, Batch 347, Batch 72/314, Loss: 1.6692826747894287\n",
            "Epoch 0, Phase val, Batch 347, Batch 72/314, Loss: 1.7681928873062134\n",
            "Epoch 0, Phase val, Batch 445, Batch 73/314, Loss: 1.6583799123764038\n",
            "Epoch 0, Phase val, Batch 445, Batch 73/314, Loss: 1.7276471853256226\n",
            "Epoch 0, Phase val, Batch 1433, Batch 74/314, Loss: 1.5869412422180176\n",
            "Epoch 0, Phase val, Batch 1433, Batch 74/314, Loss: 1.732702612876892\n",
            "Epoch 0, Phase val, Batch 1467, Batch 75/314, Loss: 1.6469051837921143\n",
            "Epoch 0, Phase val, Batch 1467, Batch 75/314, Loss: 1.7075729370117188\n",
            "Epoch 0, Phase val, Batch 1137, Batch 76/314, Loss: 1.824906349182129\n",
            "Epoch 0, Phase val, Batch 1137, Batch 76/314, Loss: 1.731428623199463\n",
            "Epoch 0, Phase val, Batch 1264, Batch 77/314, Loss: 1.6073122024536133\n",
            "Epoch 0, Phase val, Batch 1264, Batch 77/314, Loss: 1.6405866146087646\n",
            "Epoch 0, Phase val, Batch 767, Batch 78/314, Loss: 1.86013925075531\n",
            "Epoch 0, Phase val, Batch 767, Batch 78/314, Loss: 1.7901878356933594\n",
            "Epoch 0, Phase val, Batch 726, Batch 79/314, Loss: 1.832621693611145\n",
            "Epoch 0, Phase val, Batch 726, Batch 79/314, Loss: 1.885600209236145\n",
            "Epoch 0, Phase val, Batch 980, Batch 80/314, Loss: 1.9717340469360352\n",
            "Epoch 0, Phase val, Batch 980, Batch 80/314, Loss: 1.956578254699707\n",
            "Epoch 0, Phase val, Batch 1438, Batch 81/314, Loss: 1.7162723541259766\n",
            "Epoch 0, Phase val, Batch 1438, Batch 81/314, Loss: 1.8486976623535156\n",
            "Epoch 0, Phase val, Batch 551, Batch 82/314, Loss: 1.8307923078536987\n",
            "Epoch 0, Phase val, Batch 551, Batch 82/314, Loss: 1.8015929460525513\n",
            "Epoch 0, Phase val, Batch 717, Batch 83/314, Loss: 1.897397518157959\n",
            "Epoch 0, Phase val, Batch 717, Batch 83/314, Loss: 1.7552380561828613\n",
            "Epoch 0, Phase val, Batch 639, Batch 84/314, Loss: 1.7697093486785889\n",
            "Epoch 0, Phase val, Batch 639, Batch 84/314, Loss: 1.672547698020935\n",
            "Epoch 0, Phase val, Batch 104, Batch 85/314, Loss: 2.056607723236084\n",
            "Epoch 0, Phase val, Batch 104, Batch 85/314, Loss: 2.1636765003204346\n",
            "Epoch 0, Phase val, Batch 578, Batch 86/314, Loss: 1.750475287437439\n",
            "Epoch 0, Phase val, Batch 578, Batch 86/314, Loss: 1.8027679920196533\n",
            "Epoch 0, Phase val, Batch 941, Batch 87/314, Loss: 1.7437025308609009\n",
            "Epoch 0, Phase val, Batch 941, Batch 87/314, Loss: 1.771443247795105\n",
            "Epoch 0, Phase val, Batch 516, Batch 88/314, Loss: 1.6933681964874268\n",
            "Epoch 0, Phase val, Batch 516, Batch 88/314, Loss: 1.7470811605453491\n",
            "Epoch 0, Phase val, Batch 290, Batch 89/314, Loss: 1.8210058212280273\n",
            "Epoch 0, Phase val, Batch 290, Batch 89/314, Loss: 1.903943419456482\n",
            "Epoch 0, Phase val, Batch 195, Batch 90/314, Loss: 1.88494873046875\n",
            "Epoch 0, Phase val, Batch 195, Batch 90/314, Loss: 1.7602146863937378\n",
            "Epoch 0, Phase val, Batch 1567, Batch 91/314, Loss: 1.7646033763885498\n",
            "Epoch 0, Phase val, Batch 1567, Batch 91/314, Loss: 1.6509718894958496\n",
            "Epoch 0, Phase val, Batch 266, Batch 92/314, Loss: 1.8227089643478394\n",
            "Epoch 0, Phase val, Batch 266, Batch 92/314, Loss: 1.82948899269104\n",
            "Epoch 0, Phase val, Batch 1417, Batch 93/314, Loss: 1.6528080701828003\n",
            "Epoch 0, Phase val, Batch 1417, Batch 93/314, Loss: 1.582329273223877\n",
            "Epoch 0, Phase val, Batch 740, Batch 94/314, Loss: 1.9378927946090698\n",
            "Epoch 0, Phase val, Batch 740, Batch 94/314, Loss: 1.9557496309280396\n",
            "Epoch 0, Phase val, Batch 783, Batch 95/314, Loss: 1.8001559972763062\n",
            "Epoch 0, Phase val, Batch 783, Batch 95/314, Loss: 1.806062936782837\n",
            "Epoch 0, Phase val, Batch 755, Batch 96/314, Loss: 1.8011958599090576\n",
            "Epoch 0, Phase val, Batch 755, Batch 96/314, Loss: 1.6872940063476562\n",
            "Epoch 0, Phase val, Batch 1138, Batch 97/314, Loss: 1.8510842323303223\n",
            "Epoch 0, Phase val, Batch 1138, Batch 97/314, Loss: 1.7702703475952148\n",
            "Epoch 0, Phase val, Batch 918, Batch 98/314, Loss: 1.844884991645813\n",
            "Epoch 0, Phase val, Batch 918, Batch 98/314, Loss: 1.942541480064392\n",
            "Epoch 0, Phase val, Batch 123, Batch 99/314, Loss: 1.9315931797027588\n",
            "Epoch 0, Phase val, Batch 123, Batch 99/314, Loss: 1.998489499092102\n",
            "Epoch 0, Phase val, Batch 1188, Batch 100/314, Loss: 1.5970051288604736\n",
            "Epoch 0, Phase val, Batch 1188, Batch 100/314, Loss: 1.7244065999984741\n",
            "Epoch 0, Phase val, Batch 253, Batch 101/314, Loss: 1.6628495454788208\n",
            "Epoch 0, Phase val, Batch 253, Batch 101/314, Loss: 1.77614426612854\n",
            "Epoch 0, Phase val, Batch 1089, Batch 102/314, Loss: 1.8027417659759521\n",
            "Epoch 0, Phase val, Batch 1089, Batch 102/314, Loss: 1.7606160640716553\n",
            "Epoch 0, Phase val, Batch 162, Batch 103/314, Loss: 1.841541051864624\n",
            "Epoch 0, Phase val, Batch 162, Batch 103/314, Loss: 1.9717395305633545\n",
            "Epoch 0, Phase val, Batch 535, Batch 104/314, Loss: 1.8788752555847168\n",
            "Epoch 0, Phase val, Batch 535, Batch 104/314, Loss: 1.8287131786346436\n",
            "Epoch 0, Phase val, Batch 75, Batch 105/314, Loss: 1.7989765405654907\n",
            "Epoch 0, Phase val, Batch 75, Batch 105/314, Loss: 1.8633979558944702\n",
            "Epoch 0, Phase val, Batch 137, Batch 106/314, Loss: 2.0070149898529053\n",
            "Epoch 0, Phase val, Batch 137, Batch 106/314, Loss: 1.7112979888916016\n",
            "Epoch 0, Phase val, Batch 1476, Batch 107/314, Loss: 1.8080202341079712\n",
            "Epoch 0, Phase val, Batch 1476, Batch 107/314, Loss: 1.734085202217102\n",
            "Epoch 0, Phase val, Batch 775, Batch 108/314, Loss: 1.7316768169403076\n",
            "Epoch 0, Phase val, Batch 775, Batch 108/314, Loss: 1.796245813369751\n",
            "Epoch 0, Phase val, Batch 944, Batch 109/314, Loss: 1.8149677515029907\n",
            "Epoch 0, Phase val, Batch 944, Batch 109/314, Loss: 1.8790202140808105\n",
            "Epoch 0, Phase val, Batch 1007, Batch 110/314, Loss: 1.875436544418335\n",
            "Epoch 0, Phase val, Batch 1007, Batch 110/314, Loss: 1.8748905658721924\n",
            "Epoch 0, Phase val, Batch 640, Batch 111/314, Loss: 1.6612615585327148\n",
            "Epoch 0, Phase val, Batch 640, Batch 111/314, Loss: 1.8590898513793945\n",
            "Epoch 0, Phase val, Batch 1551, Batch 112/314, Loss: 1.8082817792892456\n",
            "Epoch 0, Phase val, Batch 1551, Batch 112/314, Loss: 1.571641206741333\n",
            "Epoch 0, Phase val, Batch 615, Batch 113/314, Loss: 1.9061660766601562\n",
            "Epoch 0, Phase val, Batch 615, Batch 113/314, Loss: 1.828899621963501\n",
            "Epoch 0, Phase val, Batch 1257, Batch 114/314, Loss: 1.6967929601669312\n",
            "Epoch 0, Phase val, Batch 1257, Batch 114/314, Loss: 1.6868629455566406\n",
            "Epoch 0, Phase val, Batch 1259, Batch 115/314, Loss: 1.6694676876068115\n",
            "Epoch 0, Phase val, Batch 1259, Batch 115/314, Loss: 1.6973391771316528\n",
            "Epoch 0, Phase val, Batch 1285, Batch 116/314, Loss: 1.8017475605010986\n",
            "Epoch 0, Phase val, Batch 1285, Batch 116/314, Loss: 1.7946958541870117\n",
            "Epoch 0, Phase val, Batch 1008, Batch 117/314, Loss: 1.6233144998550415\n",
            "Epoch 0, Phase val, Batch 1008, Batch 117/314, Loss: 1.931569218635559\n",
            "Epoch 0, Phase val, Batch 1130, Batch 118/314, Loss: 1.8580892086029053\n",
            "Epoch 0, Phase val, Batch 1130, Batch 118/314, Loss: 1.8422895669937134\n",
            "Epoch 0, Phase val, Batch 962, Batch 119/314, Loss: 1.985063076019287\n",
            "Epoch 0, Phase val, Batch 962, Batch 119/314, Loss: 1.8734456300735474\n",
            "Epoch 0, Phase val, Batch 1542, Batch 120/314, Loss: 1.6615309715270996\n",
            "Epoch 0, Phase val, Batch 1542, Batch 120/314, Loss: 1.7686022520065308\n",
            "Epoch 0, Phase val, Batch 113, Batch 121/314, Loss: 2.002305030822754\n",
            "Epoch 0, Phase val, Batch 113, Batch 121/314, Loss: 1.9684646129608154\n",
            "Epoch 0, Phase val, Batch 1466, Batch 122/314, Loss: 1.5742889642715454\n",
            "Epoch 0, Phase val, Batch 1466, Batch 122/314, Loss: 1.637043833732605\n",
            "Epoch 0, Phase val, Batch 702, Batch 123/314, Loss: 1.8925323486328125\n",
            "Epoch 0, Phase val, Batch 702, Batch 123/314, Loss: 1.893517017364502\n",
            "Epoch 0, Phase val, Batch 1090, Batch 124/314, Loss: 1.9769837856292725\n",
            "Epoch 0, Phase val, Batch 1090, Batch 124/314, Loss: 1.8694404363632202\n",
            "Epoch 0, Phase val, Batch 416, Batch 125/314, Loss: 1.6582587957382202\n",
            "Epoch 0, Phase val, Batch 416, Batch 125/314, Loss: 1.9847707748413086\n",
            "Epoch 0, Phase val, Batch 35, Batch 126/314, Loss: 2.0480871200561523\n",
            "Epoch 0, Phase val, Batch 35, Batch 126/314, Loss: 2.064558982849121\n",
            "Epoch 0, Phase val, Batch 1462, Batch 127/314, Loss: 1.82687246799469\n",
            "Epoch 0, Phase val, Batch 1462, Batch 127/314, Loss: 1.8103004693984985\n",
            "Epoch 0, Phase val, Batch 1559, Batch 128/314, Loss: 1.7443444728851318\n",
            "Epoch 0, Phase val, Batch 1559, Batch 128/314, Loss: 1.6839396953582764\n",
            "Epoch 0, Phase val, Batch 1104, Batch 129/314, Loss: 1.8701351881027222\n",
            "Epoch 0, Phase val, Batch 1104, Batch 129/314, Loss: 1.9228039979934692\n",
            "Epoch 0, Phase val, Batch 672, Batch 130/314, Loss: 1.8870753049850464\n",
            "Epoch 0, Phase val, Batch 672, Batch 130/314, Loss: 1.794703483581543\n",
            "Epoch 0, Phase val, Batch 308, Batch 131/314, Loss: 1.909686803817749\n",
            "Epoch 0, Phase val, Batch 308, Batch 131/314, Loss: 1.9210203886032104\n",
            "Epoch 0, Phase val, Batch 1563, Batch 132/314, Loss: 1.6915416717529297\n",
            "Epoch 0, Phase val, Batch 1563, Batch 132/314, Loss: 1.6691893339157104\n",
            "Epoch 0, Phase val, Batch 673, Batch 133/314, Loss: 1.8397574424743652\n",
            "Epoch 0, Phase val, Batch 673, Batch 133/314, Loss: 1.9444046020507812\n",
            "Epoch 0, Phase val, Batch 1006, Batch 134/314, Loss: 1.9864587783813477\n",
            "Epoch 0, Phase val, Batch 1006, Batch 134/314, Loss: 1.87874174118042\n",
            "Epoch 0, Phase val, Batch 1393, Batch 135/314, Loss: 1.75473153591156\n",
            "Epoch 0, Phase val, Batch 1393, Batch 135/314, Loss: 1.7415269613265991\n",
            "Epoch 0, Phase val, Batch 682, Batch 136/314, Loss: 1.817236065864563\n",
            "Epoch 0, Phase val, Batch 682, Batch 136/314, Loss: 1.695320963859558\n",
            "Epoch 0, Phase val, Batch 1479, Batch 137/314, Loss: 1.7954806089401245\n",
            "Epoch 0, Phase val, Batch 1479, Batch 137/314, Loss: 1.795515775680542\n",
            "Epoch 0, Phase val, Batch 1278, Batch 138/314, Loss: 1.545125126838684\n",
            "Epoch 0, Phase val, Batch 1278, Batch 138/314, Loss: 1.66387939453125\n",
            "Epoch 0, Phase val, Batch 714, Batch 139/314, Loss: 1.7840772867202759\n",
            "Epoch 0, Phase val, Batch 714, Batch 139/314, Loss: 1.9704687595367432\n",
            "Epoch 0, Phase val, Batch 1224, Batch 140/314, Loss: 1.6776632070541382\n",
            "Epoch 0, Phase val, Batch 1224, Batch 140/314, Loss: 1.5609862804412842\n",
            "Epoch 0, Phase val, Batch 593, Batch 141/314, Loss: 1.8063545227050781\n",
            "Epoch 0, Phase val, Batch 593, Batch 141/314, Loss: 2.030672311782837\n",
            "Epoch 0, Phase val, Batch 664, Batch 142/314, Loss: 1.8128172159194946\n",
            "Epoch 0, Phase val, Batch 664, Batch 142/314, Loss: 1.9307224750518799\n",
            "Epoch 0, Phase val, Batch 1167, Batch 144/314, Loss: 1.7808403968811035\n",
            "Epoch 0, Phase val, Batch 1167, Batch 144/314, Loss: 1.723655104637146\n",
            "Epoch 0, Phase val, Batch 1173, Batch 145/314, Loss: 1.8079429864883423\n",
            "Epoch 0, Phase val, Batch 1173, Batch 145/314, Loss: 1.8035657405853271\n",
            "Epoch 0, Phase val, Batch 872, Batch 146/314, Loss: 1.9064172506332397\n",
            "Epoch 0, Phase val, Batch 872, Batch 146/314, Loss: 1.735047459602356\n",
            "Epoch 0, Phase val, Batch 1287, Batch 147/314, Loss: 1.6962403059005737\n",
            "Epoch 0, Phase val, Batch 1287, Batch 147/314, Loss: 1.5533337593078613\n",
            "Epoch 0, Phase val, Batch 97, Batch 148/314, Loss: 1.8814629316329956\n",
            "Epoch 0, Phase val, Batch 97, Batch 148/314, Loss: 1.836049199104309\n",
            "Epoch 0, Phase val, Batch 393, Batch 149/314, Loss: 1.5762232542037964\n",
            "Epoch 0, Phase val, Batch 393, Batch 149/314, Loss: 1.8292189836502075\n",
            "Epoch 0, Phase val, Batch 98, Batch 150/314, Loss: 1.8848146200180054\n",
            "Epoch 0, Phase val, Batch 98, Batch 150/314, Loss: 1.8900115489959717\n",
            "Epoch 0, Phase val, Batch 770, Batch 151/314, Loss: 1.8286094665527344\n",
            "Epoch 0, Phase val, Batch 770, Batch 151/314, Loss: 1.8516242504119873\n",
            "Epoch 0, Phase val, Batch 356, Batch 152/314, Loss: 1.7794634103775024\n",
            "Epoch 0, Phase val, Batch 356, Batch 152/314, Loss: 1.694157361984253\n",
            "Epoch 0, Phase val, Batch 1050, Batch 153/314, Loss: 1.8613967895507812\n",
            "Epoch 0, Phase val, Batch 1050, Batch 153/314, Loss: 1.7760213613510132\n",
            "Epoch 0, Phase val, Batch 380, Batch 154/314, Loss: 1.7834482192993164\n",
            "Epoch 0, Phase val, Batch 380, Batch 154/314, Loss: 1.9619840383529663\n",
            "Epoch 0, Phase val, Batch 926, Batch 155/314, Loss: 1.887168049812317\n",
            "Epoch 0, Phase val, Batch 926, Batch 155/314, Loss: 1.8265773057937622\n",
            "Epoch 0, Phase val, Batch 488, Batch 156/314, Loss: 1.6910207271575928\n",
            "Epoch 0, Phase val, Batch 488, Batch 156/314, Loss: 1.7974318265914917\n",
            "Epoch 0, Phase val, Batch 1063, Batch 157/314, Loss: 1.8491239547729492\n",
            "Epoch 0, Phase val, Batch 1063, Batch 157/314, Loss: 1.5470211505889893\n",
            "Epoch 0, Phase val, Batch 620, Batch 158/314, Loss: 1.8427821397781372\n",
            "Epoch 0, Phase val, Batch 620, Batch 158/314, Loss: 1.8391493558883667\n",
            "Epoch 0, Phase val, Batch 434, Batch 159/314, Loss: 1.7450554370880127\n",
            "Epoch 0, Phase val, Batch 434, Batch 159/314, Loss: 1.9289742708206177\n",
            "Epoch 0, Phase val, Batch 1464, Batch 160/314, Loss: 1.5739860534667969\n",
            "Epoch 0, Phase val, Batch 1464, Batch 160/314, Loss: 1.8571662902832031\n",
            "Epoch 0, Phase val, Batch 1223, Batch 161/314, Loss: 1.800814151763916\n",
            "Epoch 0, Phase val, Batch 1223, Batch 161/314, Loss: 1.6459429264068604\n",
            "Epoch 0, Phase val, Batch 254, Batch 162/314, Loss: 1.8361388444900513\n",
            "Epoch 0, Phase val, Batch 254, Batch 162/314, Loss: 1.8362089395523071\n",
            "Epoch 0, Phase val, Batch 329, Batch 163/314, Loss: 1.6972436904907227\n",
            "Epoch 0, Phase val, Batch 329, Batch 163/314, Loss: 1.8092917203903198\n",
            "Epoch 0, Phase val, Batch 1096, Batch 164/314, Loss: 1.8553236722946167\n",
            "Epoch 0, Phase val, Batch 1096, Batch 164/314, Loss: 1.800971508026123\n",
            "Epoch 0, Phase val, Batch 489, Batch 165/314, Loss: 1.6170196533203125\n",
            "Epoch 0, Phase val, Batch 489, Batch 165/314, Loss: 1.743370771408081\n",
            "Epoch 0, Phase val, Batch 357, Batch 166/314, Loss: 1.712905764579773\n",
            "Epoch 0, Phase val, Batch 357, Batch 166/314, Loss: 1.687119483947754\n",
            "Epoch 0, Phase val, Batch 1477, Batch 167/314, Loss: 1.7505220174789429\n",
            "Epoch 0, Phase val, Batch 1477, Batch 167/314, Loss: 1.716048240661621\n",
            "Epoch 0, Phase val, Batch 1421, Batch 168/314, Loss: 1.6932491064071655\n",
            "Epoch 0, Phase val, Batch 1421, Batch 168/314, Loss: 1.7311302423477173\n",
            "Epoch 0, Phase val, Batch 1024, Batch 169/314, Loss: 1.9665977954864502\n",
            "Epoch 0, Phase val, Batch 1024, Batch 169/314, Loss: 1.7670347690582275\n",
            "Epoch 0, Phase val, Batch 343, Batch 170/314, Loss: 1.5806026458740234\n",
            "Epoch 0, Phase val, Batch 343, Batch 170/314, Loss: 1.9270391464233398\n",
            "Epoch 0, Phase val, Batch 1291, Batch 171/314, Loss: 1.6505491733551025\n",
            "Epoch 0, Phase val, Batch 1291, Batch 171/314, Loss: 1.6516424417495728\n",
            "Epoch 0, Phase val, Batch 115, Batch 172/314, Loss: 1.7959604263305664\n",
            "Epoch 0, Phase val, Batch 115, Batch 172/314, Loss: 1.845320463180542\n",
            "Epoch 0, Phase val, Batch 76, Batch 173/314, Loss: 1.8916553258895874\n",
            "Epoch 0, Phase val, Batch 76, Batch 173/314, Loss: 1.8637795448303223\n",
            "Epoch 0, Phase val, Batch 739, Batch 174/314, Loss: 1.6670044660568237\n",
            "Epoch 0, Phase val, Batch 739, Batch 174/314, Loss: 1.7155824899673462\n",
            "Epoch 0, Phase val, Batch 1564, Batch 175/314, Loss: 1.8732476234436035\n",
            "Epoch 0, Phase val, Batch 1564, Batch 175/314, Loss: 1.6918525695800781\n",
            "Epoch 0, Phase val, Batch 271, Batch 176/314, Loss: 1.905347228050232\n",
            "Epoch 0, Phase val, Batch 271, Batch 176/314, Loss: 1.7780146598815918\n",
            "Epoch 0, Phase val, Batch 1339, Batch 177/314, Loss: 1.71622633934021\n",
            "Epoch 0, Phase val, Batch 1339, Batch 177/314, Loss: 1.7884823083877563\n",
            "Epoch 0, Phase val, Batch 1544, Batch 178/314, Loss: 1.7523555755615234\n",
            "Epoch 0, Phase val, Batch 1544, Batch 178/314, Loss: 1.7663254737854004\n",
            "Epoch 0, Phase val, Batch 1489, Batch 179/314, Loss: 1.8230570554733276\n",
            "Epoch 0, Phase val, Batch 1489, Batch 179/314, Loss: 1.7417998313903809\n",
            "Epoch 0, Phase val, Batch 930, Batch 180/314, Loss: 1.9433056116104126\n",
            "Epoch 0, Phase val, Batch 930, Batch 180/314, Loss: 1.9470515251159668\n",
            "Epoch 0, Phase val, Batch 594, Batch 181/314, Loss: 1.8756715059280396\n",
            "Epoch 0, Phase val, Batch 594, Batch 181/314, Loss: 2.0103859901428223\n",
            "Epoch 0, Phase val, Batch 1536, Batch 182/314, Loss: 2.0813043117523193\n",
            "Epoch 0, Phase val, Batch 1536, Batch 182/314, Loss: 2.0508551597595215\n",
            "Epoch 0, Phase val, Batch 170, Batch 183/314, Loss: 1.8000293970108032\n",
            "Epoch 0, Phase val, Batch 170, Batch 183/314, Loss: 1.6691136360168457\n",
            "Epoch 0, Phase val, Batch 1507, Batch 184/314, Loss: 1.8412758111953735\n",
            "Epoch 0, Phase val, Batch 1507, Batch 184/314, Loss: 1.803714394569397\n",
            "Epoch 0, Phase val, Batch 1111, Batch 185/314, Loss: 1.7478233575820923\n",
            "Epoch 0, Phase val, Batch 1111, Batch 185/314, Loss: 1.9434528350830078\n",
            "Epoch 0, Phase val, Batch 471, Batch 186/314, Loss: 1.703818440437317\n",
            "Epoch 0, Phase val, Batch 471, Batch 186/314, Loss: 1.626783847808838\n",
            "Epoch 0, Phase val, Batch 1491, Batch 187/314, Loss: 1.7380794286727905\n",
            "Epoch 0, Phase val, Batch 1491, Batch 187/314, Loss: 1.7712007761001587\n",
            "Epoch 0, Phase val, Batch 42, Batch 188/314, Loss: 2.1560914516448975\n",
            "Epoch 0, Phase val, Batch 42, Batch 188/314, Loss: 1.895443081855774\n",
            "Epoch 0, Phase val, Batch 433, Batch 189/314, Loss: 1.8673059940338135\n",
            "Epoch 0, Phase val, Batch 433, Batch 189/314, Loss: 1.7646844387054443\n",
            "Epoch 0, Phase val, Batch 398, Batch 190/314, Loss: 1.8554033041000366\n",
            "Epoch 0, Phase val, Batch 398, Batch 190/314, Loss: 1.8853040933609009\n",
            "Epoch 0, Phase val, Batch 57, Batch 191/314, Loss: 1.8820552825927734\n",
            "Epoch 0, Phase val, Batch 57, Batch 191/314, Loss: 1.9487676620483398\n",
            "Epoch 0, Phase val, Batch 1473, Batch 192/314, Loss: 1.886473536491394\n",
            "Epoch 0, Phase val, Batch 1473, Batch 192/314, Loss: 1.7372348308563232\n",
            "Epoch 0, Phase val, Batch 989, Batch 193/314, Loss: 1.8540929555892944\n",
            "Epoch 0, Phase val, Batch 989, Batch 193/314, Loss: 1.7335426807403564\n",
            "Epoch 0, Phase val, Batch 633, Batch 194/314, Loss: 1.9849283695220947\n",
            "Epoch 0, Phase val, Batch 633, Batch 194/314, Loss: 1.9639935493469238\n",
            "Epoch 0, Phase val, Batch 1128, Batch 195/314, Loss: 1.875791311264038\n",
            "Epoch 0, Phase val, Batch 1128, Batch 195/314, Loss: 1.825656771659851\n",
            "Epoch 0, Phase val, Batch 537, Batch 196/314, Loss: 1.8563663959503174\n",
            "Epoch 0, Phase val, Batch 537, Batch 196/314, Loss: 2.040314197540283\n",
            "Epoch 0, Phase val, Batch 342, Batch 197/314, Loss: 1.9413321018218994\n",
            "Epoch 0, Phase val, Batch 342, Batch 197/314, Loss: 1.8975801467895508\n",
            "Epoch 0, Phase val, Batch 1145, Batch 198/314, Loss: 1.6612684726715088\n",
            "Epoch 0, Phase val, Batch 1145, Batch 198/314, Loss: 1.7806735038757324\n",
            "Epoch 0, Phase val, Batch 1569, Batch 199/314, Loss: 1.6868069171905518\n",
            "Epoch 0, Phase val, Batch 1569, Batch 199/314, Loss: 1.5520215034484863\n",
            "Epoch 0, Phase val, Batch 114, Batch 200/314, Loss: 1.8539315462112427\n",
            "Epoch 0, Phase val, Batch 114, Batch 200/314, Loss: 1.8321119546890259\n",
            "Epoch 0, Phase val, Batch 1230, Batch 201/314, Loss: 1.6929035186767578\n",
            "Epoch 0, Phase val, Batch 1230, Batch 201/314, Loss: 1.7758015394210815\n",
            "Epoch 0, Phase val, Batch 647, Batch 202/314, Loss: 1.8995389938354492\n",
            "Epoch 0, Phase val, Batch 647, Batch 202/314, Loss: 1.8324236869812012\n",
            "Epoch 0, Phase val, Batch 379, Batch 203/314, Loss: 1.7591986656188965\n",
            "Epoch 0, Phase val, Batch 379, Batch 203/314, Loss: 1.9144448041915894\n",
            "Epoch 0, Phase val, Batch 163, Batch 204/314, Loss: 1.6525914669036865\n",
            "Epoch 0, Phase val, Batch 163, Batch 204/314, Loss: 1.8417766094207764\n",
            "Epoch 0, Phase val, Batch 732, Batch 205/314, Loss: 1.8543285131454468\n",
            "Epoch 0, Phase val, Batch 732, Batch 205/314, Loss: 1.8999496698379517\n",
            "Epoch 0, Phase val, Batch 1034, Batch 206/314, Loss: 1.8706321716308594\n",
            "Epoch 0, Phase val, Batch 1034, Batch 206/314, Loss: 1.8680001497268677\n",
            "Epoch 0, Phase val, Batch 636, Batch 207/314, Loss: 1.7656255960464478\n",
            "Epoch 0, Phase val, Batch 636, Batch 207/314, Loss: 1.8755987882614136\n",
            "Epoch 0, Phase val, Batch 890, Batch 208/314, Loss: 1.7481008768081665\n",
            "Epoch 0, Phase val, Batch 890, Batch 208/314, Loss: 1.9110713005065918\n",
            "Epoch 0, Phase val, Batch 1525, Batch 209/314, Loss: 1.8025782108306885\n",
            "Epoch 0, Phase val, Batch 1525, Batch 209/314, Loss: 1.7999911308288574\n",
            "Epoch 0, Phase val, Batch 1092, Batch 210/314, Loss: 1.9595565795898438\n",
            "Epoch 0, Phase val, Batch 1092, Batch 210/314, Loss: 1.7643285989761353\n",
            "Epoch 0, Phase val, Batch 159, Batch 211/314, Loss: 1.9385364055633545\n",
            "Epoch 0, Phase val, Batch 159, Batch 211/314, Loss: 1.8165314197540283\n",
            "Epoch 0, Phase val, Batch 419, Batch 212/314, Loss: 1.7753684520721436\n",
            "Epoch 0, Phase val, Batch 419, Batch 212/314, Loss: 1.7480508089065552\n",
            "Epoch 0, Phase val, Batch 452, Batch 213/314, Loss: 1.7609823942184448\n",
            "Epoch 0, Phase val, Batch 452, Batch 213/314, Loss: 1.768128514289856\n",
            "Epoch 0, Phase val, Batch 1212, Batch 214/314, Loss: 1.8723763227462769\n",
            "Epoch 0, Phase val, Batch 1212, Batch 214/314, Loss: 1.7488207817077637\n",
            "Epoch 0, Phase val, Batch 1047, Batch 215/314, Loss: 1.8917900323867798\n",
            "Epoch 0, Phase val, Batch 1047, Batch 215/314, Loss: 1.8665528297424316\n",
            "Epoch 0, Phase val, Batch 1239, Batch 216/314, Loss: 1.735286831855774\n",
            "Epoch 0, Phase val, Batch 1239, Batch 216/314, Loss: 1.773451805114746\n",
            "Epoch 0, Phase val, Batch 378, Batch 217/314, Loss: 1.821025013923645\n",
            "Epoch 0, Phase val, Batch 378, Batch 217/314, Loss: 1.5736937522888184\n",
            "Epoch 0, Phase val, Batch 1073, Batch 218/314, Loss: 1.8643109798431396\n",
            "Epoch 0, Phase val, Batch 1073, Batch 218/314, Loss: 1.7843079566955566\n",
            "Epoch 0, Phase val, Batch 214, Batch 219/314, Loss: 1.9862656593322754\n",
            "Epoch 0, Phase val, Batch 214, Batch 219/314, Loss: 1.8497509956359863\n",
            "Epoch 0, Phase val, Batch 777, Batch 220/314, Loss: 1.7486943006515503\n",
            "Epoch 0, Phase val, Batch 777, Batch 220/314, Loss: 1.859226942062378\n",
            "Epoch 0, Phase val, Batch 917, Batch 221/314, Loss: 1.7965638637542725\n",
            "Epoch 0, Phase val, Batch 917, Batch 221/314, Loss: 1.967500925064087\n",
            "Epoch 0, Phase val, Batch 1253, Batch 222/314, Loss: 1.7085061073303223\n",
            "Epoch 0, Phase val, Batch 1253, Batch 222/314, Loss: 1.8062396049499512\n",
            "Epoch 0, Phase val, Batch 147, Batch 223/314, Loss: 1.8607460260391235\n",
            "Epoch 0, Phase val, Batch 147, Batch 223/314, Loss: 1.8627275228500366\n",
            "Epoch 0, Phase val, Batch 1501, Batch 224/314, Loss: 1.7712690830230713\n",
            "Epoch 0, Phase val, Batch 1501, Batch 224/314, Loss: 1.8776458501815796\n",
            "Epoch 0, Phase val, Batch 498, Batch 225/314, Loss: 1.7227531671524048\n",
            "Epoch 0, Phase val, Batch 498, Batch 225/314, Loss: 2.0339460372924805\n",
            "Epoch 0, Phase val, Batch 833, Batch 226/314, Loss: 1.7629040479660034\n",
            "Epoch 0, Phase val, Batch 833, Batch 226/314, Loss: 1.8103467226028442\n",
            "Epoch 0, Phase val, Batch 1529, Batch 227/314, Loss: 1.772289752960205\n",
            "Epoch 0, Phase val, Batch 1529, Batch 227/314, Loss: 1.8324429988861084\n",
            "Epoch 0, Phase val, Batch 945, Batch 228/314, Loss: 1.7284318208694458\n",
            "Epoch 0, Phase val, Batch 945, Batch 228/314, Loss: 1.9022397994995117\n",
            "Epoch 0, Phase val, Batch 662, Batch 229/314, Loss: 1.8899105787277222\n",
            "Epoch 0, Phase val, Batch 662, Batch 229/314, Loss: 1.9050207138061523\n",
            "Epoch 0, Phase val, Batch 351, Batch 230/314, Loss: 2.2052643299102783\n",
            "Epoch 0, Phase val, Batch 351, Batch 230/314, Loss: 2.0872089862823486\n",
            "Epoch 0, Phase val, Batch 723, Batch 231/314, Loss: 1.854729175567627\n",
            "Epoch 0, Phase val, Batch 723, Batch 231/314, Loss: 1.793989658355713\n",
            "Epoch 0, Phase val, Batch 367, Batch 232/314, Loss: 1.755669116973877\n",
            "Epoch 0, Phase val, Batch 367, Batch 232/314, Loss: 1.6996347904205322\n",
            "Epoch 0, Phase val, Batch 287, Batch 233/314, Loss: 1.8514405488967896\n",
            "Epoch 0, Phase val, Batch 287, Batch 233/314, Loss: 1.9974881410598755\n",
            "Epoch 0, Phase val, Batch 446, Batch 234/314, Loss: 1.7572895288467407\n",
            "Epoch 0, Phase val, Batch 446, Batch 234/314, Loss: 1.7528653144836426\n",
            "Epoch 0, Phase val, Batch 372, Batch 235/314, Loss: 1.930955410003662\n",
            "Epoch 0, Phase val, Batch 372, Batch 235/314, Loss: 1.7757349014282227\n",
            "Epoch 0, Phase val, Batch 467, Batch 236/314, Loss: 1.6350643634796143\n",
            "Epoch 0, Phase val, Batch 467, Batch 236/314, Loss: 1.7182683944702148\n",
            "Epoch 0, Phase val, Batch 1526, Batch 237/314, Loss: 1.5885918140411377\n",
            "Epoch 0, Phase val, Batch 1526, Batch 237/314, Loss: 1.7077131271362305\n",
            "Epoch 0, Phase val, Batch 850, Batch 238/314, Loss: 1.6509003639221191\n",
            "Epoch 0, Phase val, Batch 850, Batch 238/314, Loss: 1.8723993301391602\n",
            "Epoch 0, Phase val, Batch 1077, Batch 239/314, Loss: 1.8468022346496582\n",
            "Epoch 0, Phase val, Batch 1077, Batch 239/314, Loss: 1.8617180585861206\n",
            "Epoch 0, Phase val, Batch 574, Batch 240/314, Loss: 1.847116470336914\n",
            "Epoch 0, Phase val, Batch 574, Batch 240/314, Loss: 2.0327014923095703\n",
            "Epoch 0, Phase val, Batch 1414, Batch 241/314, Loss: 1.7834272384643555\n",
            "Epoch 0, Phase val, Batch 1414, Batch 241/314, Loss: 1.6123034954071045\n",
            "Epoch 0, Phase val, Batch 1375, Batch 242/314, Loss: 1.7212897539138794\n",
            "Epoch 0, Phase val, Batch 1375, Batch 242/314, Loss: 1.6501245498657227\n",
            "Epoch 0, Phase val, Batch 984, Batch 243/314, Loss: 1.890195369720459\n",
            "Epoch 0, Phase val, Batch 984, Batch 243/314, Loss: 1.6783970594406128\n",
            "Epoch 0, Phase val, Batch 1506, Batch 244/314, Loss: 1.6611416339874268\n",
            "Epoch 0, Phase val, Batch 1506, Batch 244/314, Loss: 1.5910882949829102\n",
            "Epoch 0, Phase val, Batch 1103, Batch 245/314, Loss: 1.8772392272949219\n",
            "Epoch 0, Phase val, Batch 1103, Batch 245/314, Loss: 1.7557835578918457\n",
            "Epoch 0, Phase val, Batch 421, Batch 246/314, Loss: 1.7908580303192139\n",
            "Epoch 0, Phase val, Batch 421, Batch 246/314, Loss: 1.639676809310913\n",
            "Epoch 0, Phase val, Batch 1256, Batch 247/314, Loss: 1.6529912948608398\n",
            "Epoch 0, Phase val, Batch 1256, Batch 247/314, Loss: 1.6733276844024658\n",
            "Epoch 0, Phase val, Batch 779, Batch 248/314, Loss: 1.7787325382232666\n",
            "Epoch 0, Phase val, Batch 779, Batch 248/314, Loss: 1.6151063442230225\n",
            "Epoch 0, Phase val, Batch 1280, Batch 249/314, Loss: 1.6111098527908325\n",
            "Epoch 0, Phase val, Batch 1280, Batch 249/314, Loss: 1.7823512554168701\n",
            "Epoch 0, Phase val, Batch 1341, Batch 250/314, Loss: 1.616919994354248\n",
            "Epoch 0, Phase val, Batch 1341, Batch 250/314, Loss: 1.7669638395309448\n",
            "Epoch 0, Phase val, Batch 131, Batch 251/314, Loss: 1.8993093967437744\n",
            "Epoch 0, Phase val, Batch 131, Batch 251/314, Loss: 1.8369580507278442\n",
            "Epoch 0, Phase val, Batch 226, Batch 252/314, Loss: 1.831591010093689\n",
            "Epoch 0, Phase val, Batch 226, Batch 252/314, Loss: 1.615402340888977\n",
            "Epoch 0, Phase val, Batch 56, Batch 253/314, Loss: 1.8821868896484375\n",
            "Epoch 0, Phase val, Batch 56, Batch 253/314, Loss: 1.9478603601455688\n",
            "Epoch 0, Phase val, Batch 241, Batch 254/314, Loss: 1.7368732690811157\n",
            "Epoch 0, Phase val, Batch 241, Batch 254/314, Loss: 1.9361040592193604\n",
            "Epoch 0, Phase val, Batch 832, Batch 255/314, Loss: 1.908293604850769\n",
            "Epoch 0, Phase val, Batch 832, Batch 255/314, Loss: 1.779336929321289\n",
            "Epoch 0, Phase val, Batch 713, Batch 256/314, Loss: 1.8383897542953491\n",
            "Epoch 0, Phase val, Batch 713, Batch 256/314, Loss: 1.8726706504821777\n",
            "Epoch 0, Phase val, Batch 331, Batch 257/314, Loss: 1.7915029525756836\n",
            "Epoch 0, Phase val, Batch 331, Batch 257/314, Loss: 1.7242839336395264\n",
            "Epoch 0, Phase val, Batch 565, Batch 258/314, Loss: 2.0666470527648926\n",
            "Epoch 0, Phase val, Batch 565, Batch 258/314, Loss: 2.1333556175231934\n",
            "Epoch 0, Phase val, Batch 820, Batch 259/314, Loss: 1.7453941106796265\n",
            "Epoch 0, Phase val, Batch 820, Batch 259/314, Loss: 1.7394667863845825\n",
            "Epoch 0, Phase val, Batch 233, Batch 260/314, Loss: 1.86788809299469\n",
            "Epoch 0, Phase val, Batch 233, Batch 260/314, Loss: 1.821997880935669\n",
            "Epoch 0, Phase val, Batch 522, Batch 261/314, Loss: 1.6712958812713623\n",
            "Epoch 0, Phase val, Batch 522, Batch 261/314, Loss: 1.7771536111831665\n",
            "Epoch 0, Phase val, Batch 972, Batch 262/314, Loss: 1.8415366411209106\n",
            "Epoch 0, Phase val, Batch 972, Batch 262/314, Loss: 1.7432684898376465\n",
            "Epoch 0, Phase val, Batch 627, Batch 263/314, Loss: 2.0361926555633545\n",
            "Epoch 0, Phase val, Batch 627, Batch 263/314, Loss: 1.964369535446167\n",
            "Epoch 0, Phase val, Batch 220, Batch 264/314, Loss: 1.8149393796920776\n",
            "Epoch 0, Phase val, Batch 220, Batch 264/314, Loss: 1.8374972343444824\n",
            "Epoch 0, Phase val, Batch 90, Batch 265/314, Loss: 1.8938853740692139\n",
            "Epoch 0, Phase val, Batch 90, Batch 265/314, Loss: 2.1043832302093506\n",
            "Epoch 0, Phase val, Batch 1497, Batch 266/314, Loss: 1.716302514076233\n",
            "Epoch 0, Phase val, Batch 1497, Batch 266/314, Loss: 1.8369256258010864\n",
            "Epoch 0, Phase val, Batch 1519, Batch 267/314, Loss: 1.7984020709991455\n",
            "Epoch 0, Phase val, Batch 1519, Batch 267/314, Loss: 1.851750373840332\n",
            "Epoch 0, Phase val, Batch 699, Batch 268/314, Loss: 1.926311731338501\n",
            "Epoch 0, Phase val, Batch 699, Batch 268/314, Loss: 1.737658143043518\n",
            "Epoch 0, Phase val, Batch 122, Batch 269/314, Loss: 1.7238229513168335\n",
            "Epoch 0, Phase val, Batch 122, Batch 269/314, Loss: 1.827841877937317\n",
            "Epoch 0, Phase val, Batch 281, Batch 270/314, Loss: 1.726585865020752\n",
            "Epoch 0, Phase val, Batch 281, Batch 270/314, Loss: 1.8439555168151855\n",
            "Epoch 0, Phase val, Batch 825, Batch 272/314, Loss: 1.8075512647628784\n",
            "Epoch 0, Phase val, Batch 825, Batch 272/314, Loss: 1.6705268621444702\n",
            "Epoch 0, Phase val, Batch 1374, Batch 273/314, Loss: 1.7571806907653809\n",
            "Epoch 0, Phase val, Batch 1374, Batch 273/314, Loss: 1.7890671491622925\n",
            "Epoch 0, Phase val, Batch 11, Batch 274/314, Loss: 2.024019479751587\n",
            "Epoch 0, Phase val, Batch 11, Batch 274/314, Loss: 2.0173726081848145\n",
            "Epoch 0, Phase val, Batch 908, Batch 275/314, Loss: 1.8587191104888916\n",
            "Epoch 0, Phase val, Batch 908, Batch 275/314, Loss: 1.7094999551773071\n",
            "Epoch 0, Phase val, Batch 966, Batch 276/314, Loss: 1.8766493797302246\n",
            "Epoch 0, Phase val, Batch 966, Batch 276/314, Loss: 1.8187718391418457\n",
            "Epoch 0, Phase val, Batch 1561, Batch 277/314, Loss: 1.6596276760101318\n",
            "Epoch 0, Phase val, Batch 1561, Batch 277/314, Loss: 1.647818922996521\n",
            "Epoch 0, Phase val, Batch 648, Batch 278/314, Loss: 2.0470473766326904\n",
            "Epoch 0, Phase val, Batch 648, Batch 278/314, Loss: 1.853847622871399\n",
            "Epoch 0, Phase val, Batch 852, Batch 279/314, Loss: 1.7000932693481445\n",
            "Epoch 0, Phase val, Batch 852, Batch 279/314, Loss: 1.8961955308914185\n",
            "Epoch 0, Phase val, Batch 566, Batch 280/314, Loss: 1.8231751918792725\n",
            "Epoch 0, Phase val, Batch 566, Batch 280/314, Loss: 1.9319756031036377\n",
            "Epoch 0, Phase val, Batch 690, Batch 281/314, Loss: 1.7544082403182983\n",
            "Epoch 0, Phase val, Batch 690, Batch 281/314, Loss: 1.7955949306488037\n",
            "Epoch 0, Phase val, Batch 408, Batch 282/314, Loss: 1.743092656135559\n",
            "Epoch 0, Phase val, Batch 408, Batch 282/314, Loss: 1.6858189105987549\n",
            "Epoch 0, Phase val, Batch 601, Batch 283/314, Loss: 2.0212674140930176\n",
            "Epoch 0, Phase val, Batch 601, Batch 283/314, Loss: 1.8698441982269287\n",
            "Epoch 0, Phase val, Batch 261, Batch 284/314, Loss: 2.0265800952911377\n",
            "Epoch 0, Phase val, Batch 261, Batch 284/314, Loss: 1.8217132091522217\n",
            "Epoch 0, Phase val, Batch 1318, Batch 285/314, Loss: 1.8796091079711914\n",
            "Epoch 0, Phase val, Batch 1318, Batch 285/314, Loss: 1.6889721155166626\n",
            "Epoch 0, Phase val, Batch 680, Batch 286/314, Loss: 1.9306517839431763\n",
            "Epoch 0, Phase val, Batch 680, Batch 286/314, Loss: 1.6929246187210083\n",
            "Epoch 0, Phase val, Batch 693, Batch 287/314, Loss: 1.7500433921813965\n",
            "Epoch 0, Phase val, Batch 693, Batch 287/314, Loss: 1.7702301740646362\n",
            "Epoch 0, Phase val, Batch 610, Batch 288/314, Loss: 1.8287264108657837\n",
            "Epoch 0, Phase val, Batch 610, Batch 288/314, Loss: 1.8334232568740845\n",
            "Epoch 0, Phase val, Batch 1272, Batch 289/314, Loss: 1.861177682876587\n",
            "Epoch 0, Phase val, Batch 1272, Batch 289/314, Loss: 1.5605062246322632\n",
            "Epoch 0, Phase val, Batch 1133, Batch 290/314, Loss: 1.8449400663375854\n",
            "Epoch 0, Phase val, Batch 1133, Batch 290/314, Loss: 1.7766072750091553\n",
            "Epoch 0, Phase val, Batch 771, Batch 291/314, Loss: 1.8080055713653564\n",
            "Epoch 0, Phase val, Batch 771, Batch 291/314, Loss: 1.8868885040283203\n",
            "Epoch 0, Phase val, Batch 948, Batch 292/314, Loss: 1.8082144260406494\n",
            "Epoch 0, Phase val, Batch 948, Batch 292/314, Loss: 1.7978113889694214\n",
            "Epoch 0, Phase val, Batch 501, Batch 293/314, Loss: 1.7792998552322388\n",
            "Epoch 0, Phase val, Batch 501, Batch 293/314, Loss: 1.83816397190094\n",
            "Epoch 0, Phase val, Batch 973, Batch 294/314, Loss: 1.7827860116958618\n",
            "Epoch 0, Phase val, Batch 973, Batch 294/314, Loss: 1.8558107614517212\n",
            "Epoch 0, Phase val, Batch 315, Batch 295/314, Loss: 1.919568657875061\n",
            "Epoch 0, Phase val, Batch 315, Batch 295/314, Loss: 1.6703715324401855\n",
            "Epoch 0, Phase val, Batch 99, Batch 296/314, Loss: 1.9302736520767212\n",
            "Epoch 0, Phase val, Batch 99, Batch 296/314, Loss: 1.9351509809494019\n",
            "Epoch 0, Phase val, Batch 1439, Batch 297/314, Loss: 1.661611795425415\n",
            "Epoch 0, Phase val, Batch 1439, Batch 297/314, Loss: 1.7334229946136475\n",
            "Epoch 0, Phase val, Batch 582, Batch 298/314, Loss: 1.8953715562820435\n",
            "Epoch 0, Phase val, Batch 582, Batch 298/314, Loss: 1.8335224390029907\n",
            "Epoch 0, Phase val, Batch 45, Batch 299/314, Loss: 1.8942375183105469\n",
            "Epoch 0, Phase val, Batch 45, Batch 299/314, Loss: 1.921759009361267\n",
            "Epoch 0, Phase val, Batch 762, Batch 300/314, Loss: 1.799834132194519\n",
            "Epoch 0, Phase val, Batch 762, Batch 300/314, Loss: 1.6839361190795898\n",
            "Epoch 0, Phase val, Batch 236, Batch 301/314, Loss: 1.7287708520889282\n",
            "Epoch 0, Phase val, Batch 236, Batch 301/314, Loss: 1.8512579202651978\n",
            "Epoch 0, Phase val, Batch 934, Batch 302/314, Loss: 1.8538832664489746\n",
            "Epoch 0, Phase val, Batch 934, Batch 302/314, Loss: 1.844096302986145\n",
            "Epoch 0, Phase val, Batch 381, Batch 303/314, Loss: 1.7311440706253052\n",
            "Epoch 0, Phase val, Batch 381, Batch 303/314, Loss: 1.7195864915847778\n",
            "Epoch 0, Phase val, Batch 503, Batch 304/314, Loss: 1.8066926002502441\n",
            "Epoch 0, Phase val, Batch 503, Batch 304/314, Loss: 1.7804752588272095\n",
            "Epoch 0, Phase val, Batch 1346, Batch 305/314, Loss: 1.748338222503662\n",
            "Epoch 0, Phase val, Batch 1346, Batch 305/314, Loss: 1.6359341144561768\n",
            "Epoch 0, Phase val, Batch 549, Batch 306/314, Loss: 1.8552898168563843\n",
            "Epoch 0, Phase val, Batch 549, Batch 306/314, Loss: 1.7987170219421387\n",
            "Epoch 0, Phase val, Batch 1002, Batch 307/314, Loss: 1.7396821975708008\n",
            "Epoch 0, Phase val, Batch 1002, Batch 307/314, Loss: 1.7281402349472046\n",
            "Epoch 0, Phase val, Batch 360, Batch 308/314, Loss: 1.6851837635040283\n",
            "Epoch 0, Phase val, Batch 360, Batch 308/314, Loss: 1.6688363552093506\n",
            "Epoch 0, Phase val, Batch 10, Batch 309/314, Loss: 2.031254291534424\n",
            "Epoch 0, Phase val, Batch 10, Batch 309/314, Loss: 2.0113730430603027\n",
            "Epoch 0, Phase val, Batch 1074, Batch 310/314, Loss: 1.845978856086731\n",
            "Epoch 0, Phase val, Batch 1074, Batch 310/314, Loss: 1.8086597919464111\n",
            "Epoch 0, Phase val, Batch 1283, Batch 311/314, Loss: 1.800610899925232\n",
            "Epoch 0, Phase val, Batch 1283, Batch 311/314, Loss: 1.9806632995605469\n",
            "Epoch 0, Phase val, Batch 313, Batch 312/314, Loss: 1.972205638885498\n",
            "Epoch 0, Phase val, Batch 313, Batch 312/314, Loss: 2.0005412101745605\n",
            "Epoch 0, Phase val, Batch 54, Batch 313/314, Loss: 2.0288376808166504\n",
            "Epoch 0, Phase val, Batch 54, Batch 313/314, Loss: 2.106452226638794\n",
            "Epoch 0, Phase val, Batch 1307, Batch 314/314, Loss: 1.7440431118011475\n",
            "Epoch 0, Phase val, Batch 1307, Batch 314/314, Loss: 1.6244488954544067\n",
            "Epoch 0, Validation Loss: 0.0, Validation Accuracy: 0.0\n",
            "Epoch 0, Phase val completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dfur2h0RCsrk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}